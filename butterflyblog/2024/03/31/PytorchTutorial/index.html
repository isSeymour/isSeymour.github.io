<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>《Pytorch Tutorial》Notes（初稿） | isSeymour</title><meta name="author" content="isSeymour"><meta name="copyright" content="isSeymour"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="《Pytorch Tutorial》Notes（初稿）  来源：《PyTorch深度学习实践》完结合集 2024-03-31~04-02 @isSeymour  [TOC] 01、Overview 1.1 引入   预测与推理       1.2 分类与历史       scikit-learn       历史       思考：降维      思考：降维的意义与原理 意义   意义：根据大数定">
<meta property="og:type" content="article">
<meta property="og:title" content="《Pytorch Tutorial》Notes（初稿）">
<meta property="og:url" content="https://isseymour.github.io/butterflyblog/2024/03/31/PytorchTutorial/index.html">
<meta property="og:site_name" content="isSeymour">
<meta property="og:description" content="《Pytorch Tutorial》Notes（初稿）  来源：《PyTorch深度学习实践》完结合集 2024-03-31~04-02 @isSeymour  [TOC] 01、Overview 1.1 引入   预测与推理       1.2 分类与历史       scikit-learn       历史       思考：降维      思考：降维的意义与原理 意义   意义：根据大数定">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/PageCover.png">
<meta property="article:published_time" content="2024-03-30T16:00:00.000Z">
<meta property="article:modified_time" content="2024-04-01T16:00:00.000Z">
<meta property="article:author" content="isSeymour">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/PageCover.png"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/IC1011.ico"><link rel="canonical" href="https://isseymour.github.io/butterflyblog/2024/03/31/PytorchTutorial/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/butterflyblog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/butterflyblog/',
  algolia: undefined,
  localSearch: {"path":"/butterflyblog/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《Pytorch Tutorial》Notes（初稿）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-02 00:00:00'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyblog/code/style.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/T6.jpg" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/butterflyblog/archives/"><div class="headline">文章</div><div class="length-num">48</div></a><a href="/butterflyblog/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/butterflyblog/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/archives/"><i class="fa-fw fa-solid fa-timeline"></i><span> 总览</span></a></li><li><a class="site-page child" href="/butterflyblog/tags/"><i class="fa-fw fa-sharp fa-solid fa-hashtag"></i><span> 标签</span></a></li><li><a class="site-page child" href="/butterflyblog/categories/"><i class="fa-fw fa-sharp fa-solid fa-folder"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-sharp fa-solid fa-list"></i><span> 功能</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/ctf/"><i class="fa-fw fa-solid fa-shield-halved"></i><span> CTF</span></a></li><li><a class="site-page child" href="/butterflyblog/music/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/butterflyblog/tools/"><i class="fa-fw fa-solid fa-screwdriver-wrench"></i><span> 下载</span></a></li><li><a class="site-page child" href="/butterflyblog/link/"><i class="fa-fw fa-solid fa-paper-plane"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-user"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/about/"><i class="fa-fw fa-regular fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/butterflyblog/message/"><i class="fa-fw fa-solid fa-message"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/butterflyblog/develop/"><i class="fa-fw fa-brands fa-windows"></i><span> 开发日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/PageCover.png')"><nav id="nav"><span id="blog-info"><a href="/butterflyblog/" title="isSeymour"><span class="site-name">isSeymour</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/archives/"><i class="fa-fw fa-solid fa-timeline"></i><span> 总览</span></a></li><li><a class="site-page child" href="/butterflyblog/tags/"><i class="fa-fw fa-sharp fa-solid fa-hashtag"></i><span> 标签</span></a></li><li><a class="site-page child" href="/butterflyblog/categories/"><i class="fa-fw fa-sharp fa-solid fa-folder"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-sharp fa-solid fa-list"></i><span> 功能</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/ctf/"><i class="fa-fw fa-solid fa-shield-halved"></i><span> CTF</span></a></li><li><a class="site-page child" href="/butterflyblog/music/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/butterflyblog/tools/"><i class="fa-fw fa-solid fa-screwdriver-wrench"></i><span> 下载</span></a></li><li><a class="site-page child" href="/butterflyblog/link/"><i class="fa-fw fa-solid fa-paper-plane"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-user"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/about/"><i class="fa-fw fa-regular fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/butterflyblog/message/"><i class="fa-fw fa-solid fa-message"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/butterflyblog/develop/"><i class="fa-fw fa-brands fa-windows"></i><span> 开发日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">《Pytorch Tutorial》Notes（初稿）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-30T16:00:00.000Z" title="发表于 2024-03-31 00:00:00">2024-03-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-01T16:00:00.000Z" title="更新于 2024-04-02 00:00:00">2024-04-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/butterflyblog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="《Pytorch Tutorial》Notes（初稿）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>《Pytorch Tutorial》Notes（初稿）</h1>
<blockquote>
<p>来源：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Y7411d7Ys/">《PyTorch深度学习实践》完结合集</a></p>
<p>2024-03-31~04-02 @isSeymour</p>
</blockquote>
<p>[TOC]</p>
<h2 id="01、Overview">01、Overview</h2>
<h3 id="1-1-引入">1.1 引入</h3>
<ul>
<li>
<h4 id="预测与推理">预测与推理</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-1.png" alt="1-1" style="zoom: 33%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-2.png" alt="1-2" style="zoom:33%;" /> 
<h3 id="1-2-分类与历史">1.2 分类与历史</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-3-1.png" alt="1-3-1" style="zoom: 50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-3-2.png" alt="1-3-2" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="scikit-learn">scikit-learn</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-3-3.png" alt="1-3-3" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="历史">历史</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-4-1.png" alt="1-4-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="思考：降维">思考：降维</h4>
</li>
</ul>
<blockquote>
<ul>
<li>
<p>思考：降维的意义与原理</p>
<p><strong>意义</strong></p>
<ul>
<li>
<p>意义：根据大数定律，当数据量足够大时，样本数据频率就能基本代表真实分布。</p>
<p>因此，我们希望能在有限但足够的数据量下，完成这个目标。</p>
</li>
<li>
<p>我们假设每一个特征，需要10个数据量，才能达到大数定律的要求。</p>
<p>那么当特征是两个（二维）时，那么将需要 10^2^ 个才能同时使得二维特征都达到大数定律要求；</p>
<p>若三维，则需要 10^3^ 个数据量；</p>
<p>以此类推，所需数据量将爆炸！——这就是“<strong>维度诅咒</strong>”。</p>
</li>
<li>
<p>因此，降维迫在眉睫，降维才能使得有限的数据量更加有效地反应真实分布。</p>
</li>
</ul>
<p><strong>原理</strong></p>
<ul>
<li>
<p>原理：</p>
<p>假设有10个数据量，每个数据量都是n维特征量，而我们希望降维到三维。</p>
<p>实际上，就是将一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>x</mi><msub><mo stretchy="false">]</mo><mrow><mi>n</mi><mo>×</mo><mn>10</mn></mrow></msub></mrow><annotation encoding="application/x-tex">[x]_{n\times 10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 的矩阵，希望线性变换到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msub><mo stretchy="false">]</mo><mrow><mn>3</mn><mo>×</mo><mn>10</mn></mrow></msub></mrow><annotation encoding="application/x-tex">[x&#x27;]_{3\times 10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 的矩阵。</p>
<p>那么我们只需要找到一个变换矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>k</mi><msub><mo stretchy="false">]</mo><mrow><mn>3</mn><mo>×</mo><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[k]_{3 \times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 就能完成变换了。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">[</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msub><mo stretchy="false">]</mo><mrow><mn>3</mn><mo>×</mo><mn>10</mn></mrow></msub><mo>=</mo><mo stretchy="false">[</mo><mi>k</mi><msub><mo stretchy="false">]</mo><mrow><mn>3</mn><mo>×</mo><mi>n</mi></mrow></msub><mo>⋅</mo><mo stretchy="false">[</mo><mi>x</mi><msub><mo stretchy="false">]</mo><mrow><mi>n</mi><mo>×</mo><mn>10</mn></mrow></msub></mrow><annotation encoding="application/x-tex">[x&#x27;]_{3\times 10} = [k]_{3\times n} \cdot [x]_{n \times 10}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0519em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<blockquote>
<ul>
<li>
<p>深入：</p>
<ul>
<li>这个线性变换是不是使部分数据损失了？若损失了，为什么能这么做？</li>
<li>换句话说，线性变换是在降维，那么这种降维到底是在做什么，能使得降维既可以突破维度诅咒，又能有效代替原维度？</li>
</ul>
</li>
<li>
<p>我的思考：</p>
<p>首先，肯定是有数据损失了。</p>
<p>我们通过具体看这个降维的矩阵变换，你会发现，（按照上例），是三次线性结合，每次线性结合是把一个数据量的n个特征都凝聚到一个特征里面，而此次线性结合每个特征的权值，就是矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span> 做的事情了。</p>
<p>所以说，你可以认为，降维实际上是在做两件事情：</p>
<ol>
<li>
<p>找有效特征间的联系：</p>
<p>通过权值，把各个特征量区别性地凝聚为一个。</p>
<p>若一次凝聚不够，就多次凝聚。</p>
<p>（上例是3次，但是不可过多，比如n次，那相当于原特征量乘了单位矩阵）</p>
</li>
<li>
<p>丢弃掉无效特征：</p>
<p>无效特征量，若是占用一整个维度，这既不合理，且妨碍计算。</p>
<p>因此无效数据量在矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span> 中会出现 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span> 即将不再影响数据性质。</p>
<p>（丢弃无效特征，本质上也是一种寻找特征间联系，这个联系是无联系）</p>
</li>
</ol>
</li>
<li>
<p>反思</p>
<p>从这里你可以看到，降维的难点在哪？</p>
<ol>
<li>如何确定维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">X&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></li>
<li>如何寻找高效的变换矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span></li>
</ol>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>
<h4 id="前馈与反向传播">前馈与反向传播</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-4-2.png" alt="1-4-2" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="一些模型">一些模型</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-4-3.png" alt="1-4-3" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/1-4-4.png" alt="1-4-4" style="zoom:50%;" /> 
<h3 id="1-3-开始吧">1.3 开始吧</h3>
<p>参考其他链接，安装好 Pytorch 和 cuda</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br></pre></td></tr></table></figure>
<h3 id="02、Linear-Model">02、Linear Model</h3>
<h3 id="2-1-引入">2.1 引入</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-1.png" alt="2-1-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-2.png" alt="2-1-2" style="zoom:50%;" /> 
<h3 id="2-2-损失函数">2.2 损失函数</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-3.png" alt="2-1-3" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-4.png" alt="2-1-4" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-5.png" alt="2-1-5" style="zoom:50%;" /> 
<blockquote>
<ul>
<li>
<p>对于线性模型 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>w</mi><mo>⋅</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y = w \cdot x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></p>
<p>我们实际上就是想找到一个合适的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 使得模型尽可能的去拟合数据</p>
<p>（即，模型输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 预测出的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span>，很接近真实的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>​ ）</p>
</li>
<li>
<p>这么多数据，我们怎么评判哪个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 是最好的？</p>
<p>因此，需要制定一个标准来从数学数值上进行量化判定，<strong>损失函数</strong> 应运而生。</p>
</li>
</ul>
</blockquote>
<h3 id="2-3-代码实现">2.3 代码实现</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-6.png" alt="2-1-6" style="zoom: 67%;" /> 
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x*w </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w=&quot;</span>, w)</span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val, y_val)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t&quot;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">    mse_val = l_sum/<span class="number">3</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MSE=&quot;</span>, mse_val)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(mse_val)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">------</span><br><span class="line">w= 0.0</span><br><span class="line">	 1.0 2.0 0.0 4.0</span><br><span class="line">	 2.0 4.0 0.0 16.0</span><br><span class="line">	 3.0 6.0 0.0 36.0</span><br><span class="line">MSE= 18.666666666666668</span><br><span class="line">------</span><br><span class="line">w= 0.1</span><br><span class="line">	 1.0 2.0 0.1 3.61</span><br><span class="line">	 2.0 4.0 0.2 14.44</span><br><span class="line">	 3.0 6.0 0.30000000000000004 32.49</span><br><span class="line">MSE= 16.846666666666668</span><br><span class="line">------</span><br><span class="line">w= 0.2</span><br><span class="line">	 1.0 2.0 0.2 3.24</span><br><span class="line">	 2.0 4.0 0.4 12.96</span><br><span class="line">	 3.0 6.0 0.6000000000000001 29.160000000000004</span><br><span class="line">MSE= 15.120000000000003</span><br><span class="line">------</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&quot;MSE&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;w&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-8.png" alt="2-1-8" style="zoom:50%;" /> 
<h3 id="2-4-课后题">*2.4 课后题</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-7.png" alt="2-1-7" style="zoom: 50%;" /> 
<blockquote>
<p>绘图参考链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://matplotlib.org/stable/users/explain/toolkits/mplot3d.html">link</a></li>
<li><a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html#numpy.meshgrid">docs</a></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x*w+b </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line">w_list, b_list = [], []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    w_list.append(w)</span><br><span class="line"><span class="keyword">for</span> b <span class="keyword">in</span> np.arange(-<span class="number">2.0</span>, <span class="number">2.10</span>, <span class="number">0.1</span>):</span><br><span class="line">    b_list.append(b)</span><br><span class="line"></span><br><span class="line">mse_list_2d = []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    mse_list = []</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> np.arange(-<span class="number">2.0</span>, <span class="number">2.10</span>, <span class="number">0.1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;------&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;w=&quot;</span>, w, <span class="string">&quot; b=&quot;</span>, b)</span><br><span class="line">        l_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">            y_pred_val = forward(x_val)</span><br><span class="line">            loss_val = loss(x_val, y_val)</span><br><span class="line">            l_sum += loss_val</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\t&quot;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">        mse_val = l_sum/<span class="number">3</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;MSE=&quot;</span>, mse_val)</span><br><span class="line">        mse_list.append(mse_val)</span><br><span class="line">    mse_list_2d.append(mse_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">------</span><br><span class="line">w= 0.0  b= -2.0</span><br><span class="line">	 1.0 2.0 -2.0 16.0</span><br><span class="line">	 2.0 4.0 -2.0 36.0</span><br><span class="line">	 3.0 6.0 -2.0 64.0</span><br><span class="line">MSE= 38.666666666666664</span><br><span class="line">------</span><br><span class="line">w= 0.0  b= -1.9</span><br><span class="line">	 1.0 2.0 -1.9 15.209999999999999</span><br><span class="line">	 2.0 4.0 -1.9 34.81</span><br><span class="line">	 3.0 6.0 -1.9 62.410000000000004</span><br><span class="line">MSE= 37.47666666666667</span><br><span class="line">------</span><br><span class="line">w= 0.0  b= -1.7999999999999998</span><br><span class="line">	 1.0 2.0 -1.7999999999999998 14.44</span><br><span class="line">	 2.0 4.0 -1.7999999999999998 33.64</span><br><span class="line">	 3.0 6.0 -1.7999999999999998 60.839999999999996</span><br><span class="line">MSE= 36.306666666666665</span><br><span class="line">------</span><br><span class="line">w= 0.0  b= -1.6999999999999997</span><br><span class="line">	 1.0 2.0 -1.6999999999999997 13.689999999999998</span><br><span class="line">	 2.0 4.0 -1.6999999999999997 32.489999999999995</span><br><span class="line">	 3.0 6.0 -1.6999999999999997 59.28999999999999</span><br><span class="line">MSE= 35.15666666666666</span><br><span class="line">------</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">&#x27;_mpl-gallery&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make data</span></span><br><span class="line">X = w_list</span><br><span class="line">Y = b_list</span><br><span class="line">X, Y = np.meshgrid(X, Y)</span><br><span class="line">Z = np.array(mse_list_2d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the surface</span></span><br><span class="line">fig, ax = plt.subplots(subplot_kw=&#123;<span class="string">&quot;projection&quot;</span>: <span class="string">&quot;3d&quot;</span>&#125;, figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">ax.plot_surface(X, Y, Z, vmin=Z.<span class="built_in">min</span>() * <span class="number">2</span>, cmap=cm.Blues)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;w&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;b&quot;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&quot;MSE&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/2-1-9.png" alt="2-1-9" style="zoom:50%;" /> 
<h2 id="03、Gradient-Descent">03、Gradient Descent</h2>
<h3 id="3-1-优化问题">3.1 优化问题</h3>
<ul>
<li>就是寻找 使得损失函数最小的 参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的值</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-1-1.png" alt="3-1-1" style="zoom:50%;" /> 
<h3 id="3-2-梯度下降算法">3.2 梯度下降算法</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-1-2.png" alt="3-1-2" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-1-3.png" alt="3-1-3" style="zoom:50%;" /> 
<h3 id="3-3-代码实现">3.3 代码实现</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-2-1.png" alt="3-2-1" style="zoom: 67%;" /> 
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    cost = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred-y)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost/<span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    grad = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        grad += <span class="number">2</span>*x*(x*w-y)</span><br><span class="line">    <span class="keyword">return</span> grad/<span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data, y_data)</span><br><span class="line">    grad_val = gradient(x_data, y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch:&quot;</span>, epoch, <span class="string">&quot; w=&quot;</span>, w, <span class="string">&quot; loss=&quot;</span>, cost_val)</span><br><span class="line">    w -= alpha * grad_val</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Predict (before training) 4 4.0</span><br><span class="line">Epoch: 0  w= 1.0  loss= 4.666666666666667</span><br><span class="line">Epoch: 1  w= 1.0933333333333333  loss= 3.8362074074074086</span><br><span class="line">Epoch: 2  w= 1.1779555555555554  loss= 3.1535329869958857</span><br><span class="line">Epoch: 3  w= 1.2546797037037036  loss= 2.592344272332262</span><br><span class="line">Epoch: 4  w= 1.3242429313580246  loss= 2.1310222071581117</span><br><span class="line">Epoch: 5  w= 1.3873135910979424  loss= 1.7517949663820642</span><br><span class="line">Epoch: 6  w= 1.4444976559288012  loss= 1.440053319920117</span><br><span class="line">Epoch: 7  w= 1.4963445413754464  loss= 1.1837878313441108</span><br><span class="line">Epoch: 8  w= 1.5433523841804047  loss= 0.9731262101573632</span><br><span class="line">Epoch: 9  w= 1.5859728283235668  loss= 0.7999529948031382</span><br><span class="line">Epoch: 10  w= 1.6246153643467005  loss= 0.6575969151946154</span><br><span class="line">...</span><br><span class="line">Epoch: 97  w= 1.9999254544053418  loss= 2.593287985380858e-08</span><br><span class="line">Epoch: 98  w= 1.9999324119941766  loss= 2.131797981222471e-08</span><br><span class="line">Epoch: 99  w= 1.9999387202080534  loss= 1.752432687141379e-08</span><br><span class="line">Predict (after training) 4 7.999777758621207</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">epoch_data = np.arange(<span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">plt.plot(epoch_data[:<span class="number">20</span>], cost_data[:<span class="number">20</span>])</span><br><span class="line">plt.title(<span class="string">&quot;GD&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;cost&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-2-3.png" alt="3-2-3" style="zoom:50%;" />  
<blockquote>
<ul>
<li>
<p>对于学习率 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 的选取不能太大，不能太小</p>
</li>
<li>
<p>太大，会直接越过最优点，导致学习失败</p>
</li>
<li>
<p>太小，学习速率太慢，甚至无法达到最优点</p>
</li>
<li>
<p>比如这里，如果 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></p>
</li>
</ul>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Predict (before training) 4 4.0</span><br><span class="line">Epoch: 0  w= 1.0  loss= 4.666666666666667</span><br><span class="line">Epoch: 1  w= 10.333333333333334  loss= 324.0740740740741</span><br><span class="line">Epoch: 2  w= -67.44444444444446  loss= 22505.144032921817</span><br><span class="line">Epoch: 3  w= 580.7037037037038  loss= 1562857.2245084594</span><br><span class="line">Epoch: 4  w= -4820.530864197532  loss= 108531751.70197636</span><br><span class="line">Epoch: 5  w= 40189.75720164609  loss= 7536927201.526134</span><br><span class="line">Epoch: 6  w= -334895.9766803841  loss= 523397722328.2038</span><br><span class="line">Epoch: 7  w= 2790818.472336534  loss= 36347064050569.695</span><br><span class="line">Epoch: 8  w= -23256801.93613778  loss= 2524101670178451.0</span><br><span class="line">Epoch: 9  w= 193806701.46781486  loss= 1.752848382068369e+17</span><br><span class="line">...</span><br><span class="line">Epoch: 97  w= 2.0865035760330826e+89  loss= 2.0316320139727924e+179</span><br><span class="line">Epoch: 98  w= -1.7387529800275687e+90  loss= 1.4108555652588838e+181</span><br><span class="line">Epoch: 99  w= 1.4489608166896407e+91  loss= 9.797608092075583e+182</span><br><span class="line">Predict (after training) 4 -4.829869388965469e+92</span><br></pre></td></tr></table></figure>
<ul>
<li>如果$ \alpha = 0.1$</li>
</ul>
<p>其实发现，这里反而是0.1时结果最好。</p>
<p>但是你看 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">loss</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span></span></span></span> 的变化，实际上不好，只是恰好滚入了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">w = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Predict (before training) 4 4.0</span><br><span class="line">Epoch: 0  w= 1.0  loss= 4.666666666666667</span><br><span class="line">Epoch: 1  w= 1.9333333333333336  loss= 0.020740740740740594</span><br><span class="line">Epoch: 2  w= 1.9955555555555555  loss= 9.218106995885089e-05</span><br><span class="line">Epoch: 3  w= 1.9997037037037038  loss= 4.096936442612555e-07</span><br><span class="line">Epoch: 4  w= 1.9999802469135801  loss= 1.820860641186282e-09</span><br><span class="line">Epoch: 5  w= 1.999998683127572  loss= 8.092713960827921e-12</span><br><span class="line">Epoch: 6  w= 1.9999999122085048  loss= 3.5967617630537535e-14</span><br><span class="line">Epoch: 7  w= 1.9999999941472337  loss= 1.598560741415451e-16</span><br><span class="line">Epoch: 8  w= 1.9999999996098157  loss= 7.104714637326717e-19</span><br><span class="line">Epoch: 9  w= 1.9999999999739877  loss= 3.1576645553726146e-21</span><br><span class="line">Epoch: 10  w= 1.9999999999982658  loss= 1.4035793259433332e-23</span><br><span class="line">Epoch: 11  w= 1.9999999999998845  loss= 6.221483005843047e-26</span><br><span class="line">Epoch: 12  w= 1.9999999999999922  loss= 2.7841859573644085e-28</span><br><span class="line">Epoch: 13  w= 1.9999999999999993  loss= 1.791371638939381e-30</span><br><span class="line">Epoch: 14  w= 2.0  loss= 0.0</span><br><span class="line">Epoch: 15  w= 2.0  loss= 0.0</span><br><span class="line">Epoch: 16  w= 2.0  loss= 0.0</span><br><span class="line">Epoch: 17  w= 2.0  loss= 0.0</span><br><span class="line">...</span><br><span class="line">Epoch: 99  w= 2.0  loss= 0.0</span><br><span class="line">Predict (after training) 4 8.0</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="3-4-随机梯度下降">3.4 随机梯度下降</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-3-1.png" alt="3-3-1" style="zoom: 50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-4-1.png" alt="3-4-1" style="zoom: 50%;" /> 
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred-y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*x*(x*w-y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">cost_data = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        grad = gradient(x, y)</span><br><span class="line">        w -= alpha * grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tgrad: &quot;</span>, x, y, grad)</span><br><span class="line">        cost += loss(x, y)</span><br><span class="line">    cost_data.append(cost)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;progress: &quot;</span>, epoch, <span class="string">&quot; w=&quot;</span>, w, <span class="string">&quot; loss=&quot;</span>, cost)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Predict (before training) 4 4.0</span><br><span class="line">	grad:  1.0 2.0 -2.0</span><br><span class="line">	grad:  2.0 4.0 -7.84</span><br><span class="line">	grad:  3.0 6.0 -16.2288</span><br><span class="line">progress:  0  w= 1.260688  loss= 9.131170340095998</span><br><span class="line">	grad:  1.0 2.0 -1.478624</span><br><span class="line">	grad:  2.0 4.0 -5.796206079999999</span><br><span class="line">	grad:  3.0 6.0 -11.998146585599997</span><br><span class="line">progress:  1  w= 1.453417766656  loss= 4.990935477534164</span><br><span class="line">	grad:  1.0 2.0 -1.093164466688</span><br><span class="line">	grad:  2.0 4.0 -4.285204709416961</span><br><span class="line">	grad:  3.0 6.0 -8.87037374849311</span><br><span class="line">progress:  2  w= 1.5959051959019805  loss= 2.727956659786429</span><br><span class="line">	grad:  1.0 2.0 -0.8081896081960389</span><br><span class="line">	grad:  2.0 4.0 -3.1681032641284723</span><br><span class="line">	grad:  3.0 6.0 -6.557973756745939</span><br><span class="line">progress:  3  w= 1.701247862192685  loss= 1.4910526435717042</span><br><span class="line">	grad:  1.0 2.0 -0.59750427561463</span><br><span class="line">	grad:  2.0 4.0 -2.3422167604093502</span><br><span class="line">	grad:  3.0 6.0 -4.848388694047353</span><br><span class="line">	...</span><br><span class="line">progress:  98  w= 1.9999999999998967  loss= 1.7819519823469544e-25</span><br><span class="line">	grad:  1.0 2.0 -2.0650148258027912e-13</span><br><span class="line">	grad:  2.0 4.0 -8.100187187665142e-13</span><br><span class="line">	grad:  3.0 6.0 -1.6786572132332367e-12</span><br><span class="line">progress:  99  w= 1.9999999999999236  loss= 9.755053953963032e-26</span><br><span class="line">Predict (after training) 4 7.9999999999996945</span><br></pre></td></tr></table></figure>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/3-2-4.png" alt="3-2-4" style="zoom:50%;" /> 
<h2 id="04、Back-Propagation">04、Back Propagation</h2>
<h3 id="4-1-引入">4.1 引入</h3>
<ul>
<li>
<p>在前面例子中，我们是如何计算 损失函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">Loss</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">oss</span></span></span></span> 对 参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的偏导数？</p>
<p>我们是直接手工计算，化简得到的。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-1-1.png" alt="4-1-1" style="zoom:50%;" /> 
<ul>
<li>
<p>当模型很简单时，这么做当然是可行的。</p>
<p>但是若模型如下图，每一层会出现很多的 参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> ，且交错复杂，此时就不可行了。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-1-2.png" alt="4-1-2" style="zoom:50%;" /> 
<h3 id="4-2-计算图">4.2 计算图</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-2-1.png" alt="4-2-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="思考：连续两层-Linear-？">思考：连续两层 Linear ？</h4>
</li>
</ul>
<blockquote>
<ul>
<li>
<p><strong>直接</strong>的连续两层 Linear 可以说是<strong>没有意义</strong>的。</p>
<p>因为实际上，可以直接化简发现，实际上一层就能达到这里所谓“两层”的效果。</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-2-2.png" alt="4-2-2" style="zoom:50%;" /> 
</li>
<li>
<p>那是什么导致实际上一层就能达到两层的效果？</p>
<p>因为 Linear 本身的特性——线性！</p>
</li>
<li>
<p>因此，为了消除这个问题，我们需要进行 <strong>非线性激活</strong>。</p>
<p>在进入下一层 Linear 之前，先进行 非线性变换，再作为下一层 Linear 的输入。</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-2-3.png" alt="4-2-3" style="zoom:50%;" /> 
<p>由于两层 Linear 之间有非线性函数存在，自然使得没办法把两次的线性变换化简为一次线性变换了。</p>
<p>由此，间接的连续两层 Linear 就有意义了。</p>
</li>
</ul>
</blockquote>
<h3 id="4-3-Chain-Rule-链式法则">4.3 Chain Rule 链式法则</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-3-1.png" alt="4-3-1" style="zoom: 50%;" /> 
<ul>
<li>链式法则的使用</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-3-2.png" alt="4-3-2" style="zoom:50%;" /> 
<ul>
<li>举例：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-3-3.png" alt="4-3-3" style="zoom:50%;" /> 
<h3 id="4-4-Linear-Model-计算图">4.4 Linear Model 计算图</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/4-4-1.png" alt="4-4-1" style="zoom:50%;" /> 
<h2 id="05、Linear-Regression-with-Pytorch">05、Linear Regression with Pytorch</h2>
<blockquote>
<p>函数解释可参考官网<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/">Pytorch Tutorial</a></p>
</blockquote>
<h3 id="5-1-流程速览">5.1 流程速览</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/5-1-1.png" alt="5-1-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="1、准备数据集">1、准备数据集</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/5-1-2.png" alt="5-1-2" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="2、设计模型">2、设计模型</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/5-1-3.png" alt="5-1-3" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="3、构建损失函数和优化器">3、构建损失函数和优化器</h4>
</li>
</ul>
 <img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/5-1-4.png" alt="5-1-4" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="4、训练轮次">4、训练轮次</h4>
<ol>
<li>forward</li>
<li>backward</li>
<li>update</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/5-1-5.png" alt="5-1-5" style="zoom:50%;" /> 
<h3 id="5-2-代码实现">5.2 代码实现</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/5-2-1.png" alt="5-2-1" style="zoom: 67%;" /> 
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">model = LinearModel()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())  <span class="comment"># 必须加 item 否则会构建计算图占用内存，多循环，内存过大</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w = &quot;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b = &quot;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([<span class="number">4.0</span>])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_pred = &quot;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">0 31.277143478393555</span><br><span class="line">1 24.81671142578125</span><br><span class="line">2 19.709630966186523</span><br><span class="line">3 15.67231273651123</span><br><span class="line">4 12.480579376220703</span><br><span class="line">5 9.957242965698242</span><br><span class="line">...</span><br><span class="line">998 0.00375506654381752</span><br><span class="line">999 0.0037370261270552874</span><br><span class="line">w =  1.9291706085205078</span><br><span class="line">b =  0.16101209819316864</span><br><span class="line">y_pred =  tensor([7.8777])</span><br></pre></td></tr></table></figure>
<h2 id="06、Logistic-Regression">06、Logistic Regression</h2>
<h3 id="6-1-分类问题">6.1 分类问题</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-1-1.png" alt="6-1-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-1-2.png" alt="6-1-2" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-1-3.png" alt="6-1-3" style="zoom:50%;" /> 
<h3 id="6-2-概率函数">6.2 概率函数</h3>
<ul>
<li>需要把数据值从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> 映射到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-2-1.png" alt="6-2-1" style="zoom:50%;" /> 
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Logistic_function">图中链接</a></li>
</ul>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-2-2.png" alt="6-2-2" style="zoom:50%;" /> 
<h3 id="6-3-逻辑斯蒂回归模型">6.3 逻辑斯蒂回归模型</h3>
<ul>
<li>
<p>实际上，就在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>a</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">Linear</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">in</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span> 层加一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">sigmoid</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span></span></span></span> 函数把值映射到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>​ 即可</p>
</li>
<li>
<h4 id="模型">模型</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-3-1.png" alt="6-3-1" style="zoom:50%;" /> 
 <img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-3-4.png" alt="6-3-4" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="损失函数">损失函数</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-3-2.png" alt="6-3-2" style="zoom:50%;" /> 
<ul>
<li>例如：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-3-3.png" alt="6-3-3" style="zoom:50%;" /> 
<ul>
<li>其中 总的 loss 函数调用为 BCELoss</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-3-5.png" alt="6-3-5" style="zoom:50%;" /> 
<h3 id="6-4-代码实现">6.4 代码实现</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-4-1.png" alt="6-4-1" style="zoom: 67%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-4-2.png" alt="6-4-2" style="zoom: 67%;" /> 
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegressionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = F.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># Forward</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0 2.551990509033203</span><br><span class="line">1 2.520275592803955</span><br><span class="line">2 2.490438222885132</span><br><span class="line">3 2.462395429611206</span><br><span class="line">4 2.4360623359680176</span><br><span class="line">...</span><br><span class="line">998 1.0626757144927979</span><br><span class="line">999 1.0621814727783203</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">200</span>)</span><br><span class="line">x_t = torch.Tensor(x).view((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y_t = model(x_t)</span><br><span class="line">y = y_t.data.numpy()</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], c=<span class="string">&#x27;r&#x27;</span>) <span class="comment"># 画一条中间红线</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;Hour&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Probability of Pass&quot;</span>)</span><br><span class="line">plt.grid()  <span class="comment"># 显示网格</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-4-3.png" alt="6-4-3" style="zoom:50%;" /> 
<blockquote>
<p>注意：训练一定要足够。</p>
<p>我第一次写错了代码，把epoch轮次写为了100，得到的比较粗糙，是下面这样的：</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/6-4-4.png" alt="6-4-4" style="zoom: 50%;" /> 
</blockquote>
<h2 id="07、Multiple-Dimension-Input">07、Multiple Dimension Input</h2>
<h3 id="7-1-多维特征数据集">7.1 多维特征数据集</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-1-1.png" alt="7-1-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-1-2.png" alt="7-1-2" style="zoom:50%;" /> 
<ul>
<li>
<p>每一次 mini-batch</p>
<p>是 N 个样本，每个样本有 8 个特征。</p>
<p>线性变换到 1 个输出。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-1-3.png" alt="7-1-3" style="zoom:50%;" /> 
<h3 id="7-2-Linear-参数">7.2 Linear 参数</h3>
<ul>
<li>第一个参数是输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> 的维度</li>
<li>第二个参数是输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span></span></span></span> 的维度</li>
</ul>
<blockquote>
<p>这里的维度，指的是<strong>特征量的维度</strong>（几个特征量）。</p>
<p>而与 一次 mini-batch 的样本数量无关。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-2-1.png" alt="7-2-1" style="zoom:50%;" /> 
<ul>
<li>神经网络还是一样做法：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-2-2.png" alt="7-2-2" style="zoom:50%;" /> 
<ul>
<li>
<p>例如</p>
<p>注意每次线性层的输入输出的特征维度。</p>
<blockquote>
<p>非线性层是不会改变维度的。</p>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-2-3.png" alt="7-2-3" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="激活函数有很多">激活函数有很多</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-5-1.png" alt="7-5-1" style="zoom: 67%;" /> 
<h3 id="7-3-示例：糖尿病预测">7.3 示例：糖尿病预测</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-3-1.png" alt="7-3-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="1、准备数据">1、准备数据</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-4-1.png" alt="7-4-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="2、设计模型-2">2、设计模型</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-4-2.png" alt="7-4-2" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="3、构建损失函数、优化器">3、构建损失函数、优化器</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-4-3.png" alt="7-4-3" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="4、训练轮次-2">4、训练轮次</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/7-4-4.png" alt="7-4-4" style="zoom:50%;" /> 
<h3 id="7-4-代码实现">7.4 代码实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">x_data = np.form_numpy(xy[:,:-<span class="number">1</span>])</span><br><span class="line">y_data = np.from_numpy(xy[:,[-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)        </span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.activate = torch.nn.Sigmoid()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))        </span><br><span class="line">        x = self.activate(self.linear3(x))   </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># Forward</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0 519.3440551757812</span><br><span class="line">1 704.1787109375</span><br><span class="line">2 1048.6502685546875</span><br><span class="line">3 955.771240234375</span><br><span class="line">...</span><br><span class="line">997 379.4390869140625</span><br><span class="line">998 375.6844177246094</span><br><span class="line">999 379.42242431640625</span><br></pre></td></tr></table></figure>
<h2 id="08、Dataset-DataLoader">08、Dataset DataLoader</h2>
<blockquote>
<p>Windows 下若使用多线程，需要注意必须加 <code>__main__</code></p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-0-1.png" alt="8-0-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-0-2.png" alt="8-0-2" style="zoom:50%;" /> 
</blockquote>
<h3 id="8-1-引入Epoch，BatchSize，Iterations">8.1 引入Epoch，BatchSize，Iterations</h3>
<ul>
<li>原本：我们是直接使用全部数据的</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-1-1.png" alt="8-1-1" style="zoom:50%;" /> 
<ul>
<li>
<p>新概念</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mtext>，</mtext><mi>B</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi><mtext>，</mtext><mi>I</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">Epoch，BatchSize，Iterations</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Ep</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">h</span><span class="mord cjk_fallback">，</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mord cjk_fallback">，</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">s</span></span></span></span></p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-1-2.png" alt="8-1-2" style="zoom:50%;" /> 
<ul>
<li>示例</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-1-3.png" alt="8-1-3" style="zoom:50%;" /> 
<h3 id="8-2-实现流程">8.2 实现流程</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-2-1.png" alt="8-2-1" style="zoom:50%;" /> 
<blockquote>
<ul>
<li>必须实现以上三个函数，才能实例化 Dataset（Dataset是抽象类，本身无法被实例化为对象）</li>
<li>DataLoader参数
<ul>
<li>dataset 是 Dataset 对象</li>
<li>batch_size 是 批次大小</li>
<li>shuffle 是每次批次抽取前是否打乱分批（一般train时True，test时False）</li>
<li>num_works 是多线程数目</li>
</ul>
</li>
</ul>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-2-2.png" alt="8-2-2" style="zoom: 67%;" /> 
<ul>
<li>
<h4 id="完整流程">完整流程</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-2-3.png" alt="8-2-3" style="zoom:67%;" /> 
<h3 id="8-3-代码实现">8.3 代码实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiabetesDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype= np.float32)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        self.x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line">    </span><br><span class="line">dataset = DiabetesDataset(<span class="string">&quot;diabetes.csv.gz&quot;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)        </span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.activate = torch.nn.Sigmoid()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))        </span><br><span class="line">        x = self.activate(self.linear3(x))   </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># Forward</span></span><br><span class="line">        y_pred = model(inputs)</span><br><span class="line">        loss = criterion(y_pred, labels)</span><br><span class="line">        <span class="built_in">print</span>(epoch, i, loss.item())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update</span></span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里运行出错，不知</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: DataLoader worker (pid(s) 24432, 20608) exited unexpectedly</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="8-4-可用数据集">*8.4 可用数据集</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-3-1.png" alt="8-3-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="使用实例">使用实例</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-3-2.png" alt="8-3-2" style="zoom:50%;" /> 
<h3 id="8-5-课后题">*8.5 课后题</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/titanic/data">Kaggle 课后题</a></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/8-4-1.png" alt="8-4-1" style="zoom: 33%;" /> 
<h2 id="09、Softmax-Classifier">09、Softmax Classifier</h2>
<h3 id="9-1-引入">9.1 引入</h3>
<ul>
<li>
<h4 id="对于糖尿病预测">对于糖尿病预测</h4>
<p>只有两种可能：</p>
<ul>
<li>患病</li>
<li>不患病</li>
</ul>
<p>这是二分类问题。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-1-1.png" alt="9-1-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="对于数字识别-MNIST-数据集">对于数字识别 MNIST 数据集</h4>
<p>有 10 个类别，不再是二分类问题了。</p>
<p>如何设计神经网络？</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-1-2.png" alt="9-1-2" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="直接使用-sigmoid-控制-10-个输出吗？">直接使用 sigmoid 控制 10 个输出吗？</h4>
<p>这是不合理的，因为我们是希望同一个样本的10个输出应该是互相影响且是互相抑制的关系。</p>
<p>或者换句话说，我们希望输出是一个分布。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-1-3.png" alt="9-1-3" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="使用-Softmax-输出预测的分布">使用 Softmax 输出预测的分布</h4>
<p>这 10 个输出都是一个概率。</p>
<p>且满足 概率均非负，且和为1。这符合概率分布的要求。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-1-4.png" alt="9-1-4" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="Softmax-函数">Softmax 函数</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-2-1.png" alt="9-2-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="Softmax-层">Softmax 层</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-2-2.png" alt="9-2-2" style="zoom:50%;" /> 
<h3 id="9-2-损失函数">9.2 损失函数</h3>
<ul>
<li>
<h4 id="NULLLoss">NULLLoss</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-3-1.png" alt="9-3-1" style="zoom:50%;" /> 
<ul>
<li>
<h3 id="CrossEntropyLoss-交叉熵">CrossEntropyLoss 交叉熵</h3>
</li>
</ul>
<blockquote>
<p>可以直接把从 softmax 层往后都结合，这就是 一个 CrossEntropyLoss 层。</p>
<p>注：以下只能二选一</p>
<ol>
<li>选用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>U</mi><mi>L</mi><mi>L</mi><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">NULLLoss</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">ULLL</span><span class="mord mathnormal">oss</span></span></span></span> ，则需要多添加前面的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">softmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span> 并再求出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\log \hat{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1412em;vertical-align:-0.1944em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>，再和真实的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span> 进行乘积得到 损失函数值。</li>
<li>选用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>E</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">CrossEntropyLoss</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">ross</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ro</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">L</span><span class="mord mathnormal">oss</span></span></span></span> ，前面直接是激活函数 sigmoid 即可。</li>
</ol>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-3-2.png" alt="9-3-2" style="zoom:50%;" /> 
<ul>
<li>
<p>示例：使用 CrossEntropyLoss 交叉熵</p>
<p>可以看到，的确很有效地反映损失情况。</p>
<p>值越小，损失越小。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-3-3.png" alt="9-3-3" style="zoom:50%;" /> 
<h3 id="9-3-练习1">*9.3 练习1</h3>
<ul>
<li>
<h4 id="Exercise-9-1-CrossEntropyLoss-vs-NLLLoss">Exercise 9-1: CrossEntropyLoss vs NLLLoss</h4>
</li>
</ul>
<ol>
<li>
<p>两者有什么区别？</p>
</li>
<li>
<p>阅读：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss">CrossEntropyLoss</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss">NLLLoss</a></li>
</ul>
</li>
<li>
<p>尝试理解</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>E</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>log</mi><mo>⁡</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>+</mo><mi>N</mi><mi>L</mi><mi>L</mi><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">CrossEntropyLoss = \log softmax + NLLLoss</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">ross</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ro</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">L</span><span class="mord mathnormal">oss</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">LLL</span><span class="mord mathnormal">oss</span></span></span></span></p>
</li>
</ol>
<h3 id="9-4-示例：数字识别">9.4 示例：数字识别</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-4-1.png" alt="9-4-1" style="zoom: 50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-4-2.png" alt="9-4-2" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="仍然是基本流程">仍然是基本流程</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-4-3.png" alt="9-4-3" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="本次，我们需要使用到的注意点有：">本次，我们需要使用到的注意点有：</h4>
</li>
</ul>
<ol>
<li>激活函数采用 ReLU</li>
<li>使用 Dataset 和 Dataloader</li>
<li>使用图片作为输入（后面讲解）</li>
</ol>
<h4 id="1、准备数据-2">1、准备数据</h4>
<ul>
<li>
<h4 id="图片输入问题：数值变换-transform">图片输入问题：数值变换 transform</h4>
<p>这里我们的一张照片是 28*28 = 784 个像素点，</p>
<p>每个像素点是的值 pixel 都是 0~255 中的一个整数。</p>
<p>因此，我们映射需要到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 上。</p>
<blockquote>
<p>为什么要映射到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 上？</p>
<p>因为神经网络往往对 值域为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 的分布有更好的学习能力。</p>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-5-1.png" alt="9-5-1" style="zoom:50%;" /> 
<ul>
<li>
<p>如何映射？</p>
<p>按照概率论的分布理论，就是将分布标准化（归一化）</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">x&#x27; = \frac{x-\mu}{\sigma}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8019em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.9463em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<blockquote>
<p>下图中的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.1307</mn></mrow><annotation encoding="application/x-tex">0.1307</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.1307</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.3081</mn></mrow><annotation encoding="application/x-tex">0.3081</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.3081</span></span></span></span> 怎么来的？</p>
<p>经验之谈，一般对于像素点值的均值和方差，就是这个值。</p>
<p>故采用这两个值来进行标准化。</p>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-5-2.png" alt="9-5-2" style="zoom:50%;" /> 
<h4 id="2、设计模型-3">2、设计模型</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-5-3.png" alt="9-5-3" style="zoom: 67%;" /> 
<h4 id="3、构建损失函数、优化器-2">3、构建损失函数、优化器</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-5-4.png" alt="9-5-4" style="zoom:67%;" /> 
<h4 id="4、训练轮次-3">4、训练轮次</h4>
<ul>
<li>
<h5 id="train-训练">train 训练</h5>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-5-5.png" alt="9-5-5" style="zoom: 67%;" /> 
<ul>
<li>
<h5 id="tets-测试">tets 测试</h5>
<p>测试下是不需要计算梯度的。（因为不进行反向传播和优化）</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-5-6.png" alt="9-5-6" style="zoom: 67%;" /> 
<ul>
<li>
<h5 id="运行">运行</h5>
</li>
</ul>
 <img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/9-5-7.png" alt="9-5-7" style="zoom: 67%;" /> 
<h3 id="9-5-代码实现">9.5 代码实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(dataset=train_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.l1 = torch.nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.l2 = torch.nn.Linear(<span class="number">512</span>, <span class="number">256</span>)        </span><br><span class="line">        self.l3 = torch.nn.Linear(<span class="number">256</span>, <span class="number">128</span>)        </span><br><span class="line">        self.l4 = torch.nn.Linear(<span class="number">128</span>, <span class="number">64</span>)        </span><br><span class="line">        self.l5 = torch.nn.Linear(<span class="number">64</span>, <span class="number">10</span>)        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        x = F.relu(self.l1(x))</span><br><span class="line">        x = F.relu(self.l2(x))        </span><br><span class="line">        x = F.relu(self.l3(x))        </span><br><span class="line">        x = F.relu(self.l4(x))</span><br><span class="line">        <span class="keyword">return</span> self.l5(x)</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Forward + Backward + Update</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_i % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[%d, %5d] loss: %.3f&quot;</span>%(epoch+<span class="number">1</span>, batch_i+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: %d %%&quot;</span>%(<span class="number">100</span> * correct / total))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="9-6-练习2">*9.6 练习2</h3>
<ul>
<li>
<h4 id="Exercise-9-2-Classifier-Implementation">Exercise 9-2: Classifier Implementation</h4>
</li>
</ul>
<ol>
<li>
<p>尝试实现一个分类器</p>
<p>Otto Group Product Classification Challenge</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/otto-group-product-classification-challenge/data">数据集</a></p>
</li>
</ol>
<h2 id="10、Basic-CNN">10、Basic CNN</h2>
<h3 id="10-1-引入">10.1 引入</h3>
<ul>
<li>
<h4 id="回顾之前：全连接网络">回顾之前：全连接网络</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-1-1.png" alt="10-1-1" style="zoom: 67%;" /> 
<ul>
<li>
<h3 id="本次需求：卷积神经网络">本次需求：卷积神经网络</h3>
<p>Convolutional Neural Network 卷积神经网络</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-1-2.png" alt="10-1-2" style="zoom:67%;" /> 
<h3 id="10-2-卷积">10.2 卷积</h3>
<ul>
<li>
<h4 id="介绍卷积操作">介绍卷积操作</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-2-1.png" alt="10-2-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="计算过程如下">计算过程如下</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-2-2.png" alt="10-2-2" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="若输入为-三通道">若输入为 三通道</h4>
<p>计算过程：</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-2-3.png" alt="10-2-3" style="zoom: 67%;" /> 
<ul>
<li>架构情况：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-2-4.png" alt="10-2-4" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="输入-N-通道，输出-M-通道">输入 N 通道，输出 M 通道</h4>
</li>
</ul>
<blockquote>
<p>做法是：</p>
<ol>
<li>做 <strong>M 次</strong>如下的 filter 卷积（每次的 filter1不一样，即 kernel 不一样）</li>
<li>每一次的 filter 卷积都是一次 <strong>N 通道输入，1 通道输出</strong>。（即前面的方法）</li>
<li>得到的所有 结果，在 <strong>channel 维度上连接</strong> 起来即可。</li>
</ol>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-2-5.png" alt="10-2-5" style="zoom:67%;" /> 
<blockquote>
<p>具体代码做法如下：</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-2-6.png" alt="10-2-6" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-2-7.png" alt="10-2-7" style="zoom:67%;" /> 
</blockquote>
<h3 id="10-3-卷积参数">10.3 卷积参数</h3>
<ul>
<li>
<h3 id="padding-外框">padding 外框</h3>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-3-1-1.png" alt="10-3-1-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-3-1-2.png" alt="10-3-1-2" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="stride-步长">stride 步长</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-3-2-1.png" alt="10-3-2-1" style="zoom: 50%;" /> 
 <img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-3-2-2.png" alt="10-3-2-2" style="zoom:67%;" /> 
<h3 id="10-4-Max-Pool-最大池化">10.4 Max Pool 最大池化</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-4-1-1.png" alt="10-4-1-1" style="zoom: 50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-4-1-2.png" alt="10-4-1-2" style="zoom: 67%;" /> 
<h3 id="10-5-示例：数字识别">10.5 示例：数字识别</h3>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-5-1.png" alt="10-5-1" style="zoom:67%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-5-2.png" alt="10-5-2" style="zoom:67%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-5-3.png" alt="10-5-3" style="zoom:67%;" /> 
<h3 id="10-6-使用GPU">*10.6 使用GPU</h3>
<ul>
<li>
<p>代码仍然不变</p>
<p>只需要加上一些内容即可</p>
</li>
<li>
<p>添加</p>
<ol>
<li>定义 CPU 设备</li>
<li>将 model 移动到 GPU 上计算</li>
<li>将训练的数据输入输出 inputs、outputs 移动到 GPU 上</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-6-1.png" alt="10-6-1" style="zoom: 67%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-6-2.png" alt="10-6-2" style="zoom:67%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-6-3.png" alt="10-6-3" style="zoom:67%;" /> 
<h3 id="10-7-练习">*10.7 练习</h3>
<ul>
<li>
<h4 id="Exercise-10-1-Try-a-more-complex-CNN-Try-a-more-complex-CNN">Exercise 10-1: Try a more complex CNN / Try a more complex CNN</h4>
</li>
</ul>
<ol>
<li>设计一个更复杂的卷积神经网络
<ul>
<li>3个卷积层</li>
<li>3个非线性激活层</li>
<li>3个最大池化层</li>
<li>3个线性层</li>
</ul>
</li>
<li>比较不同卷积神经网络的性能</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/10-7-1.png" alt="10-7-1" style="zoom:67%;" /> 
<h2 id="11、Advanced-CNN">11、Advanced CNN</h2>
<h3 id="11-1-引入">11.1 引入</h3>
<ul>
<li>
<p>复杂的卷积神经网络，如下图的 GoogLeNet</p>
<p>层次很多，但是会发现，有一些结构是相同的！</p>
<p>这样有两个好处：</p>
<ol>
<li>减少代码冗余，提高代码复用性</li>
<li>代码可读性提高，后期修改更方便</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-1-1.png" alt="11-1-1" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="Incetion-Modulde">Incetion Modulde</h4>
<p>如盗梦空间一样，这里的重复相同的结构，</p>
<p>我们把他看作是神经网络中的神经网络，一般叫做 Inception Module 。</p>
</li>
</ul>
 <img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-1-2.png" alt="11-1-2" style="zoom:67%;" />
<h3 id="11-2-1x1-卷积">11.2 1x1 卷积</h3>
<ul>
<li>
<p>为什么会需要 1x1 的卷积？有什么效果？</p>
</li>
<li>
<h4 id="计算过程">计算过程</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-2-1.png" alt="11-2-1" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="对比时间复杂度">对比时间复杂度</h4>
<p>卷积神经网络往往对于计算需求很高，这也是当前的一个难点。</p>
<p>因此，若能降低计算次数，但是达到几乎相同的效果，那当然是最好的。</p>
<p>对比下面 使用与不使用 1x1卷积的相同输入输出结构，计算次数降低到了 1/10。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-2-2.png" alt="11-2-2" style="zoom:67%;" /> 
<h3 id="11-3-Inception-Module">11.3 Inception Module</h3>
<ul>
<li>继续说，这个神经网络中的神经网络 Inception Module</li>
</ul>
<blockquote>
<ul>
<li>
<h5 id="我的思考：它在做什么？意义何在？">我的思考：它在做什么？意义何在？</h5>
<ul>
<li>
<p>从图可以看出来，它在并行的计算4种不一样的神经网络架构，最后把4种结构连接起来。</p>
</li>
<li>
<p>仔细思考，神经网络本身是做什么的？神经网络本质上是在寻找使得损失函数最小的参数。</p>
<p>所以说，只要是涉及参数的问题，在理论上来说，都是可以交给神经网络自己去学习的。</p>
<p>但是，模型定义是你做的呀！你怎么在训练之前就能知道定义什么样的卷积神经网络呢？（即，你怎么知道该使用多大的 kernel 呢？）</p>
</li>
<li>
<p>回过头来，你再看这个 Inception Module</p>
<p>实际上，它就是希望神经网络自己去找到最好的 合适大小的 kernel。</p>
<p>而我们是把不同的 kernel 并行的交给了它，最后合并，而这个合并的过程，就是它在自己学习到最好的 kernel （因为 1x1 卷积本身是有权值的）</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="架构分析">架构分析</h4>
<ul>
<li>
<h4 id="整体流程图">整体流程图</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-3-3.png" alt="11-3-3" style="zoom: 67%;" /> 
<ul>
<li>
<h4 id="1：并行计算">1：并行计算</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-3-1.png" alt="11-3-1" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="2：连接合并">2：连接合并</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-3-2.png" alt="11-3-2" style="zoom:67%;" /> 
<h4 id="代码使用">代码使用</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-3-4.png" alt="11-3-4" style="zoom:67%;" /> 
<h4 id="结果">结果</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-3-5.png" alt="11-3-5" style="zoom:67%;" /> 
<h3 id="11-4-更深！">11.4 更深！</h3>
<ul>
<li>
<p>按照一般逻辑来看，</p>
<p>当神经网络越深（层次更多），那么对数据的处理细节会更多，自然应该更能达到高效损失地贴合数据。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-4-1.png" alt="11-4-1" style="zoom:67%;" /> 
<ul>
<li>
<p>但是，事实上</p>
<p>白部署完全遵循这个逻辑的，如下图。</p>
<p>实际上，你学的太好了，反而可能是把噪声也学进去了——过拟合。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-4-2.png" alt="11-4-2" style="zoom:67%;" /> 
<blockquote>
<p>图中底部文字：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:770-778.</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="11-5-Residual-Net">11.5 Residual Net</h3>
<h4 id="原理">原理</h4>
<ul>
<li>
<p><strong>残差块</strong> 解决 <strong>梯度消失</strong> 问题</p>
<p>我们知道，虽然 梯度下降 算法在应用中实际上并不会存在局部与全局最优点的问题。</p>
<p>但是，梯度消失问题的存在的。</p>
<p>（梯度近乎为0，导致卡在该点无法行动。前面我们的方案是采用SGD，期望噪声帮我们跃迁过去。）</p>
</li>
<li>
<p>这里，我们采用在输出之后，在非线性激活之前，直接加一个输入 x 。</p>
<p>这样子，梯度由围绕0改为了围绕1，自然不会有梯度消失。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-5-1.png" alt="11-5-1" style="zoom:67%;" /> 
<ul>
<li>试看如下</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-5-2.png" alt="11-5-2" style="zoom:67%;" /> 
<h4 id="简单示例">简单示例</h4>
<ul>
<li>
<h4 id="架构">架构</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-5-3.png" alt="11-5-3" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="Residual-操作">Residual 操作</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-5-4.png" alt="11-5-4" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="代码流程">代码流程</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-5-5.png" alt="11-5-5" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="测试结果">测试结果</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-5-6.png" alt="11-5-6" style="zoom:67%;" /> 
<h3 id="11-6-练习">*11.6 练习</h3>
<ul>
<li>
<h4 id="Exercise-11-1-Reading-Paper-and-Implementing">Exercise 11-1: Reading Paper and Implementing</h4>
</li>
</ul>
<ol>
<li>
<p>下面这篇论文中，有很多关于 Resiudal 操作的示例，尝试阅读执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">He K, Zhang X, Ren S, et al. Identity Mappings in Deep Residual Networks[C]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-6-1.png" alt="11-6-1" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="Exercise-11-2-Reading-and-Implementing-DenseNet">Exercise 11-2: Reading and Implementing DenseNet</h4>
</li>
</ul>
<ol>
<li>
<p>阅读并执行下文的 DenseNet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks[J]. 2016:2261-2269.</span><br></pre></td></tr></table></figure>
</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/11-6-2.png" alt="11-6-2" style="zoom:67%;" /> 
<h2 id="12、Basic-RNN">12、Basic RNN</h2>
<ul>
<li>特点：
<ol>
<li>序列 Data</li>
<li>循环共享权值</li>
</ol>
</li>
</ul>
<h3 id="12-1-引入">12.1 引入</h3>
<ul>
<li>
<h4 id="什么是RNN？">什么是RNN？</h4>
<p>Recurrent Neural Network 循环神经网络</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-1-1.png" alt="12-1-1" style="zoom:67%;" /> 
<ul>
<li>
<h4 id="RNN单元-计算过程">RNN单元 计算过程</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-1-2.png" alt="12-1-2" style="zoom:67%;" /> 
<h3 id="12-2-使用方法1：RNNCell">12.2 使用方法1：RNNCell</h3>
<h4 id="使用内容示意">使用内容示意</h4>
<p>RNNCell 是只一次的单元计算，需要你自己写循环，把 RNNCell 串起来。</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-2-1.png" alt="12-2-1" style="zoom: 67%;" /> 
<h4 id="参数说明">参数说明</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-2-4.png" alt="12-2-4" style="zoom: 50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-2-2.png" alt="12-2-2" style="zoom: 50%;" /> 
<h4 id="代码示例">代码示例</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-2-3.png" alt="12-2-3" style="zoom:67%;" /> 
<h3 id="12-3-使用方法2：RNN">12.3 使用方法2：RNN</h3>
<h4 id="使用内容示意-2">使用内容示意</h4>
<p>RNN 是直接把多个 RNNCell 循环都部署好了，不需要你手动写循环。</p>
<p>但也因此，你的input_size 和 hidden_size 维度是比前面Cell 的那种要高一个维度，用来指示多少个Cell 运算。</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-3-1.png" alt="12-3-1" style="zoom:67%;" /> 
<blockquote>
<p>其中 num_layers 参数是指同一输入的处理层数，如下是多层的情况：</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-3-4.png" alt="12-3-4" style="zoom:67%;" /> 
</blockquote>
<h4 id="参数说明-2">参数说明</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-3-2.png" alt="12-3-2" style="zoom: 50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-3-3.png" alt="12-3-3" style="zoom: 50%;" /> 
<blockquote>
<p>这里还有一个参数，batch_first，表示是否把 batch_size 放在第一个维度（本来应该是第二个）</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-4-1.png" alt="12-4-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-4-2.png" alt="12-4-2" style="zoom: 50%;" /> 
</blockquote>
<h4 id="代码示例-2">代码示例</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-3-5.png" alt="12-3-5" style="zoom:67%;" /> 
<h3 id="12-4-示例：训练字符串输出">12.4 示例：训练字符串输出</h3>
<ul>
<li>
<h4 id="目标任务">目标任务</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-5-1.png" alt="12-5-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="数据预处理">数据预处理</h4>
<p>RNN 只能处理数字，因此我们建立字母到数字的索引字典。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-5-2.png" alt="12-5-2" style="zoom: 50%;" /> 
<ul>
<li>
<h4 id="损失函数-2">损失函数</h4>
<p>采用 交叉熵 CrossEntropyLoss</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-5-3.png" alt="12-5-3" style="zoom: 50%;" /> 
<h4 id="完整代码">完整代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">暂无</span><br></pre></td></tr></table></figure>
<h3 id="12-5-Embedding-嵌入层">12.5 Embedding 嵌入层</h3>
<h4 id="原理-2">原理</h4>
<ul>
<li>
<h4 id="关联字符与数字的方法">关联字符与数字的方法</h4>
<ol>
<li>
<p>独热编码</p>
<p>特点： 高维度、稀疏、硬编码。</p>
</li>
<li>
<p>Embedding 嵌入层</p>
<p>特点：低维度、密集、学习自数据。</p>
<p>方法2更好。</p>
</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-6-1.png" alt="12-6-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-6-2.png" alt="12-6-2" style="zoom: 50%;" /> 
<blockquote>
<ul>
<li>
<h4 id="Embedding-层计算过程">Embedding 层计算过程</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-6-3.png" alt="12-6-3" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-6-4.png" alt="12-6-4" style="zoom:50%;" /> 
</blockquote>
<h4 id="使用架构">使用架构</h4>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-6-5.png" alt="12-6-5" style="zoom:67%;" /> 
<h3 id="12-6-拓展：LSTM与GRU">12.6 拓展：LSTM与GRU</h3>
<ul>
<li>
<h4 id="LSTM">LSTM</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-7-1.png" alt="12-7-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-7-2.png" alt="12-7-2" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="GRU">GRU</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-7-3.png" alt="12-7-3" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/12-7-4.png" alt="12-7-4" style="zoom:50%;" /> 
<h2 id="13、RNN-Classifier">13、RNN Classifier</h2>
<h3 id="13-1-引入">13.1 引入</h3>
<ul>
<li>
<h4 id="姓名对应国家分类问题">姓名对应国家分类问题</h4>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-1-1.png" alt="13-1-1" style="zoom:50%;" /> 
<ul>
<li>
<h4 id="设计模型">设计模型</h4>
</li>
<li>
<p>本来可以仍然设计如下，</p>
<p>但是，我们实际上并不需要每次都输出 o（即便输出，我们也不知道是什么，也没有对应的 label ，没意义），</p>
<p>只有最后一次的输出 o 才是有效的。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-1-2.png" alt="13-1-2" style="zoom:50%;" /> 
<ul>
<li>因此，我们可以去除，只留下最后一次的输出。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-1-3.png" alt="13-1-3" style="zoom:50%;" /> 
<ul>
<li>另外，我们改用 RGU 进行：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-1-4.png" alt="13-1-4" style="zoom: 50%;" /> 
<h3 id="13-2-设计模型">13.2 设计模型</h3>
<h4 id="数据预处理-2">数据预处理</h4>
<p>同理，无法处理字符串，我们需要先建立对应的数字索引，用来处理。</p>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-2-1.png" alt="13-2-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-2-2.png" alt="13-2-2" style="zoom:50%;" /> 
<h4 id="Bi-direction-参数">Bi-direction 参数</h4>
<ul>
<li>
<p>是否同时也进行反向的检测（不是反向传播）</p>
<p>若是，则输出output 会是每一次的左右两向的拼接的 h，</p>
<p>同时需要同时提供 左右两向的初始 h0f、b0b，</p>
<p>输出也会有 hNf 、 hNb两个。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-3-1.png" alt="13-3-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-3-2.png" alt="13-3-2" style="zoom:50%;" /> 
<h4 id="完整代码-2">完整代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">暂无</span><br></pre></td></tr></table></figure>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-4-1.png" alt="13-4-1" style="zoom:50%;" /> 
<h3 id="13-3-练习题">*13.3 练习题</h3>
<ul>
<li>
<h4 id="Exercise-13-1-Sentiment-Analysis-on-Movie-Reviews">Exercise 13-1 Sentiment Analysis on Movie Reviews</h4>
</li>
</ul>
<ol>
<li>
<p>分析《烂番茄》电影评论的情感程度。</p>
<p>数据集：<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data">Kaggle 数据集</a></p>
<p>该数据集由tab分隔的文件组成，其中包含来自烂番茄数据集的短语。</p>
</li>
<li>
<p>情感标签有：</p>
<ul>
<li>0–消极</li>
<li>1–有点消极</li>
<li>2–中性</li>
<li>3–有点积极</li>
<li>4–积极</li>
</ul>
</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-5-1.png" alt="13-5-1" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-5-2.png" alt="13-5-2" style="zoom:50%;" /> 
<img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/13-5-3.png" alt="13-5-3" style="zoom:50%;" /> 
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://isSeymour.github.io/butterflyblog">isSeymour</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://isseymour.github.io/butterflyblog/2024/03/31/PytorchTutorial/">https://isseymour.github.io/butterflyblog/2024/03/31/PytorchTutorial/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://isSeymour.github.io/butterflyblog" target="_blank">isSeymour</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/butterflyblog/tags/Python/">Python</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/PageCover.png" data-sites="wechat,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY1.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY1.jpg" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY2.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY2.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/butterflyblog/2024/03/02/LaTeX-Math/" title="LaTeX数学公式总览"><img class="cover" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/img/latex/latex0.jpg" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">LaTeX数学公式总览</div></div></a></div><div class="next-post pull-right"><a href="/butterflyblog/2024/05/01/Crypto-Graphic/" title="《图解密码技术》Notes"><img class="cover" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/Crypto-Graphic/P0.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">《图解密码技术》Notes</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/butterflyblog/2024/01/31/PY-Matplotlib/" title="《Matplotlib》Notes"><img class="cover" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/Matplotlib/Matplotlib.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-31</div><div class="title">《Matplotlib》Notes</div></div></a></div><div><a href="/butterflyblog/2024/02/01/PY-PyTorch/" title="《PyTorch入门教程》"><img class="cover" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PyTorchFirst/PyTorch00.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-01</div><div class="title">《PyTorch入门教程》</div></div></a></div><div><a href="/butterflyblog/2024/01/30/PY-SciPy/" title="《SciPy》Notes"><img class="cover" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/Matplotlib/SciPy.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">《SciPy》Notes</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/T6.jpg" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">isSeymour</div><div class="author-info__description">志之所趋，无远弗届，穷山距海，不能限也。</div></div><div class="card-info-data site-data is-center"><a href="/butterflyblog/archives/"><div class="headline">文章</div><div class="length-num">48</div></a><a href="/butterflyblog/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/butterflyblog/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isSeymour/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://isSeymour.github.io/profile/" target="_blank" title="学术主页"><i class="fa-regular fa-address-card" style="color: #000000;"></i></a><a class="social-icon" href="https://github.com/isSeymour/" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="https://space.bilibili.com/79699613/" target="_blank" title="B站"><i class="fa-brands fa-bilibili" style="color: #000000;"></i></a><a class="social-icon" href="https://blog.csdn.net/m0_63205991/" target="_blank" title="CSDN"><i class="fa-solid fa-code" style="color: #000000;"></i></a><a class="social-icon" href="mailto:isSeymour@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">1.建议使用科学上网！ 2.评论区已关闭。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">《Pytorch Tutorial》Notes（初稿）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#01%E3%80%81Overview"><span class="toc-text">01、Overview</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%BC%95%E5%85%A5"><span class="toc-text">1.1 引入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E4%B8%8E%E6%8E%A8%E7%90%86"><span class="toc-text">预测与推理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8E%86%E5%8F%B2"><span class="toc-text">1.2 分类与历史</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#scikit-learn"><span class="toc-text">scikit-learn</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%86%E5%8F%B2"><span class="toc-text">历史</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%9D%E8%80%83%EF%BC%9A%E9%99%8D%E7%BB%B4"><span class="toc-text">思考：降维</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">前馈与反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%A8%A1%E5%9E%8B"><span class="toc-text">一些模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%BC%80%E5%A7%8B%E5%90%A7"><span class="toc-text">1.3 开始吧</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#02%E3%80%81Linear-Model"><span class="toc-text">02、Linear Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%BC%95%E5%85%A5"><span class="toc-text">2.1 引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">2.2 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">2.3 代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E8%AF%BE%E5%90%8E%E9%A2%98"><span class="toc-text">*2.4 课后题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#03%E3%80%81Gradient-Descent"><span class="toc-text">03、Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-text">3.1 优化问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-text">3.2 梯度下降算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">3.3 代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.4 随机梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#04%E3%80%81Back-Propagation"><span class="toc-text">04、Back Propagation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%BC%95%E5%85%A5"><span class="toc-text">4.1 引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">4.2 计算图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%9D%E8%80%83%EF%BC%9A%E8%BF%9E%E7%BB%AD%E4%B8%A4%E5%B1%82-Linear-%EF%BC%9F"><span class="toc-text">思考：连续两层 Linear ？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Chain-Rule-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-text">4.3 Chain Rule 链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Linear-Model-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">4.4 Linear Model 计算图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#05%E3%80%81Linear-Regression-with-Pytorch"><span class="toc-text">05、Linear Regression with Pytorch</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%B5%81%E7%A8%8B%E9%80%9F%E8%A7%88"><span class="toc-text">5.1 流程速览</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">1、准备数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-text">2、设计模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E6%9E%84%E5%BB%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">3、构建损失函数和优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81%E8%AE%AD%E7%BB%83%E8%BD%AE%E6%AC%A1"><span class="toc-text">4、训练轮次</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">5.2 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#06%E3%80%81Logistic-Regression"><span class="toc-text">06、Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">6.1 分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%A6%82%E7%8E%87%E5%87%BD%E6%95%B0"><span class="toc-text">6.2 概率函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-text">6.3 逻辑斯蒂回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-text">模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">6.4 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#07%E3%80%81Multiple-Dimension-Input"><span class="toc-text">07、Multiple Dimension Input</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">7.1 多维特征数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Linear-%E5%8F%82%E6%95%B0"><span class="toc-text">7.2 Linear 参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%9C%89%E5%BE%88%E5%A4%9A"><span class="toc-text">激活函数有很多</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-%E7%A4%BA%E4%BE%8B%EF%BC%9A%E7%B3%96%E5%B0%BF%E7%97%85%E9%A2%84%E6%B5%8B"><span class="toc-text">7.3 示例：糖尿病预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-text">1、准备数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B-2"><span class="toc-text">2、设计模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E6%9E%84%E5%BB%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">3、构建损失函数、优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81%E8%AE%AD%E7%BB%83%E8%BD%AE%E6%AC%A1-2"><span class="toc-text">4、训练轮次</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">7.4 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#08%E3%80%81Dataset-DataLoader"><span class="toc-text">08、Dataset DataLoader</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E5%BC%95%E5%85%A5Epoch%EF%BC%8CBatchSize%EF%BC%8CIterations"><span class="toc-text">8.1 引入Epoch，BatchSize，Iterations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B"><span class="toc-text">8.2 实现流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B"><span class="toc-text">完整流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">8.3 代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">*8.4 可用数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B"><span class="toc-text">使用实例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-%E8%AF%BE%E5%90%8E%E9%A2%98"><span class="toc-text">*8.5 课后题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#09%E3%80%81Softmax-Classifier"><span class="toc-text">09、Softmax Classifier</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-%E5%BC%95%E5%85%A5"><span class="toc-text">9.1 引入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E7%B3%96%E5%B0%BF%E7%97%85%E9%A2%84%E6%B5%8B"><span class="toc-text">对于糖尿病预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB-MNIST-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">对于数字识别 MNIST 数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8-sigmoid-%E6%8E%A7%E5%88%B6-10-%E4%B8%AA%E8%BE%93%E5%87%BA%E5%90%97%EF%BC%9F"><span class="toc-text">直接使用 sigmoid 控制 10 个输出吗？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-Softmax-%E8%BE%93%E5%87%BA%E9%A2%84%E6%B5%8B%E7%9A%84%E5%88%86%E5%B8%83"><span class="toc-text">使用 Softmax 输出预测的分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Softmax-%E5%87%BD%E6%95%B0"><span class="toc-text">Softmax 函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Softmax-%E5%B1%82"><span class="toc-text">Softmax 层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">9.2 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#NULLLoss"><span class="toc-text">NULLLoss</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrossEntropyLoss-%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-text">CrossEntropyLoss 交叉熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-%E7%BB%83%E4%B9%A01"><span class="toc-text">*9.3 练习1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Exercise-9-1-CrossEntropyLoss-vs-NLLLoss"><span class="toc-text">Exercise 9-1: CrossEntropyLoss vs NLLLoss</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-%E7%A4%BA%E4%BE%8B%EF%BC%9A%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="toc-text">9.4 示例：数字识别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8D%E7%84%B6%E6%98%AF%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-text">仍然是基本流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AC%E6%AC%A1%EF%BC%8C%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E4%BD%BF%E7%94%A8%E5%88%B0%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9%E6%9C%89%EF%BC%9A"><span class="toc-text">本次，我们需要使用到的注意点有：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE-2"><span class="toc-text">1、准备数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E8%BE%93%E5%85%A5%E9%97%AE%E9%A2%98%EF%BC%9A%E6%95%B0%E5%80%BC%E5%8F%98%E6%8D%A2-transform"><span class="toc-text">图片输入问题：数值变换 transform</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B-3"><span class="toc-text">2、设计模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E6%9E%84%E5%BB%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8-2"><span class="toc-text">3、构建损失函数、优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81%E8%AE%AD%E7%BB%83%E8%BD%AE%E6%AC%A1-3"><span class="toc-text">4、训练轮次</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#train-%E8%AE%AD%E7%BB%83"><span class="toc-text">train 训练</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tets-%E6%B5%8B%E8%AF%95"><span class="toc-text">tets 测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C"><span class="toc-text">运行</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-5-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">9.5 代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-6-%E7%BB%83%E4%B9%A02"><span class="toc-text">*9.6 练习2</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Exercise-9-2-Classifier-Implementation"><span class="toc-text">Exercise 9-2: Classifier Implementation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10%E3%80%81Basic-CNN"><span class="toc-text">10、Basic CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-%E5%BC%95%E5%85%A5"><span class="toc-text">10.1 引入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E9%A1%BE%E4%B9%8B%E5%89%8D%EF%BC%9A%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C"><span class="toc-text">回顾之前：全连接网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%AC%A1%E9%9C%80%E6%B1%82%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">本次需求：卷积神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-%E5%8D%B7%E7%A7%AF"><span class="toc-text">10.2 卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-text">介绍卷积操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%A6%82%E4%B8%8B"><span class="toc-text">计算过程如下</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8B%A5%E8%BE%93%E5%85%A5%E4%B8%BA-%E4%B8%89%E9%80%9A%E9%81%93"><span class="toc-text">若输入为 三通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5-N-%E9%80%9A%E9%81%93%EF%BC%8C%E8%BE%93%E5%87%BA-M-%E9%80%9A%E9%81%93"><span class="toc-text">输入 N 通道，输出 M 通道</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-%E5%8D%B7%E7%A7%AF%E5%8F%82%E6%95%B0"><span class="toc-text">10.3 卷积参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#padding-%E5%A4%96%E6%A1%86"><span class="toc-text">padding 外框</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#stride-%E6%AD%A5%E9%95%BF"><span class="toc-text">stride 步长</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-Max-Pool-%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96"><span class="toc-text">10.4 Max Pool 最大池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-5-%E7%A4%BA%E4%BE%8B%EF%BC%9A%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="toc-text">10.5 示例：数字识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-6-%E4%BD%BF%E7%94%A8GPU"><span class="toc-text">*10.6 使用GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-7-%E7%BB%83%E4%B9%A0"><span class="toc-text">*10.7 练习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Exercise-10-1-Try-a-more-complex-CNN-Try-a-more-complex-CNN"><span class="toc-text">Exercise 10-1: Try a more complex CNN &#x2F; Try a more complex CNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11%E3%80%81Advanced-CNN"><span class="toc-text">11、Advanced CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-%E5%BC%95%E5%85%A5"><span class="toc-text">11.1 引入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Incetion-Modulde"><span class="toc-text">Incetion Modulde</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-1x1-%E5%8D%B7%E7%A7%AF"><span class="toc-text">11.2 1x1 卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-text">计算过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">对比时间复杂度</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-Inception-Module"><span class="toc-text">11.3 Inception Module</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%88%91%E7%9A%84%E6%80%9D%E8%80%83%EF%BC%9A%E5%AE%83%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F%E6%84%8F%E4%B9%89%E4%BD%95%E5%9C%A8%EF%BC%9F"><span class="toc-text">我的思考：它在做什么？意义何在？</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90"><span class="toc-text">架构分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-text">整体流程图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%9A%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="toc-text">1：并行计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%9A%E8%BF%9E%E6%8E%A5%E5%90%88%E5%B9%B6"><span class="toc-text">2：连接合并</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8"><span class="toc-text">代码使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-text">结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-4-%E6%9B%B4%E6%B7%B1%EF%BC%81"><span class="toc-text">11.4 更深！</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-5-Residual-Net"><span class="toc-text">11.5 Residual Net</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E7%A4%BA%E4%BE%8B"><span class="toc-text">简单示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-text">架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Residual-%E6%93%8D%E4%BD%9C"><span class="toc-text">Residual 操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E6%B5%81%E7%A8%8B"><span class="toc-text">代码流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="toc-text">测试结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-6-%E7%BB%83%E4%B9%A0"><span class="toc-text">*11.6 练习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Exercise-11-1-Reading-Paper-and-Implementing"><span class="toc-text">Exercise 11-1: Reading Paper and Implementing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Exercise-11-2-Reading-and-Implementing-DenseNet"><span class="toc-text">Exercise 11-2: Reading and Implementing DenseNet</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12%E3%80%81Basic-RNN"><span class="toc-text">12、Basic RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-%E5%BC%95%E5%85%A5"><span class="toc-text">12.1 引入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFRNN%EF%BC%9F"><span class="toc-text">什么是RNN？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RNN%E5%8D%95%E5%85%83-%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-text">RNN单元 计算过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%951%EF%BC%9ARNNCell"><span class="toc-text">12.2 使用方法1：RNNCell</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%86%85%E5%AE%B9%E7%A4%BA%E6%84%8F"><span class="toc-text">使用内容示意</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-text">参数说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-text">代码示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%952%EF%BC%9ARNN"><span class="toc-text">12.3 使用方法2：RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%86%85%E5%AE%B9%E7%A4%BA%E6%84%8F-2"><span class="toc-text">使用内容示意</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E-2"><span class="toc-text">参数说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-2"><span class="toc-text">代码示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-%E7%A4%BA%E4%BE%8B%EF%BC%9A%E8%AE%AD%E7%BB%83%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BE%93%E5%87%BA"><span class="toc-text">12.4 示例：训练字符串输出</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E4%BB%BB%E5%8A%A1"><span class="toc-text">目标任务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-text">完整代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-Embedding-%E5%B5%8C%E5%85%A5%E5%B1%82"><span class="toc-text">12.5 Embedding 嵌入层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-2"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E5%AD%97%E7%AC%A6%E4%B8%8E%E6%95%B0%E5%AD%97%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">关联字符与数字的方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedding-%E5%B1%82%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-text">Embedding 层计算过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%9E%B6%E6%9E%84"><span class="toc-text">使用架构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-6-%E6%8B%93%E5%B1%95%EF%BC%9ALSTM%E4%B8%8EGRU"><span class="toc-text">12.6 拓展：LSTM与GRU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LSTM"><span class="toc-text">LSTM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GRU"><span class="toc-text">GRU</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13%E3%80%81RNN-Classifier"><span class="toc-text">13、RNN Classifier</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-%E5%BC%95%E5%85%A5"><span class="toc-text">13.1 引入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A7%93%E5%90%8D%E5%AF%B9%E5%BA%94%E5%9B%BD%E5%AE%B6%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">姓名对应国家分类问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-text">设计模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-text">13.2 设计模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-2"><span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bi-direction-%E5%8F%82%E6%95%B0"><span class="toc-text">Bi-direction 参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-2"><span class="toc-text">完整代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-3-%E7%BB%83%E4%B9%A0%E9%A2%98"><span class="toc-text">*13.3 练习题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Exercise-13-1-Sentiment-Analysis-on-Movie-Reviews"><span class="toc-text">Exercise 13-1 Sentiment Analysis on Movie Reviews</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/butterflyblog/2024/07/21/NTL-tutorial/" title="《NTL库》使用教程（C++ 现代密码学）"><img src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/NTL-tutorial/P0.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="《NTL库》使用教程（C++ 现代密码学）"/></a><div class="content"><a class="title" href="/butterflyblog/2024/07/21/NTL-tutorial/" title="《NTL库》使用教程（C++ 现代密码学）">《NTL库》使用教程（C++ 现代密码学）</a><time datetime="2024-07-21T09:00:00.000Z" title="发表于 2024-07-21 17:00:00">2024-07-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/butterflyblog/2024/06/23/DataBaseSystem/" title="《数据库系统原理》知识点梳理"><img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/DatabaseSystemPrinciples/CoverPage.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="《数据库系统原理》知识点梳理"/></a><div class="content"><a class="title" href="/butterflyblog/2024/06/23/DataBaseSystem/" title="《数据库系统原理》知识点梳理">《数据库系统原理》知识点梳理</a><time datetime="2024-06-23T13:00:00.000Z" title="发表于 2024-06-23 21:00:00">2024-06-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/butterflyblog/2024/06/02/CVDL-9-T/" title="CVDL - Transformer"><img src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/CVDL-bupt/C9-Page0.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="CVDL - Transformer"/></a><div class="content"><a class="title" href="/butterflyblog/2024/06/02/CVDL-9-T/" title="CVDL - Transformer">CVDL - Transformer</a><time datetime="2024-06-01T16:00:00.000Z" title="发表于 2024-06-02 00:00:00">2024-06-02</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/posts/PytorchTutorial/PageCover.png')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By isSeymour</div><div class="footer_custom_text">欢迎乘坐我的生活地铁！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/butterflyblog/js/utils.js"></script><script src="/butterflyblog/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23liAZxomGv7Hapw8g',
      clientSecret: 'f7cafde192c4ada8bef4b76952c422d90575cf8b',
      repo: 'gitalk',
      owner: 'isSeymour',
      admin: ['isSeymour'],
      id: 'ed4223db002e2268bf7ea36dcfd9459a',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/butterflyblog/js/search/local-search.js"></script></div></div></body></html>