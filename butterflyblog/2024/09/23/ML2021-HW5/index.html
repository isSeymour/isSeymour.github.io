<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ML2021 - HW 5 | isSeymour</title><meta name="author" content="isSeymour"><meta name="copyright" content="isSeymour"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Homework 5 - Sequence-to-sequence If you have any questions, feel free to email us at: ntu-ml-2021spring-ta@googlegroups.com (4&#x2F;21 Updates)  Link to reference training curves.  (4&#x2F;14 Updates)  Link to">
<meta property="og:type" content="article">
<meta property="og:title" content="ML2021 - HW 5">
<meta property="og:url" content="https://isseymour.github.io/butterflyblog/2024/09/23/ML2021-HW5/index.html">
<meta property="og:site_name" content="isSeymour">
<meta property="og:description" content="Homework 5 - Sequence-to-sequence If you have any questions, feel free to email us at: ntu-ml-2021spring-ta@googlegroups.com (4&#x2F;21 Updates)  Link to reference training curves.  (4&#x2F;14 Updates)  Link to">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/ML2021/ML2021.png">
<meta property="article:published_time" content="2024-09-22T16:00:00.000Z">
<meta property="article:modified_time" content="2024-09-22T16:00:00.000Z">
<meta property="article:author" content="isSeymour">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/ML2021/ML2021.png"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/IC1011.ico"><link rel="canonical" href="https://isseymour.github.io/butterflyblog/2024/09/23/ML2021-HW5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/butterflyblog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/butterflyblog/',
  algolia: undefined,
  localSearch: {"path":"/butterflyblog/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ML2021 - HW 5',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-23 00:00:00'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyblog/code/style.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/T6.jpg" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/butterflyblog/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/butterflyblog/tags/"><div class="headline">标签</div><div class="length-num">34</div></a><a href="/butterflyblog/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-bookmark"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/archives/"><i class="fa-fw fa-solid fa-timeline"></i><span> 总览</span></a></li><li><a class="site-page child" href="/butterflyblog/tags/"><i class="fa-fw fa-sharp fa-solid fa-hashtag"></i><span> 标签</span></a></li><li><a class="site-page child" href="/butterflyblog/categories/"><i class="fa-fw fa-sharp fa-solid fa-folder"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-sharp fa-solid fa-list"></i><span> 功能</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/ctf/"><i class="fa-fw fa-solid fa-shield-halved"></i><span> CTF</span></a></li><li><a class="site-page child" href="/butterflyblog/music/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/butterflyblog/tools/"><i class="fa-fw fa-solid fa-screwdriver-wrench"></i><span> 下载</span></a></li><li><a class="site-page child" href="/butterflyblog/link/"><i class="fa-fw fa-solid fa-paper-plane"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-university"></i><span> 算法</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/algorithm/hot100/"><i class="fa-fw fa-solid fa-fire"></i><span> HOT100</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/template/"><i class="fa-fw fa-solid fa-code"></i><span> Template</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/labuladong/"><i class="fa-fw fa-solid fa-coffee"></i><span> labuladong</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/lanqiao/"><i class="fa-fw fa-solid fa-magnet"></i><span> 蓝桥杯2025</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-plane"></i><span> 旅途</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/journey/diary/"><i class="fa-fw fa-solid fa-hashtag"></i><span> 心路</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/english/"><i class="fa-fw fa-solid fa-globe"></i><span> 英语</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/major/"><i class="fa-fw fa-solid fa-map"></i><span> 专业课</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/mathAI/"><i class="fa-fw fa-solid fa-bar-chart"></i><span> 数学&amp;AI</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/RecSys/"><i class="fa-fw fa-solid fa-thumbs-up"></i><span> RecSys</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-user"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/about/"><i class="fa-fw fa-regular fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/butterflyblog/message/"><i class="fa-fw fa-solid fa-message"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/butterflyblog/develop/"><i class="fa-fw fa-brands fa-windows"></i><span> 开发日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/ML2021/ML2021.png')"><nav id="nav"><span id="blog-info"><a href="/butterflyblog/" title="isSeymour"><span class="site-name">isSeymour</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-bookmark"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/archives/"><i class="fa-fw fa-solid fa-timeline"></i><span> 总览</span></a></li><li><a class="site-page child" href="/butterflyblog/tags/"><i class="fa-fw fa-sharp fa-solid fa-hashtag"></i><span> 标签</span></a></li><li><a class="site-page child" href="/butterflyblog/categories/"><i class="fa-fw fa-sharp fa-solid fa-folder"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-sharp fa-solid fa-list"></i><span> 功能</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/ctf/"><i class="fa-fw fa-solid fa-shield-halved"></i><span> CTF</span></a></li><li><a class="site-page child" href="/butterflyblog/music/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/butterflyblog/tools/"><i class="fa-fw fa-solid fa-screwdriver-wrench"></i><span> 下载</span></a></li><li><a class="site-page child" href="/butterflyblog/link/"><i class="fa-fw fa-solid fa-paper-plane"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-university"></i><span> 算法</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/algorithm/hot100/"><i class="fa-fw fa-solid fa-fire"></i><span> HOT100</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/template/"><i class="fa-fw fa-solid fa-code"></i><span> Template</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/labuladong/"><i class="fa-fw fa-solid fa-coffee"></i><span> labuladong</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/lanqiao/"><i class="fa-fw fa-solid fa-magnet"></i><span> 蓝桥杯2025</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-plane"></i><span> 旅途</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/journey/diary/"><i class="fa-fw fa-solid fa-hashtag"></i><span> 心路</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/english/"><i class="fa-fw fa-solid fa-globe"></i><span> 英语</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/major/"><i class="fa-fw fa-solid fa-map"></i><span> 专业课</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/mathAI/"><i class="fa-fw fa-solid fa-bar-chart"></i><span> 数学&amp;AI</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/RecSys/"><i class="fa-fw fa-solid fa-thumbs-up"></i><span> RecSys</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-user"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/about/"><i class="fa-fw fa-regular fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/butterflyblog/message/"><i class="fa-fw fa-solid fa-message"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/butterflyblog/develop/"><i class="fa-fw fa-brands fa-windows"></i><span> 开发日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">ML2021 - HW 5</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-22T16:00:00.000Z" title="发表于 2024-09-23 00:00:00">2024-09-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-22T16:00:00.000Z" title="更新于 2024-09-23 00:00:00">2024-09-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/butterflyblog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>47分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ML2021 - HW 5"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1><strong>Homework 5 - Sequence-to-sequence</strong></h1>
<p>If you have any questions, feel free to email us at: <a href="mailto:ntu-ml-2021spring-ta@googlegroups.com">ntu-ml-2021spring-ta@googlegroups.com</a></p>
<h3 id="4-21-Updates">(4/21 Updates)</h3>
<ol>
<li>Link to reference <a target="_blank" rel="noopener" href="https://wandb.ai/george0828zhang/hw5.seq2seq.new">training curves</a>.</li>
</ol>
<h3 id="4-14-Updates">(4/14 Updates)</h3>
<ol>
<li>Link to tutorial video <a target="_blank" rel="noopener" href="https://youtu.be/1pjS5_L5REI">part 1</a> <a target="_blank" rel="noopener" href="https://youtu.be/3XX9d0ymKgQ">part 2</a>.</li>
<li>Now defaults to load <code>&quot;avg_last_5_checkpoint.pt&quot;</code> to generate prediction.</li>
<li>Expected run time on Colab with Tesla T4</li>
</ol>
<table>
<thead>
<tr>
<th>Baseline</th>
<th style="text-align:center">Details</th>
<th style="text-align:center">Total Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Simple</td>
<td style="text-align:center">2m 15s $\times$30 epochs</td>
<td style="text-align:center">1hr 8m</td>
</tr>
<tr>
<td>Medium</td>
<td style="text-align:center">4m $\times$30 epochs</td>
<td style="text-align:center">2hr</td>
</tr>
<tr>
<td>Strong</td>
<td style="text-align:center">8m $\times$30 epochs (backward)<br>+1hr (back-translation)<br>+15m $\times$30 epochs (forward)</td>
<td style="text-align:center">12hr 30m</td>
</tr>
</tbody>
</table>
<h2 id="Sequence-to-Sequence-Introduction">Sequence-to-Sequence Introduction</h2>
<ul>
<li>Typical sequence-to-sequence (seq2seq) models are encoder-decoder models, which usually consists of two parts, the encoder and decoder, respectively. These two parts can be implemented with recurrent neural network (RNN) or transformer, primarily to deal with input/output sequences of dynamic length.</li>
<li><strong>Encoder</strong> encodes a sequence of inputs, such as text, video or audio, into a single vector, which can be viewed as the abstractive representation of the inputs, containing information of the whole sequence.</li>
<li><strong>Decoder</strong> decodes the vector output of encoder one step at a time, until the final output sequence is complete. Every decoding step is affected by previous step(s). Generally, one would add “&lt; BOS &gt;” at the begining of the sequence to indicate start of decoding, and “&lt; EOS &gt;” at the end to indicate end of decoding.</li>
</ul>
<p><img src="https://i.imgur.com/0zeDyuI.png" alt="seq2seq"></p>
<h2 id="Homework-Description">Homework Description</h2>
<ul>
<li>
<p>English to Chinese (Traditional) Translation</p>
<ul>
<li>Input: an English sentence         (e.g.		tom is a student .)</li>
<li>Output: the Chinese translation  (e.g. 		湯姆 是 個 學生 。)</li>
</ul>
</li>
<li>
<p>TODO</p>
<ul>
<li>Train a simple RNN seq2seq to acheive translation</li>
<li>Switch to transformer model to boost performance</li>
<li>Apply Back-translation to furthur boost performance</li>
</ul>
</li>
</ul>
<h2 id="Download-and-import-required-packages">Download and import required packages</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pip install <span class="string">&#x27;torch&gt;=1.6.0&#x27;</span> editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb</span><br><span class="line">!pip install --upgrade jupyter ipywidgets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!git clone https://github.com/pytorch/fairseq.git</span><br><span class="line">!cd fairseq &amp;&amp; git checkout 9a1c497</span><br><span class="line">!pip install --upgrade ./fairseq/</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"><span class="keyword">import</span> pprint</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tqdm.auto <span class="keyword">as</span> tqdm</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> Namespace</span><br><span class="line"><span class="keyword">from</span> fairseq <span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="Fix-random-seed">Fix random seed</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">seed = <span class="number">73</span></span><br><span class="line">random.seed(seed)</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h2 id="Dataset-Information">Dataset Information</h2>
<h3 id="En-Zh-Bilingual-Parallel-Corpus">En-Zh Bilingual Parallel Corpus</h3>
<ul>
<li><a href="#reimers-2020-multilingual-sentence-bert">TED2020</a>
<ul>
<li>Raw: 398,066 (sentences)</li>
<li>Processed: 393,980 (sentences)</li>
</ul>
</li>
</ul>
<h3 id="Testdata">Testdata</h3>
<ul>
<li>Size: 4,000 (sentences)</li>
<li><strong>Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a ‘。’</strong></li>
</ul>
<h2 id="Dataset-Download">Dataset Download</h2>
<h3 id="Install-megatools-optional">Install megatools (optional)</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!apt-get install megatools</span></span><br></pre></td></tr></table></figure>
<h3 id="Download-and-extract">Download and extract</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">data_dir = <span class="string">&#x27;./DATA/rawdata&#x27;</span></span><br><span class="line">dataset_name = <span class="string">&#x27;ted2020&#x27;</span></span><br><span class="line">urls = (</span><br><span class="line">    <span class="string">&#x27;&quot;https://onedrive.live.com/download?cid=3E549F3B24B238B4&amp;resid=3E549F3B24B238B4%214989&amp;authkey=AGgQ-DaR8eFSl1A&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;&quot;https://onedrive.live.com/download?cid=3E549F3B24B238B4&amp;resid=3E549F3B24B238B4%214987&amp;authkey=AA4qP_azsicwZZM&quot;&#x27;</span>,</span><br><span class="line"><span class="comment"># # If the above links die, use the following instead.</span></span><br><span class="line"><span class="comment">#     &quot;https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted2020.tgz&quot;,</span></span><br><span class="line"><span class="comment">#     &quot;https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/test.tgz&quot;,</span></span><br><span class="line"><span class="comment"># # If the above links die, use the following instead.</span></span><br><span class="line"><span class="comment">#     &quot;https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U&quot;,</span></span><br><span class="line"><span class="comment">#     &quot;https://mega.nz/#!zNcnGIoJ!oPJX9AvVVs11jc0SaK6vxP_lFUNTkEcK2WbxJpvjU5Y&quot;,</span></span><br><span class="line">)</span><br><span class="line">file_names = (</span><br><span class="line">    <span class="string">&#x27;ted2020.tgz&#x27;</span>, <span class="comment"># train &amp; dev</span></span><br><span class="line">    <span class="string">&#x27;test.tgz&#x27;</span>, <span class="comment"># test</span></span><br><span class="line">)</span><br><span class="line">prefix = Path(data_dir).absolute() / dataset_name</span><br><span class="line"></span><br><span class="line">prefix.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> u, f <span class="keyword">in</span> <span class="built_in">zip</span>(urls, file_names):</span><br><span class="line">    path = prefix/f</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> path.exists():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;mega&#x27;</span> <span class="keyword">in</span> u:</span><br><span class="line">            !megadl &#123;u&#125; --path &#123;path&#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            !wget &#123;u&#125; -O &#123;path&#125;</span><br><span class="line">    <span class="keyword">if</span> path.suffix == <span class="string">&quot;.tgz&quot;</span>:</span><br><span class="line">        !tar -xvf &#123;path&#125; -C &#123;prefix&#125;</span><br><span class="line">    <span class="keyword">elif</span> path.suffix == <span class="string">&quot;.zip&quot;</span>:</span><br><span class="line">        !unzip -o &#123;path&#125; -d &#123;prefix&#125;</span><br><span class="line">!mv &#123;prefix/<span class="string">&#x27;raw.en&#x27;</span>&#125; &#123;prefix/<span class="string">&#x27;train_dev.raw.en&#x27;</span>&#125;</span><br><span class="line">!mv &#123;prefix/<span class="string">&#x27;raw.zh&#x27;</span>&#125; &#123;prefix/<span class="string">&#x27;train_dev.raw.zh&#x27;</span>&#125;</span><br><span class="line">!mv &#123;prefix/<span class="string">&#x27;test.en&#x27;</span>&#125; &#123;prefix/<span class="string">&#x27;test.raw.en&#x27;</span>&#125;</span><br><span class="line">!mv &#123;prefix/<span class="string">&#x27;test.zh&#x27;</span>&#125; &#123;prefix/<span class="string">&#x27;test.raw.zh&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Language">Language</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">src_lang = <span class="string">&#x27;en&#x27;</span></span><br><span class="line">tgt_lang = <span class="string">&#x27;zh&#x27;</span></span><br><span class="line"></span><br><span class="line">data_prefix = <span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>/train_dev.raw&#x27;</span></span><br><span class="line">test_prefix = <span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>/test.raw&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!head &#123;data_prefix+<span class="string">&#x27;.&#x27;</span>+src_lang&#125; -n <span class="number">5</span></span><br><span class="line">!head &#123;data_prefix+<span class="string">&#x27;.&#x27;</span>+tgt_lang&#125; -n <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h3 id="Preprocess-files">Preprocess files</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">strQ2B</span>(<span class="params">ustring</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Full width -&gt; half width&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># reference:https://ithelp.ithome.com.tw/articles/10233122</span></span><br><span class="line">    ss = []</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> ustring:</span><br><span class="line">        rstring = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> uchar <span class="keyword">in</span> s:</span><br><span class="line">            inside_code = <span class="built_in">ord</span>(uchar)</span><br><span class="line">            <span class="keyword">if</span> inside_code == <span class="number">12288</span>:  <span class="comment"># Full width space: direct conversion</span></span><br><span class="line">                inside_code = <span class="number">32</span></span><br><span class="line">            <span class="keyword">elif</span> (inside_code &gt;= <span class="number">65281</span> <span class="keyword">and</span> inside_code &lt;= <span class="number">65374</span>):  <span class="comment"># Full width chars (except space) conversion</span></span><br><span class="line">                inside_code -= <span class="number">65248</span></span><br><span class="line">            rstring += <span class="built_in">chr</span>(inside_code)</span><br><span class="line">        ss.append(rstring)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(ss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_s</span>(<span class="params">s, lang</span>):</span><br><span class="line">    <span class="keyword">if</span> lang == <span class="string">&#x27;en&#x27;</span>:</span><br><span class="line">        s = re.sub(<span class="string">r&quot;\([^()]*\)&quot;</span>, <span class="string">&quot;&quot;</span>, s) <span class="comment"># remove ([text])</span></span><br><span class="line">        s = s.replace(<span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;&#x27;</span>) <span class="comment"># remove &#x27;-&#x27;</span></span><br><span class="line">        s = re.sub(<span class="string">&#x27;([.,;!?()\&quot;])&#x27;</span>, <span class="string">r&#x27; \1 &#x27;</span>, s) <span class="comment"># keep punctuation</span></span><br><span class="line">    <span class="keyword">elif</span> lang == <span class="string">&#x27;zh&#x27;</span>:</span><br><span class="line">        s = strQ2B(s) <span class="comment"># Q2B</span></span><br><span class="line">        s = re.sub(<span class="string">r&quot;\([^()]*\)&quot;</span>, <span class="string">&quot;&quot;</span>, s) <span class="comment"># remove ([text])</span></span><br><span class="line">        s = s.replace(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        s = s.replace(<span class="string">&#x27;—&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        s = s.replace(<span class="string">&#x27;“&#x27;</span>, <span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">        s = s.replace(<span class="string">&#x27;”&#x27;</span>, <span class="string">&#x27;&quot;&#x27;</span>)</span><br><span class="line">        s = s.replace(<span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        s = re.sub(<span class="string">&#x27;([。,;!?()\&quot;~「」])&#x27;</span>, <span class="string">r&#x27; \1 &#x27;</span>, s) <span class="comment"># keep punctuation</span></span><br><span class="line">    s = <span class="string">&#x27; &#x27;</span>.join(s.strip().split())</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">len_s</span>(<span class="params">s, lang</span>):</span><br><span class="line">    <span class="keyword">if</span> lang == <span class="string">&#x27;zh&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(s)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(s.split())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_corpus</span>(<span class="params">prefix, l1, l2, ratio=<span class="number">9</span>, max_len=<span class="number">1000</span>, min_len=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> Path(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>.clean.<span class="subst">&#123;l1&#125;</span>&#x27;</span>).exists() <span class="keyword">and</span> Path(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>.clean.<span class="subst">&#123;l2&#125;</span>&#x27;</span>).exists():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>.clean.<span class="subst">&#123;l1&#125;</span> &amp; <span class="subst">&#123;l2&#125;</span> exists. skipping clean.&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;l1&#125;</span>&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> l1_in_f:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;l2&#125;</span>&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> l2_in_f:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>.clean.<span class="subst">&#123;l1&#125;</span>&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> l1_out_f:</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>.clean.<span class="subst">&#123;l2&#125;</span>&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> l2_out_f:</span><br><span class="line">                    <span class="keyword">for</span> s1 <span class="keyword">in</span> l1_in_f:</span><br><span class="line">                        s1 = s1.strip()</span><br><span class="line">                        s2 = l2_in_f.readline().strip()</span><br><span class="line">                        s1 = clean_s(s1, l1)</span><br><span class="line">                        s2 = clean_s(s2, l2)</span><br><span class="line">                        s1_len = len_s(s1, l1)</span><br><span class="line">                        s2_len = len_s(s2, l2)</span><br><span class="line">                        <span class="keyword">if</span> min_len &gt; <span class="number">0</span>: <span class="comment"># remove short sentence</span></span><br><span class="line">                            <span class="keyword">if</span> s1_len &lt; min_len <span class="keyword">or</span> s2_len &lt; min_len:</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> max_len &gt; <span class="number">0</span>: <span class="comment"># remove long sentence</span></span><br><span class="line">                            <span class="keyword">if</span> s1_len &gt; max_len <span class="keyword">or</span> s2_len &gt; max_len:</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> ratio &gt; <span class="number">0</span>: <span class="comment"># remove by ratio of length</span></span><br><span class="line">                            <span class="keyword">if</span> s1_len/s2_len &gt; ratio <span class="keyword">or</span> s2_len/s1_len &gt; ratio:</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                        <span class="built_in">print</span>(s1, file=l1_out_f)</span><br><span class="line">                        <span class="built_in">print</span>(s2, file=l2_out_f)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clean_corpus(data_prefix, src_lang, tgt_lang)</span><br><span class="line">clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-<span class="number">1</span>, min_len=-<span class="number">1</span>, max_len=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!head &#123;data_prefix+<span class="string">&#x27;.clean.&#x27;</span>+src_lang&#125; -n <span class="number">5</span></span><br><span class="line">!head &#123;data_prefix+<span class="string">&#x27;.clean.&#x27;</span>+tgt_lang&#125; -n <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h3 id="Split-into-train-valid">Split into train/valid</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">valid_ratio = <span class="number">0.01</span> <span class="comment"># 3000~4000 would suffice</span></span><br><span class="line">train_ratio = <span class="number">1</span> - valid_ratio</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (prefix/<span class="string">f&#x27;train.clean.<span class="subst">&#123;src_lang&#125;</span>&#x27;</span>).exists() \</span><br><span class="line"><span class="keyword">and</span> (prefix/<span class="string">f&#x27;train.clean.<span class="subst">&#123;tgt_lang&#125;</span>&#x27;</span>).exists() \</span><br><span class="line"><span class="keyword">and</span> (prefix/<span class="string">f&#x27;valid.clean.<span class="subst">&#123;src_lang&#125;</span>&#x27;</span>).exists() \</span><br><span class="line"><span class="keyword">and</span> (prefix/<span class="string">f&#x27;valid.clean.<span class="subst">&#123;tgt_lang&#125;</span>&#x27;</span>).exists():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;train/valid splits exists. skipping split.&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    line_num = <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;data_prefix&#125;</span>.clean.<span class="subst">&#123;src_lang&#125;</span>&#x27;</span>))</span><br><span class="line">    labels = <span class="built_in">list</span>(<span class="built_in">range</span>(line_num))</span><br><span class="line">    random.shuffle(labels)</span><br><span class="line">    <span class="keyword">for</span> lang <span class="keyword">in</span> [src_lang, tgt_lang]:</span><br><span class="line">        train_f = <span class="built_in">open</span>(os.path.join(data_dir, dataset_name, <span class="string">f&#x27;train.clean.<span class="subst">&#123;lang&#125;</span>&#x27;</span>), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">        valid_f = <span class="built_in">open</span>(os.path.join(data_dir, dataset_name, <span class="string">f&#x27;valid.clean.<span class="subst">&#123;lang&#125;</span>&#x27;</span>), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;data_prefix&#125;</span>.clean.<span class="subst">&#123;lang&#125;</span>&#x27;</span>, <span class="string">&#x27;r&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> labels[count]/line_num &lt; train_ratio:</span><br><span class="line">                train_f.write(line)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valid_f.write(line)</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">        train_f.close()</span><br><span class="line">        valid_f.close()</span><br></pre></td></tr></table></figure>
<h3 id="Subword-Units">Subword Units</h3>
<p>Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.</p>
<ul>
<li>We will use the <a href="#kudo-richardson-2018-sentencepiece">sentencepiece</a> package</li>
<li>select ‘unigram’ or ‘byte-pair encoding (BPE)’ algorithm</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line">vocab_size = <span class="number">8000</span></span><br><span class="line"><span class="keyword">if</span> (prefix/<span class="string">f&#x27;spm<span class="subst">&#123;vocab_size&#125;</span>.model&#x27;</span>).exists():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>/spm<span class="subst">&#123;vocab_size&#125;</span>.model exists. skipping spm_train.&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    spm.SentencePieceTrainer.train(</span><br><span class="line">        <span class="built_in">input</span>=<span class="string">&#x27;,&#x27;</span>.join([<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>/train.clean.<span class="subst">&#123;src_lang&#125;</span>&#x27;</span>,</span><br><span class="line">                        <span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>/valid.clean.<span class="subst">&#123;src_lang&#125;</span>&#x27;</span>,</span><br><span class="line">                        <span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>/train.clean.<span class="subst">&#123;tgt_lang&#125;</span>&#x27;</span>,</span><br><span class="line">                        <span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>/valid.clean.<span class="subst">&#123;tgt_lang&#125;</span>&#x27;</span>]),</span><br><span class="line">        model_prefix=prefix/<span class="string">f&#x27;spm<span class="subst">&#123;vocab_size&#125;</span>&#x27;</span>,</span><br><span class="line">        vocab_size=vocab_size,</span><br><span class="line">        character_coverage=<span class="number">1</span>,</span><br><span class="line">        model_type=<span class="string">&#x27;unigram&#x27;</span>, <span class="comment"># &#x27;bpe&#x27; works as well</span></span><br><span class="line">        input_sentence_size=<span class="number">1e6</span>,</span><br><span class="line">        shuffle_input_sentence=<span class="literal">True</span>,</span><br><span class="line">        normalization_rule_name=<span class="string">&#x27;nmt_nfkc_cf&#x27;</span>,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spm_model = spm.SentencePieceProcessor(model_file=<span class="built_in">str</span>(prefix/<span class="string">f&#x27;spm<span class="subst">&#123;vocab_size&#125;</span>.model&#x27;</span>))</span><br><span class="line">in_tag = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: <span class="string">&#x27;train.clean&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;valid&#x27;</span>: <span class="string">&#x27;valid.clean&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;test&#x27;</span>: <span class="string">&#x27;test.raw.clean&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]:</span><br><span class="line">    <span class="keyword">for</span> lang <span class="keyword">in</span> [src_lang, tgt_lang]:</span><br><span class="line">        out_path = prefix/<span class="string">f&#x27;<span class="subst">&#123;split&#125;</span>.<span class="subst">&#123;lang&#125;</span>&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> out_path.exists():</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;out_path&#125;</span> exists. skipping spm_encode.&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(prefix/<span class="string">f&#x27;<span class="subst">&#123;split&#125;</span>.<span class="subst">&#123;lang&#125;</span>&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> out_f:</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(prefix/<span class="string">f&#x27;<span class="subst">&#123;in_tag[split]&#125;</span>.<span class="subst">&#123;lang&#125;</span>&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> in_f:</span><br><span class="line">                    <span class="keyword">for</span> line <span class="keyword">in</span> in_f:</span><br><span class="line">                        line = line.strip()</span><br><span class="line">                        tok = spm_model.encode(line, out_type=<span class="built_in">str</span>)</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(tok), file=out_f)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!head &#123;data_dir+<span class="string">&#x27;/&#x27;</span>+dataset_name+<span class="string">&#x27;/train.&#x27;</span>+src_lang&#125; -n <span class="number">5</span></span><br><span class="line">!head &#123;data_dir+<span class="string">&#x27;/&#x27;</span>+dataset_name+<span class="string">&#x27;/train.&#x27;</span>+tgt_lang&#125; -n <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h3 id="Binarize-the-data-with-fairseq">Binarize the data with fairseq</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">binpath = Path(<span class="string">&#x27;./DATA/data-bin&#x27;</span>, dataset_name)</span><br><span class="line"><span class="keyword">if</span> binpath.exists():</span><br><span class="line">    <span class="built_in">print</span>(binpath, <span class="string">&quot;exists, will not overwrite!&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    !python -m fairseq_cli.preprocess \</span><br><span class="line">        --source-lang &#123;src_lang&#125;\</span><br><span class="line">        --target-lang &#123;tgt_lang&#125;\</span><br><span class="line">        --trainpref &#123;prefix/<span class="string">&#x27;train&#x27;</span>&#125;\</span><br><span class="line">        --validpref &#123;prefix/<span class="string">&#x27;valid&#x27;</span>&#125;\</span><br><span class="line">        --testpref &#123;prefix/<span class="string">&#x27;test&#x27;</span>&#125;\</span><br><span class="line">        --destdir &#123;binpath&#125;\</span><br><span class="line">        --joined-dictionary\</span><br><span class="line">        --workers <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="Configuration-for-Experiments">Configuration for Experiments</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">config = Namespace(</span><br><span class="line">    datadir = <span class="string">&quot;./DATA/data-bin/ted2020&quot;</span>,</span><br><span class="line">    savedir = <span class="string">&quot;./checkpoints/rnn&quot;</span>,</span><br><span class="line">    source_lang = <span class="string">&quot;en&quot;</span>,</span><br><span class="line">    target_lang = <span class="string">&quot;zh&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cpu threads when fetching &amp; processing data.</span></span><br><span class="line">    num_workers=<span class="number">2</span>,</span><br><span class="line">    <span class="comment"># batch size in terms of tokens. gradient accumulation increases the effective batchsize.</span></span><br><span class="line">    max_tokens=<span class="number">8192</span>,</span><br><span class="line">    accum_steps=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.</span></span><br><span class="line">    lr_factor=<span class="number">2.</span>,</span><br><span class="line">    lr_warmup=<span class="number">4000</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># clipping gradient norm helps alleviate gradient exploding</span></span><br><span class="line">    clip_norm=<span class="number">1.0</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># maximum epochs for training</span></span><br><span class="line">    max_epoch=<span class="number">30</span>,</span><br><span class="line">    start_epoch=<span class="number">1</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># beam size for beam search</span></span><br><span class="line">    beam=<span class="number">5</span>,</span><br><span class="line">    <span class="comment"># generate sequences of maximum length ax + b, where x is the source length</span></span><br><span class="line">    max_len_a=<span class="number">1.2</span>,</span><br><span class="line">    max_len_b=<span class="number">10</span>,</span><br><span class="line">    <span class="comment"># when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.</span></span><br><span class="line">    post_process = <span class="string">&quot;sentencepiece&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="comment"># checkpoints</span></span><br><span class="line">    keep_last_epochs=<span class="number">5</span>,</span><br><span class="line">    resume=<span class="literal">None</span>, <span class="comment"># if resume from checkpoint name (under config.savedir)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># logging</span></span><br><span class="line">    use_wandb=<span class="literal">False</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="Logging">Logging</h2>
<ul>
<li>logging package logs ordinary messages</li>
<li>wandb logs the loss, bleu, etc. in the training process</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s | %(levelname)s | %(name)s | %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>,</span><br><span class="line">    level=<span class="string">&quot;INFO&quot;</span>, <span class="comment"># &quot;DEBUG&quot; &quot;WARNING&quot; &quot;ERROR&quot;</span></span><br><span class="line">    stream=sys.stdout,</span><br><span class="line">)</span><br><span class="line">proj = <span class="string">&quot;hw5.seq2seq&quot;</span></span><br><span class="line">logger = logging.getLogger(proj)</span><br><span class="line"><span class="keyword">if</span> config.use_wandb:</span><br><span class="line">    <span class="keyword">import</span> wandb</span><br><span class="line">    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)</span><br></pre></td></tr></table></figure>
<h2 id="CUDA-Environment">CUDA Environment</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cuda_env = utils.CudaEnvironment()</span><br><span class="line">utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Dataloading">Dataloading</h2>
<h3 id="We-borrow-the-TranslationTask-from-fairseq">We borrow the TranslationTask from fairseq</h3>
<ul>
<li>used to load the binarized data created above</li>
<li>well-implemented data iterator (dataloader)</li>
<li>built-in task.source_dictionary and task.target_dictionary are also handy</li>
<li>well-implemented beach search decoder</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fairseq.tasks.translation <span class="keyword">import</span> TranslationConfig, TranslationTask</span><br><span class="line"></span><br><span class="line"><span class="comment">## setup task</span></span><br><span class="line">task_cfg = TranslationConfig(</span><br><span class="line">    data=config.datadir,</span><br><span class="line">    source_lang=config.source_lang,</span><br><span class="line">    target_lang=config.target_lang,</span><br><span class="line">    train_subset=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">    required_seq_len_multiple=<span class="number">8</span>,</span><br><span class="line">    dataset_impl=<span class="string">&quot;mmap&quot;</span>,</span><br><span class="line">    upsample_primary=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line">task = TranslationTask.setup_task(task_cfg)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logger.info(<span class="string">&quot;loading data for epoch 1&quot;</span>)</span><br><span class="line">task.load_dataset(split=<span class="string">&quot;train&quot;</span>, epoch=<span class="number">1</span>, combine=<span class="literal">True</span>) <span class="comment"># combine if you have back-translation data.</span></span><br><span class="line">task.load_dataset(split=<span class="string">&quot;valid&quot;</span>, epoch=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sample = task.dataset(<span class="string">&quot;valid&quot;</span>)[<span class="number">1</span>]</span><br><span class="line">pprint.pprint(sample)</span><br><span class="line">pprint.pprint(</span><br><span class="line">    <span class="string">&quot;Source: &quot;</span> + \</span><br><span class="line">    task.source_dictionary.string(</span><br><span class="line">        sample[<span class="string">&#x27;source&#x27;</span>],</span><br><span class="line">        config.post_process,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">pprint.pprint(</span><br><span class="line">    <span class="string">&quot;Target: &quot;</span> + \</span><br><span class="line">    task.target_dictionary.string(</span><br><span class="line">        sample[<span class="string">&#x27;target&#x27;</span>],</span><br><span class="line">        config.post_process,</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Dataset-Iterator">Dataset Iterator</h3>
<ul>
<li>Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency</li>
<li>Shuffles the training set for every epoch</li>
<li>Ignore sentences exceeding maximum length</li>
<li>Pad all sentences in a batch to the same length, which enables parallel computing by GPU</li>
<li>Add eos and shift one token
<ul>
<li>teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.</li>
<li>generally, prepending bos to the target would do the job (as shown below)<br>
<img src="https://i.imgur.com/0zeDyuI.png" alt="seq2seq"></li>
<li>in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:</li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># output target (target) and Decoder input (prev_output_tokens):</span><br><span class="line">               eos = 2</span><br><span class="line">            target = 419,  711,  238,  888,  792,   60,  968,    8,    2</span><br><span class="line">prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_iterator</span>(<span class="params">task, split, epoch=<span class="number">1</span>, max_tokens=<span class="number">4000</span>, num_workers=<span class="number">1</span>, cached=<span class="literal">True</span></span>):</span><br><span class="line">    batch_iterator = task.get_batch_iterator(</span><br><span class="line">        dataset=task.dataset(split),</span><br><span class="line">        max_tokens=max_tokens,</span><br><span class="line">        max_sentences=<span class="literal">None</span>,</span><br><span class="line">        max_positions=utils.resolve_max_positions(</span><br><span class="line">            task.max_positions(),</span><br><span class="line">            max_tokens,</span><br><span class="line">        ),</span><br><span class="line">        ignore_invalid_inputs=<span class="literal">True</span>,</span><br><span class="line">        seed=seed,</span><br><span class="line">        num_workers=num_workers,</span><br><span class="line">        epoch=epoch,</span><br><span class="line">        disable_iterator_cache=<span class="keyword">not</span> cached,</span><br><span class="line">        <span class="comment"># Set this to False to speed up. However, if set to False, changing max_tokens beyond</span></span><br><span class="line">        <span class="comment"># first call of this method has no effect.</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> batch_iterator</span><br><span class="line"></span><br><span class="line">demo_epoch_obj = load_data_iterator(task, <span class="string">&quot;valid&quot;</span>, epoch=<span class="number">1</span>, max_tokens=<span class="number">20</span>, num_workers=<span class="number">1</span>, cached=<span class="literal">False</span>)</span><br><span class="line">demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=<span class="literal">True</span>)</span><br><span class="line">sample = <span class="built_in">next</span>(demo_iter)</span><br><span class="line">sample</span><br></pre></td></tr></table></figure>
<ul>
<li>each batch is a python dict, with string key and Tensor value. Contents are described below:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch = &#123;</span><br><span class="line">    <span class="string">&quot;id&quot;</span>: <span class="built_in">id</span>, <span class="comment"># id for each example</span></span><br><span class="line">    <span class="string">&quot;nsentences&quot;</span>: <span class="built_in">len</span>(samples), <span class="comment"># batch size (sentences)</span></span><br><span class="line">    <span class="string">&quot;ntokens&quot;</span>: ntokens, <span class="comment"># batch size (tokens)</span></span><br><span class="line">    <span class="string">&quot;net_input&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;src_tokens&quot;</span>: src_tokens, <span class="comment"># sequence in source language</span></span><br><span class="line">        <span class="string">&quot;src_lengths&quot;</span>: src_lengths, <span class="comment"># sequence length of each example before padding</span></span><br><span class="line">        <span class="string">&quot;prev_output_tokens&quot;</span>: prev_output_tokens, <span class="comment"># right shifted target, as mentioned above.</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;target&quot;</span>: target, <span class="comment"># target sequence</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Model-Architecture">Model Architecture</h2>
<ul>
<li>We again inherit fairseq’s encoder, decoder and model, so that in the testing phase we can directly leverage fairseq’s beam search decoder.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fairseq.models <span class="keyword">import</span> (</span><br><span class="line">    FairseqEncoder,</span><br><span class="line">    FairseqIncrementalDecoder,</span><br><span class="line">    FairseqEncoderDecoderModel</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Encoder">Encoder</h3>
<ul>
<li>
<p>The Encoder is a RNN or Transformer Encoder. The following description is for RNN. For every input token, Encoder will generate a output vector and a hidden states vector, and the hidden states vector is passed on to the next step. In other words, the Encoder sequentially reads in the input sequence, and outputs a single vector at each timestep, then finally outputs the final hidden states, or content vector, at the last timestep.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li><em>args</em>
<ul>
<li>encoder_embed_dim: the dimension of embeddings, this compresses the one-hot vector into fixed dimensions, which achieves dimension reduction</li>
<li>encoder_ffn_embed_dim is the dimension of hidden states and output vectors</li>
<li>encoder_layers is the number of layers for Encoder RNN</li>
<li>dropout determines the probability of a neuron’s activation being set to 0, in order to prevent overfitting. Generally this is applied in training, and removed in testing.</li>
</ul>
</li>
<li><em>dictionary</em>: the dictionary provided by fairseq. it’s used to obtain the padding index, and in turn the encoder padding mask.</li>
<li><em>embed_tokens</em>: an instance of token embeddings (nn.Embedding)</li>
</ul>
</li>
<li>
<p>Inputs:</p>
<ul>
<li><em>src_tokens</em>: integer sequence representing english e.g. 1, 28, 29, 205, 2</li>
</ul>
</li>
<li>
<p>Outputs:</p>
<ul>
<li><em>outputs</em>: the output of RNN at each timestep, can be furthur processed by Attention</li>
<li><em>final_hiddens</em>: the hidden states of each timestep, will be passed to decoder for decoding</li>
<li><em>encoder_padding_mask</em>: this tells the decoder which position to ignore</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNEncoder</span>(<span class="title class_ inherited__">FairseqEncoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args, dictionary, embed_tokens</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(dictionary)</span><br><span class="line">        self.embed_tokens = embed_tokens</span><br><span class="line"></span><br><span class="line">        self.embed_dim = args.encoder_embed_dim</span><br><span class="line">        self.hidden_dim = args.encoder_ffn_embed_dim</span><br><span class="line">        self.num_layers = args.encoder_layers</span><br><span class="line"></span><br><span class="line">        self.dropout_in_module = nn.Dropout(args.dropout)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            self.embed_dim,</span><br><span class="line">            self.hidden_dim,</span><br><span class="line">            self.num_layers,</span><br><span class="line">            dropout=args.dropout,</span><br><span class="line">            batch_first=<span class="literal">False</span>,</span><br><span class="line">            bidirectional=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        self.dropout_out_module = nn.Dropout(args.dropout)</span><br><span class="line"></span><br><span class="line">        self.padding_idx = dictionary.pad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combine_bidir</span>(<span class="params">self, outs, bsz: <span class="built_in">int</span></span>):</span><br><span class="line">        out = outs.view(self.num_layers, <span class="number">2</span>, bsz, -<span class="number">1</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">        <span class="keyword">return</span> out.view(self.num_layers, bsz, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src_tokens, **unused</span>):</span><br><span class="line">        bsz, seqlen = src_tokens.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get embeddings</span></span><br><span class="line">        x = self.embed_tokens(src_tokens)</span><br><span class="line">        x = self.dropout_in_module(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># B x T x C -&gt; T x B x C</span></span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass thru bidirectional RNN</span></span><br><span class="line">        h0 = x.new_zeros(<span class="number">2</span> * self.num_layers, bsz, self.hidden_dim)</span><br><span class="line">        x, final_hiddens = self.rnn(x, h0)</span><br><span class="line">        outputs = self.dropout_out_module(x)</span><br><span class="line">        <span class="comment"># outputs = [sequence len, batch size, hid dim * directions]</span></span><br><span class="line">        <span class="comment"># hidden =  [num_layers * directions, batch size  , hid dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Since Encoder is bidirectional, we need to concatenate the hidden states of two directions</span></span><br><span class="line">        final_hiddens = self.combine_bidir(final_hiddens, bsz)</span><br><span class="line">        <span class="comment"># hidden =  [num_layers x batch x num_directions*hidden]</span></span><br><span class="line"></span><br><span class="line">        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">tuple</span>(</span><br><span class="line">            (</span><br><span class="line">                outputs,  <span class="comment"># seq_len x batch x hidden</span></span><br><span class="line">                final_hiddens,  <span class="comment"># num_layers x batch x num_directions*hidden</span></span><br><span class="line">                encoder_padding_mask,  <span class="comment"># seq_len x batch</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reorder_encoder_out</span>(<span class="params">self, encoder_out, new_order</span>):</span><br><span class="line">        <span class="comment"># This is used by fairseq&#x27;s beam search. How and why is not particularly important here.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">tuple</span>(</span><br><span class="line">            (</span><br><span class="line">                encoder_out[<span class="number">0</span>].index_select(<span class="number">1</span>, new_order),</span><br><span class="line">                encoder_out[<span class="number">1</span>].index_select(<span class="number">1</span>, new_order),</span><br><span class="line">                encoder_out[<span class="number">2</span>].index_select(<span class="number">1</span>, new_order),</span><br><span class="line">            )</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Attention">Attention</h3>
<ul>
<li>
<p>When the input sequence is long, “content vector” alone cannot accurately represent the whole sequence, attention mechanism can provide the Decoder more information.</p>
</li>
<li>
<p>According to the <strong>Decoder embeddings</strong> of the current timestep, match the <strong>Encoder outputs</strong> with decoder embeddings to determine correlation, and then sum the Encoder outputs weighted by the correlation as the input to <strong>Decoder</strong> RNN.</p>
</li>
<li>
<p>Common attention implementations use neural network / dot product as the correlation between <strong>query</strong> (decoder embeddings) and <strong>key</strong> (Encoder outputs), followed by <strong>softmax</strong>  to obtain a distribution, and finally <strong>values</strong> (Encoder outputs) is <strong>weighted sum</strong>-ed by said distribution.</p>
</li>
<li>
<p>Parameters:</p>
<ul>
<li><em>input_embed_dim</em>: dimensionality of key, should be that of the vector in decoder to attend others</li>
<li><em>source_embed_dim</em>: dimensionality of query, should be that of the vector to be attended to (encoder outputs)</li>
<li><em>output_embed_dim</em>: dimensionality of value, should be that of the vector after attention, expected by the next layer</li>
</ul>
</li>
<li>
<p>Inputs:</p>
<ul>
<li><em>inputs</em>: is the key, the vector to attend to others</li>
<li><em>encoder_outputs</em>:  is the query/value, the vector to be attended to</li>
<li><em>encoder_padding_mask</em>: this tells the decoder which position to ignore</li>
</ul>
</li>
<li>
<p>Outputs:</p>
<ul>
<li><em>output</em>: the context vector after attention</li>
<li><em>attention score</em>: the attention distribution</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_embed_dim, source_embed_dim, output_embed_dim, bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)</span><br><span class="line">        self.output_proj = nn.Linear(</span><br><span class="line">            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, encoder_outputs, encoder_padding_mask</span>):</span><br><span class="line">        <span class="comment"># inputs: T, B, dim</span></span><br><span class="line">        <span class="comment"># encoder_outputs: S x B x dim</span></span><br><span class="line">        <span class="comment"># padding mask:  S x B</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># convert all to batch first</span></span><br><span class="line">        inputs = inputs.transpose(<span class="number">1</span>,<span class="number">0</span>) <span class="comment"># B, T, dim</span></span><br><span class="line">        encoder_outputs = encoder_outputs.transpose(<span class="number">1</span>,<span class="number">0</span>) <span class="comment"># B, S, dim</span></span><br><span class="line">        encoder_padding_mask = encoder_padding_mask.transpose(<span class="number">1</span>,<span class="number">0</span>) <span class="comment"># B, S</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># project to the dimensionality of encoder_outputs</span></span><br><span class="line">        x = self.input_proj(inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute attention</span></span><br><span class="line">        <span class="comment"># (B, T, dim) x (B, dim, S) = (B, T, S)</span></span><br><span class="line">        attn_scores = torch.bmm(x, encoder_outputs.transpose(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cancel the attention at positions corresponding to padding</span></span><br><span class="line">        <span class="keyword">if</span> encoder_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># leveraging broadcast  B, S -&gt; (B, 1, S)</span></span><br><span class="line">            encoder_padding_mask = encoder_padding_mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            attn_scores = (</span><br><span class="line">                attn_scores.<span class="built_in">float</span>()</span><br><span class="line">                .masked_fill_(encoder_padding_mask, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">                .type_as(attn_scores)</span><br><span class="line">            )  <span class="comment"># FP16 support: cast to float and back</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># softmax on the dimension corresponding to source sequence</span></span><br><span class="line">        attn_scores = F.softmax(attn_scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum</span></span><br><span class="line">        x = torch.bmm(attn_scores, encoder_outputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (B, T, dim)</span></span><br><span class="line">        x = torch.cat((x, inputs), dim=-<span class="number">1</span>)</span><br><span class="line">        x = torch.tanh(self.output_proj(x)) <span class="comment"># concat + linear + tanh</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># restore shape (B, T, dim) -&gt; (T, B, dim)</span></span><br><span class="line">        <span class="keyword">return</span> x.transpose(<span class="number">1</span>,<span class="number">0</span>), attn_scores</span><br></pre></td></tr></table></figure>
<h3 id="Decoder">Decoder</h3>
<ul>
<li>The hidden states of <strong>Decoder</strong> will be initialized by the final hidden states of <strong>Encoder</strong> (the content vector)</li>
<li>At the same time, <strong>Decoder</strong> will change its hidden states based on the input of the current timestep (the outputs of previous timesteps), and generates an output</li>
<li>Attention improves the performance</li>
<li>The seq2seq steps are implemented in decoder, so that later the Seq2Seq class can accept RNN and Transformer, without furthur modification.</li>
</ul>
<ul>
<li>Parameters:
<ul>
<li><em>args</em>
<ul>
<li>decoder_embed_dim: is the dimensionality of the decoder embeddings, similar to encoder_embed_dim，</li>
<li>decoder_ffn_embed_dim: is the dimensionality of the decoder RNN hidden states, similar to encoder_ffn_embed_dim</li>
<li>decoder_layers: number of layers of RNN decoder</li>
<li>share_decoder_input_output_embed: usually, the projection matrix of the decoder will share weights with the decoder input embeddings</li>
</ul>
</li>
<li><em>dictionary</em>: the dictionary provided by fairseq</li>
<li><em>embed_tokens</em>: an instance of token embeddings (nn.Embedding)</li>
</ul>
</li>
<li>Inputs:
<ul>
<li><em>prev_output_tokens</em>: integer sequence representing the right-shifted target e.g. 1, 28, 29, 205, 2</li>
<li><em>encoder_out</em>: encoder’s output.</li>
<li><em>incremental_state</em>: in order to speed up decoding during test time, we will save the hidden state of each timestep. see forward() for details.</li>
</ul>
</li>
<li>Outputs:
<ul>
<li><em>outputs</em>: the logits (before softmax) output of decoder for each timesteps</li>
<li><em>extra</em>: unsused</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNDecoder</span>(<span class="title class_ inherited__">FairseqIncrementalDecoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args, dictionary, embed_tokens</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(dictionary)</span><br><span class="line">        self.embed_tokens = embed_tokens</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> args.decoder_layers == args.encoder_layers, <span class="string">f&quot;&quot;&quot;seq2seq rnn requires that encoder</span></span><br><span class="line"><span class="string">        and decoder have same layers of rnn. got: <span class="subst">&#123;args.encoder_layers, args.decoder_layers&#125;</span>&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*<span class="number">2</span>, <span class="string">f&quot;&quot;&quot;seq2seq-rnn requires</span></span><br><span class="line"><span class="string">        that decoder hidden to be 2*encoder hidden dim. got: <span class="subst">&#123;args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*<span class="number">2</span>&#125;</span>&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        self.embed_dim = args.decoder_embed_dim</span><br><span class="line">        self.hidden_dim = args.decoder_ffn_embed_dim</span><br><span class="line">        self.num_layers = args.decoder_layers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.dropout_in_module = nn.Dropout(args.dropout)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            self.embed_dim,</span><br><span class="line">            self.hidden_dim,</span><br><span class="line">            self.num_layers,</span><br><span class="line">            dropout=args.dropout,</span><br><span class="line">            batch_first=<span class="literal">False</span>,</span><br><span class="line">            bidirectional=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        self.attention = AttentionLayer(</span><br><span class="line">            self.embed_dim, self.hidden_dim, self.embed_dim, bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># self.attention = None</span></span><br><span class="line">        self.dropout_out_module = nn.Dropout(args.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.hidden_dim != self.embed_dim:</span><br><span class="line">            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.project_out_dim = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> args.share_decoder_input_output_embed:</span><br><span class="line">            self.output_projection = nn.Linear(</span><br><span class="line">                self.embed_tokens.weight.shape[<span class="number">1</span>],</span><br><span class="line">                self.embed_tokens.weight.shape[<span class="number">0</span>],</span><br><span class="line">                bias=<span class="literal">False</span>,</span><br><span class="line">            )</span><br><span class="line">            self.output_projection.weight = self.embed_tokens.weight</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.output_projection = nn.Linear(</span><br><span class="line">                self.output_embed_dim, <span class="built_in">len</span>(dictionary), bias=<span class="literal">False</span></span><br><span class="line">            )</span><br><span class="line">            nn.init.normal_(</span><br><span class="line">                self.output_projection.weight, mean=<span class="number">0</span>, std=self.output_embed_dim ** -<span class="number">0.5</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, prev_output_tokens, encoder_out, incremental_state=<span class="literal">None</span>, **unused</span>):</span><br><span class="line">        <span class="comment"># extract the outputs from encoder</span></span><br><span class="line">        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out</span><br><span class="line">        <span class="comment"># outputs:          seq_len x batch x num_directions*hidden</span></span><br><span class="line">        <span class="comment"># encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden</span></span><br><span class="line">        <span class="comment"># padding_mask:     seq_len x batch</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> incremental_state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(incremental_state) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># if the information from last timestep is retained, we can continue from there instead of starting from bos</span></span><br><span class="line">            prev_output_tokens = prev_output_tokens[:, -<span class="number">1</span>:]</span><br><span class="line">            cache_state = self.get_incremental_state(incremental_state, <span class="string">&quot;cached_state&quot;</span>)</span><br><span class="line">            prev_hiddens = cache_state[<span class="string">&quot;prev_hiddens&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># incremental state does not exist, either this is training time, or the first timestep of test time</span></span><br><span class="line">            <span class="comment"># prepare for seq2seq: pass the encoder_hidden to the decoder hidden states</span></span><br><span class="line">            prev_hiddens = encoder_hiddens</span><br><span class="line"></span><br><span class="line">        bsz, seqlen = prev_output_tokens.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># embed tokens</span></span><br><span class="line">        x = self.embed_tokens(prev_output_tokens)</span><br><span class="line">        x = self.dropout_in_module(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># B x T x C -&gt; T x B x C</span></span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder-to-encoder attention</span></span><br><span class="line">        <span class="keyword">if</span> self.attention <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass thru unidirectional RNN</span></span><br><span class="line">        x, final_hiddens = self.rnn(x, prev_hiddens)</span><br><span class="line">        <span class="comment"># outputs = [sequence len, batch size, hid dim]</span></span><br><span class="line">        <span class="comment"># hidden =  [num_layers * directions, batch size  , hid dim]</span></span><br><span class="line">        x = self.dropout_out_module(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># project to embedding size (if hidden differs from embed size, and share_embedding is True,</span></span><br><span class="line">        <span class="comment"># we need to do an extra projection)</span></span><br><span class="line">        <span class="keyword">if</span> self.project_out_dim != <span class="literal">None</span>:</span><br><span class="line">            x = self.project_out_dim(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># project to vocab size</span></span><br><span class="line">        x = self.output_projection(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># T x B x C -&gt; B x T x C</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if incremental, record the hidden states of current timestep, which will be restored in the next timestep</span></span><br><span class="line">        cache_state = &#123;</span><br><span class="line">            <span class="string">&quot;prev_hiddens&quot;</span>: final_hiddens,</span><br><span class="line">        &#125;</span><br><span class="line">        self.set_incremental_state(incremental_state, <span class="string">&quot;cached_state&quot;</span>, cache_state)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reorder_incremental_state</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        incremental_state,</span></span><br><span class="line"><span class="params">        new_order,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="comment"># This is used by fairseq&#x27;s beam search. How and why is not particularly important here.</span></span><br><span class="line">        cache_state = self.get_incremental_state(incremental_state, <span class="string">&quot;cached_state&quot;</span>)</span><br><span class="line">        prev_hiddens = cache_state[<span class="string">&quot;prev_hiddens&quot;</span>]</span><br><span class="line">        prev_hiddens = [p.index_select(<span class="number">0</span>, new_order) <span class="keyword">for</span> p <span class="keyword">in</span> prev_hiddens]</span><br><span class="line">        cache_state = &#123;</span><br><span class="line">            <span class="string">&quot;prev_hiddens&quot;</span>: torch.stack(prev_hiddens),</span><br><span class="line">        &#125;</span><br><span class="line">        self.set_incremental_state(incremental_state, <span class="string">&quot;cached_state&quot;</span>, cache_state)</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<h3 id="Seq2Seq">Seq2Seq</h3>
<ul>
<li>Composed of <strong>Encoder</strong> and <strong>Decoder</strong></li>
<li>Recieves inputs and pass to <strong>Encoder</strong></li>
<li>Pass the outputs from <strong>Encoder</strong> to <strong>Decoder</strong></li>
<li><strong>Decoder</strong> will decode according to outputs of previous timesteps as well as <strong>Encoder</strong> outputs</li>
<li>Once done decoding, return the <strong>Decoder</strong> outputs</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(<span class="title class_ inherited__">FairseqEncoderDecoderModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(encoder, decoder)</span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        src_tokens,</span></span><br><span class="line"><span class="params">        src_lengths,</span></span><br><span class="line"><span class="params">        prev_output_tokens,</span></span><br><span class="line"><span class="params">        return_all_hiddens: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Run the forward pass for an encoder-decoder model.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        encoder_out = self.encoder(</span><br><span class="line">            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens</span><br><span class="line">        )</span><br><span class="line">        logits, extra = self.decoder(</span><br><span class="line">            prev_output_tokens,</span><br><span class="line">            encoder_out=encoder_out,</span><br><span class="line">            src_lengths=src_lengths,</span><br><span class="line">            return_all_hiddens=return_all_hiddens,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> logits, extra</span><br></pre></td></tr></table></figure>
<h2 id="Model-Initialization">Model Initialization</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # HINT: transformer architecture</span></span><br><span class="line"><span class="comment"># from fairseq.models.transformer import (</span></span><br><span class="line"><span class="comment">#     TransformerEncoder,</span></span><br><span class="line"><span class="comment">#     TransformerDecoder,</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model</span>(<span class="params">args, task</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; build a model instance based on hyperparameters &quot;&quot;&quot;</span></span><br><span class="line">    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary</span><br><span class="line"></span><br><span class="line">    <span class="comment"># token embeddings</span></span><br><span class="line">    encoder_embed_tokens = nn.Embedding(<span class="built_in">len</span>(src_dict), args.encoder_embed_dim, src_dict.pad())</span><br><span class="line">    decoder_embed_tokens = nn.Embedding(<span class="built_in">len</span>(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># encoder decoder</span></span><br><span class="line">    <span class="comment"># HINT: <span class="doctag">TODO:</span> switch to TransformerEncoder &amp; TransformerDecoder</span></span><br><span class="line">    encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)</span><br><span class="line">    decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sequence to sequence model</span></span><br><span class="line">    model = Seq2Seq(args, encoder, decoder)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialization for seq2seq model is important, requires extra handling</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_params</span>(<span class="params">module</span>):</span><br><span class="line">        <span class="keyword">from</span> fairseq.modules <span class="keyword">import</span> MultiheadAttention</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Linear):</span><br><span class="line">            module.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">if</span> module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.bias.data.zero_()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Embedding):</span><br><span class="line">            module.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">if</span> module.padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.weight.data[module.padding_idx].zero_()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, MultiheadAttention):</span><br><span class="line">            module.q_proj.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            module.k_proj.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            module.v_proj.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.RNNBase):</span><br><span class="line">            <span class="keyword">for</span> name, param <span class="keyword">in</span> module.named_parameters():</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&quot;bias&quot;</span> <span class="keyword">in</span> name:</span><br><span class="line">                    param.data.uniform_(-<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># weight initialization</span></span><br><span class="line">    model.apply(init_params)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h3 id="Architecture-Related-Configuration">Architecture Related Configuration</h3>
<p>reference implementation</p>
<table>
<thead>
<tr>
<th>model</th>
<th>embedding dim</th>
<th>encoder ffn</th>
<th>encoder layers</th>
<th>decoder ffn</th>
<th>decoder layers</th>
</tr>
</thead>
<tbody>
<tr>
<td>RNN</td>
<td>256</td>
<td>512</td>
<td>1</td>
<td>1024</td>
<td>1</td>
</tr>
<tr>
<td>Transformer</td>
<td>256</td>
<td>1024</td>
<td>4</td>
<td>1024</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>For strong baseline, please refer to the hyperparameters for <em>transformer-base</em> in Table 3 in <a href="#vaswani2017">Attention is all you need</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">arch_args = Namespace(</span><br><span class="line">    encoder_embed_dim=<span class="number">256</span>,</span><br><span class="line">    encoder_ffn_embed_dim=<span class="number">512</span>,</span><br><span class="line">    encoder_layers=<span class="number">1</span>,</span><br><span class="line">    decoder_embed_dim=<span class="number">256</span>,</span><br><span class="line">    decoder_ffn_embed_dim=<span class="number">1024</span>,</span><br><span class="line">    decoder_layers=<span class="number">1</span>,</span><br><span class="line">    share_decoder_input_output_embed=<span class="literal">True</span>,</span><br><span class="line">    dropout=<span class="number">0.3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # HINT: these patches on parameters for Transformer</span></span><br><span class="line"><span class="comment"># def add_transformer_args(args):</span></span><br><span class="line"><span class="comment">#     args.encoder_attention_heads=4</span></span><br><span class="line"><span class="comment">#     args.encoder_normalize_before=True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     args.decoder_attention_heads=4</span></span><br><span class="line"><span class="comment">#     args.decoder_normalize_before=True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     args.activation_fn=&quot;relu&quot;</span></span><br><span class="line"><span class="comment">#     args.max_source_positions=1024</span></span><br><span class="line"><span class="comment">#     args.max_target_positions=1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     # patches on default parameters for Transformer (those not set above)</span></span><br><span class="line"><span class="comment">#     from fairseq.models.transformer import base_architecture</span></span><br><span class="line"><span class="comment">#     base_architecture(arch_args)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add_transformer_args(arch_args)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> config.use_wandb:</span><br><span class="line">    wandb.config.update(<span class="built_in">vars</span>(arch_args))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = build_model(arch_args, task)</span><br><span class="line">logger.info(model)</span><br></pre></td></tr></table></figure>
<h2 id="Optimization">Optimization</h2>
<h3 id="Loss-Label-Smoothing-Regularization">Loss: Label Smoothing Regularization</h3>
<ul>
<li>let the model learn to generate less concentrated distribution, and prevent over-confidence</li>
<li>sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels</li>
<li>avoids overfitting</li>
</ul>
<p>code <a target="_blank" rel="noopener" href="https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html">source</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelSmoothedCrossEntropyCriterion</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, smoothing, ignore_index=<span class="literal">None</span>, reduce=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.ignore_index = ignore_index</span><br><span class="line">        self.reduce = reduce</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, lprobs, target</span>):</span><br><span class="line">        <span class="keyword">if</span> target.dim() == lprobs.dim() - <span class="number">1</span>:</span><br><span class="line">            target = target.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss</span></span><br><span class="line">        nll_loss = -lprobs.gather(dim=-<span class="number">1</span>, index=target)</span><br><span class="line">        <span class="comment">#  reserve some probability for other labels. thus when calculating cross-entropy,</span></span><br><span class="line">        <span class="comment"># equivalent to summing the log probs of all labels</span></span><br><span class="line">        smooth_loss = -lprobs.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.ignore_index <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            pad_mask = target.eq(self.ignore_index)</span><br><span class="line">            nll_loss.masked_fill_(pad_mask, <span class="number">0.0</span>)</span><br><span class="line">            smooth_loss.masked_fill_(pad_mask, <span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nll_loss = nll_loss.squeeze(-<span class="number">1</span>)</span><br><span class="line">            smooth_loss = smooth_loss.squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.reduce:</span><br><span class="line">            nll_loss = nll_loss.<span class="built_in">sum</span>()</span><br><span class="line">            smooth_loss = smooth_loss.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="comment"># when calculating cross-entropy, add the loss of other labels</span></span><br><span class="line">        eps_i = self.smoothing / lprobs.size(-<span class="number">1</span>)</span><br><span class="line">        loss = (<span class="number">1.0</span> - self.smoothing) * nll_loss + eps_i * smooth_loss</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># generally, 0.1 is good enough</span></span><br><span class="line">criterion = LabelSmoothedCrossEntropyCriterion(</span><br><span class="line">    smoothing=<span class="number">0.1</span>,</span><br><span class="line">    ignore_index=task.target_dictionary.pad(),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Optimizer-Adam-lr-scheduling">Optimizer: Adam + lr scheduling</h3>
<p>Inverse square root scheduling is important to the stability when training Transformer. It’s later used on RNN as well.<br>
Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>=</mo><msubsup><mi>d</mi><mtext>model</mtext><mrow><mo>−</mo><mn>0.5</mn></mrow></msubsup><mo>⋅</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><msup><mrow><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>n</mi><mi>u</mi><mi>m</mi></mrow><mrow><mo>−</mo><mn>0.5</mn></mrow></msup><mo separator="true">,</mo><mrow><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>n</mi><mi>u</mi><mi>m</mi></mrow><mo>⋅</mo><msup><mrow><mi>w</mi><mi>a</mi><mi>r</mi><mi>m</mi><mi>u</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>s</mi></mrow><mrow><mo>−</mo><mn>1.5</mn></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">lrate = d_{\text{model}}^{-0.5}\cdot\min({step\_num}^{-0.5},{step\_num}\cdot{warmup\_steps}^{-1.5})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1555em;vertical-align:-0.2914em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.4086em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0.5</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2914em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1741em;vertical-align:-0.31em;"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0.5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1741em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord mathnormal">s</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1.5</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>code <a target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">source</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NoamOpt</span>:</span><br><span class="line">    <span class="string">&quot;Optim wrapper that implements rate.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_size, factor, warmup, optimizer</span>):</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">param_groups</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.optimizer.param_groups</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">multiply_grads</span>(<span class="params">self, c</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Multiplies grads by a constant *c*.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    p.grad.data.mul_(c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;Update parameters and rate&quot;</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            p[<span class="string">&#x27;lr&#x27;</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rate</span>(<span class="params">self, step = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;Implement `lrate` above&quot;</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> <span class="keyword">not</span> step <span class="keyword">else</span> self.factor * \</span><br><span class="line">            (self.model_size ** (-<span class="number">0.5</span>) *</span><br><span class="line">            <span class="built_in">min</span>(step ** (-<span class="number">0.5</span>), step * self.warmup ** (-<span class="number">1.5</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="Scheduling-Visualized">Scheduling Visualized</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optimizer = NoamOpt(</span><br><span class="line">    model_size=arch_args.encoder_embed_dim,</span><br><span class="line">    factor=config.lr_factor,</span><br><span class="line">    warmup=config.lr_warmup,</span><br><span class="line">    optimizer=torch.optim.AdamW(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>, weight_decay=<span class="number">0.0001</span>))</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100000</span>), [optimizer.rate(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">100000</span>)])</span><br><span class="line">plt.legend([<span class="string">f&quot;<span class="subst">&#123;optimizer.model_size&#125;</span>:<span class="subst">&#123;optimizer.warmup&#125;</span>&quot;</span>])</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="Training-Procedure">Training Procedure</h2>
<h3 id="Training">Training</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fairseq.data <span class="keyword">import</span> iterators</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> GradScaler, autocast</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">epoch_itr, model, task, criterion, optimizer, accum_steps=<span class="number">1</span></span>):</span><br><span class="line">    itr = epoch_itr.next_epoch_itr(shuffle=<span class="literal">True</span>)</span><br><span class="line">    itr = iterators.GroupedIterator(itr, accum_steps) <span class="comment"># gradient accumulation: update every accum_steps samples</span></span><br><span class="line"></span><br><span class="line">    stats = &#123;<span class="string">&quot;loss&quot;</span>: []&#125;</span><br><span class="line">    scaler = GradScaler() <span class="comment"># automatic mixed precision (amp)</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    progress = tqdm.tqdm(itr, desc=<span class="string">f&quot;train epoch <span class="subst">&#123;epoch_itr.epoch&#125;</span>&quot;</span>, leave=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> samples <span class="keyword">in</span> progress:</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        accum_loss = <span class="number">0</span></span><br><span class="line">        sample_size = <span class="number">0</span></span><br><span class="line">        <span class="comment"># gradient accumulation: update every accum_steps samples</span></span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># emptying the CUDA cache after the first step can reduce the chance of OOM</span></span><br><span class="line">                torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">            sample = utils.move_to_cuda(sample, device=device)</span><br><span class="line">            target = sample[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">            sample_size_i = sample[<span class="string">&quot;ntokens&quot;</span>]</span><br><span class="line">            sample_size += sample_size_i</span><br><span class="line"></span><br><span class="line">            <span class="comment"># mixed precision training</span></span><br><span class="line">            <span class="keyword">with</span> autocast():</span><br><span class="line">                net_output = model.forward(**sample[<span class="string">&quot;net_input&quot;</span>])</span><br><span class="line">                lprobs = F.log_softmax(net_output[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">                loss = criterion(lprobs.view(-<span class="number">1</span>, lprobs.size(-<span class="number">1</span>)), target.view(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># logging</span></span><br><span class="line">                accum_loss += loss.item()</span><br><span class="line">                <span class="comment"># back-prop</span></span><br><span class="line">                scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        scaler.unscale_(optimizer)</span><br><span class="line">        optimizer.multiply_grads(<span class="number">1</span> / (sample_size <span class="keyword">or</span> <span class="number">1.0</span>)) <span class="comment"># (sample_size or 1.0) handles the case of a zero gradient</span></span><br><span class="line">        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) <span class="comment"># grad norm clipping prevents gradient exploding</span></span><br><span class="line"></span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line">        scaler.update()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># logging</span></span><br><span class="line">        loss_print = accum_loss/sample_size</span><br><span class="line">        stats[<span class="string">&quot;loss&quot;</span>].append(loss_print)</span><br><span class="line">        progress.set_postfix(loss=loss_print)</span><br><span class="line">        <span class="keyword">if</span> config.use_wandb:</span><br><span class="line">            wandb.log(&#123;</span><br><span class="line">                <span class="string">&quot;train/loss&quot;</span>: loss_print,</span><br><span class="line">                <span class="string">&quot;train/grad_norm&quot;</span>: gnorm.item(),</span><br><span class="line">                <span class="string">&quot;train/lr&quot;</span>: optimizer.rate(),</span><br><span class="line">                <span class="string">&quot;train/sample_size&quot;</span>: sample_size,</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">    loss_print = np.mean(stats[<span class="string">&quot;loss&quot;</span>])</span><br><span class="line">    logger.info(<span class="string">f&quot;training loss: <span class="subst">&#123;loss_print:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> stats</span><br></pre></td></tr></table></figure>
<h3 id="Validation-Inference">Validation &amp; Inference</h3>
<p>To prevent overfitting, validation is required every epoch to validate the performance on unseen data.</p>
<ul>
<li>the procedure is essensially same as training, with the addition of inference step</li>
<li>after validation we can save the model weights</li>
</ul>
<p>Validation loss alone cannot describe the actual performance of the model</p>
<ul>
<li>Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation</li>
<li>We can also manually examine the hypotheses’ quality</li>
<li>We use fairseq’s sequence generator for beam search to generate translation hypotheses</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fairseq&#x27;s beam search generator</span></span><br><span class="line"><span class="comment"># given model and input seqeunce, produce translation hypotheses by beam search</span></span><br><span class="line">sequence_generator = task.build_generator([model], config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">toks, dictionary</span>):</span><br><span class="line">    <span class="comment"># convert from Tensor to human readable sentence</span></span><br><span class="line">    s = dictionary.string(</span><br><span class="line">        toks.<span class="built_in">int</span>().cpu(),</span><br><span class="line">        config.post_process,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> s <span class="keyword">if</span> s <span class="keyword">else</span> <span class="string">&quot;&lt;unk&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">inference_step</span>(<span class="params">sample, model</span>):</span><br><span class="line">    gen_out = sequence_generator.generate([model], sample)</span><br><span class="line">    srcs = []</span><br><span class="line">    hyps = []</span><br><span class="line">    refs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gen_out)):</span><br><span class="line">        <span class="comment"># for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU</span></span><br><span class="line">        srcs.append(decode(</span><br><span class="line">            utils.strip_pad(sample[<span class="string">&quot;net_input&quot;</span>][<span class="string">&quot;src_tokens&quot;</span>][i], task.source_dictionary.pad()),</span><br><span class="line">            task.source_dictionary,</span><br><span class="line">        ))</span><br><span class="line">        hyps.append(decode(</span><br><span class="line">            gen_out[i][<span class="number">0</span>][<span class="string">&quot;tokens&quot;</span>], <span class="comment"># 0 indicates using the top hypothesis in beam</span></span><br><span class="line">            task.target_dictionary,</span><br><span class="line">        ))</span><br><span class="line">        refs.append(decode(</span><br><span class="line">            utils.strip_pad(sample[<span class="string">&quot;target&quot;</span>][i], task.target_dictionary.pad()),</span><br><span class="line">            task.target_dictionary,</span><br><span class="line">        ))</span><br><span class="line">    <span class="keyword">return</span> srcs, hyps, refs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> sacrebleu</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validate</span>(<span class="params">model, task, criterion, log_to_wandb=<span class="literal">True</span></span>):</span><br><span class="line">    logger.info(<span class="string">&#x27;begin validation&#x27;</span>)</span><br><span class="line">    itr = load_data_iterator(task, <span class="string">&quot;valid&quot;</span>, <span class="number">1</span>, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    stats = &#123;<span class="string">&quot;loss&quot;</span>:[], <span class="string">&quot;bleu&quot;</span>: <span class="number">0</span>, <span class="string">&quot;srcs&quot;</span>:[], <span class="string">&quot;hyps&quot;</span>:[], <span class="string">&quot;refs&quot;</span>:[]&#125;</span><br><span class="line">    srcs = []</span><br><span class="line">    hyps = []</span><br><span class="line">    refs = []</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    progress = tqdm.tqdm(itr, desc=<span class="string">f&quot;validation&quot;</span>, leave=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(progress):</span><br><span class="line">            <span class="comment"># validation loss</span></span><br><span class="line">            sample = utils.move_to_cuda(sample, device=device)</span><br><span class="line">            net_output = model.forward(**sample[<span class="string">&quot;net_input&quot;</span>])</span><br><span class="line"></span><br><span class="line">            lprobs = F.log_softmax(net_output[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">            target = sample[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">            sample_size = sample[<span class="string">&quot;ntokens&quot;</span>]</span><br><span class="line">            loss = criterion(lprobs.view(-<span class="number">1</span>, lprobs.size(-<span class="number">1</span>)), target.view(-<span class="number">1</span>)) / sample_size</span><br><span class="line">            progress.set_postfix(valid_loss=loss.item())</span><br><span class="line">            stats[<span class="string">&quot;loss&quot;</span>].append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># do inference</span></span><br><span class="line">            s, h, r = inference_step(sample, model)</span><br><span class="line">            srcs.extend(s)</span><br><span class="line">            hyps.extend(h)</span><br><span class="line">            refs.extend(r)</span><br><span class="line"></span><br><span class="line">    tok = <span class="string">&#x27;zh&#x27;</span> <span class="keyword">if</span> task.cfg.target_lang == <span class="string">&#x27;zh&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;13a&#x27;</span></span><br><span class="line">    stats[<span class="string">&quot;loss&quot;</span>] = torch.stack(stats[<span class="string">&quot;loss&quot;</span>]).mean().item()</span><br><span class="line">    stats[<span class="string">&quot;bleu&quot;</span>] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) <span class="comment"># 計算BLEU score</span></span><br><span class="line">    stats[<span class="string">&quot;srcs&quot;</span>] = srcs</span><br><span class="line">    stats[<span class="string">&quot;hyps&quot;</span>] = hyps</span><br><span class="line">    stats[<span class="string">&quot;refs&quot;</span>] = refs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> config.use_wandb <span class="keyword">and</span> log_to_wandb:</span><br><span class="line">        wandb.log(&#123;</span><br><span class="line">            <span class="string">&quot;valid/loss&quot;</span>: stats[<span class="string">&quot;loss&quot;</span>],</span><br><span class="line">            <span class="string">&quot;valid/bleu&quot;</span>: stats[<span class="string">&quot;bleu&quot;</span>].score,</span><br><span class="line">        &#125;, commit=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    showid = np.random.randint(<span class="built_in">len</span>(hyps))</span><br><span class="line">    logger.info(<span class="string">&quot;example source: &quot;</span> + srcs[showid])</span><br><span class="line">    logger.info(<span class="string">&quot;example hypothesis: &quot;</span> + hyps[showid])</span><br><span class="line">    logger.info(<span class="string">&quot;example reference: &quot;</span> + refs[showid])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># show bleu results</span></span><br><span class="line">    logger.info(<span class="string">f&quot;validation loss:\t<span class="subst">&#123;stats[<span class="string">&#x27;loss&#x27;</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    logger.info(stats[<span class="string">&quot;bleu&quot;</span>].<span class="built_in">format</span>())</span><br><span class="line">    <span class="keyword">return</span> stats</span><br></pre></td></tr></table></figure>
<h2 id="Save-and-Load-Model-Weights">Save and Load Model Weights</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">validate_and_save</span>(<span class="params">model, task, criterion, optimizer, epoch, save=<span class="literal">True</span></span>):</span><br><span class="line">    stats = validate(model, task, criterion)</span><br><span class="line">    bleu = stats[<span class="string">&#x27;bleu&#x27;</span>]</span><br><span class="line">    loss = stats[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> save:</span><br><span class="line">        <span class="comment"># save epoch checkpoints</span></span><br><span class="line">        savedir = Path(config.savedir).absolute()</span><br><span class="line">        savedir.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        check = &#123;</span><br><span class="line">            <span class="string">&quot;model&quot;</span>: model.state_dict(),</span><br><span class="line">            <span class="string">&quot;stats&quot;</span>: &#123;<span class="string">&quot;bleu&quot;</span>: bleu.score, <span class="string">&quot;loss&quot;</span>: loss&#125;,</span><br><span class="line">            <span class="string">&quot;optim&quot;</span>: &#123;<span class="string">&quot;step&quot;</span>: optimizer._step&#125;</span><br><span class="line">        &#125;</span><br><span class="line">        torch.save(check, savedir/<span class="string">f&quot;checkpoint<span class="subst">&#123;epoch&#125;</span>.pt&quot;</span>)</span><br><span class="line">        shutil.copy(savedir/<span class="string">f&quot;checkpoint<span class="subst">&#123;epoch&#125;</span>.pt&quot;</span>, savedir/<span class="string">f&quot;checkpoint_last.pt&quot;</span>)</span><br><span class="line">        logger.info(<span class="string">f&quot;saved epoch checkpoint: <span class="subst">&#123;savedir&#125;</span>/checkpoint<span class="subst">&#123;epoch&#125;</span>.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># save epoch samples</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(savedir/<span class="string">f&quot;samples<span class="subst">&#123;epoch&#125;</span>.<span class="subst">&#123;config.source_lang&#125;</span>-<span class="subst">&#123;config.target_lang&#125;</span>.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> s, h <span class="keyword">in</span> <span class="built_in">zip</span>(stats[<span class="string">&quot;srcs&quot;</span>], stats[<span class="string">&quot;hyps&quot;</span>]):</span><br><span class="line">                f.write(<span class="string">f&quot;<span class="subst">&#123;s&#125;</span>\t<span class="subst">&#123;h&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get best valid bleu</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">getattr</span>(validate_and_save, <span class="string">&quot;best_bleu&quot;</span>, <span class="number">0</span>) &lt; bleu.score:</span><br><span class="line">            validate_and_save.best_bleu = bleu.score</span><br><span class="line">            torch.save(check, savedir/<span class="string">f&quot;checkpoint_best.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">        del_file = savedir / <span class="string">f&quot;checkpoint<span class="subst">&#123;epoch - config.keep_last_epochs&#125;</span>.pt&quot;</span></span><br><span class="line">        <span class="keyword">if</span> del_file.exists():</span><br><span class="line">            del_file.unlink()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stats</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_load_checkpoint</span>(<span class="params">model, optimizer=<span class="literal">None</span>, name=<span class="literal">None</span></span>):</span><br><span class="line">    name = name <span class="keyword">if</span> name <span class="keyword">else</span> <span class="string">&quot;checkpoint_last.pt&quot;</span></span><br><span class="line">    checkpath = Path(config.savedir)/name</span><br><span class="line">    <span class="keyword">if</span> checkpath.exists():</span><br><span class="line">        check = torch.load(checkpath)</span><br><span class="line">        model.load_state_dict(check[<span class="string">&quot;model&quot;</span>])</span><br><span class="line">        stats = check[<span class="string">&quot;stats&quot;</span>]</span><br><span class="line">        step = <span class="string">&quot;unknown&quot;</span></span><br><span class="line">        <span class="keyword">if</span> optimizer != <span class="literal">None</span>:</span><br><span class="line">            optimizer._step = step = check[<span class="string">&quot;optim&quot;</span>][<span class="string">&quot;step&quot;</span>]</span><br><span class="line">        logger.info(<span class="string">f&quot;loaded checkpoint <span class="subst">&#123;checkpath&#125;</span>: step=<span class="subst">&#123;step&#125;</span> loss=<span class="subst">&#123;stats[<span class="string">&#x27;loss&#x27;</span>]&#125;</span> bleu=<span class="subst">&#123;stats[<span class="string">&#x27;bleu&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logger.info(<span class="string">f&quot;no checkpoints found at <span class="subst">&#123;checkpath&#125;</span>!&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Main">Main</h2>
<h3 id="Training-loop">Training loop</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = model.to(device=device)</span><br><span class="line">criterion = criterion.to(device=device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">logger.info(<span class="string">&quot;task: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(task.__class__.__name__))</span><br><span class="line">logger.info(<span class="string">&quot;encoder: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(model.encoder.__class__.__name__))</span><br><span class="line">logger.info(<span class="string">&quot;decoder: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(model.decoder.__class__.__name__))</span><br><span class="line">logger.info(<span class="string">&quot;criterion: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(criterion.__class__.__name__))</span><br><span class="line">logger.info(<span class="string">&quot;optimizer: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(optimizer.__class__.__name__))</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">&quot;num. model params: &#123;:,&#125; (num. trained: &#123;:,&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">        <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()),</span><br><span class="line">        <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad),</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">logger.info(<span class="string">f&quot;max tokens per batch = <span class="subst">&#123;config.max_tokens&#125;</span>, accumulate steps = <span class="subst">&#123;config.accum_steps&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epoch_itr = load_data_iterator(task, <span class="string">&quot;train&quot;</span>, config.start_epoch, config.max_tokens, config.num_workers)</span><br><span class="line">try_load_checkpoint(model, optimizer, name=config.resume)</span><br><span class="line"><span class="keyword">while</span> epoch_itr.next_epoch_idx &lt;= config.max_epoch:</span><br><span class="line">    <span class="comment"># train for one epoch</span></span><br><span class="line">    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)</span><br><span class="line">    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)</span><br><span class="line">    logger.info(<span class="string">&quot;end of epoch &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch_itr.epoch))</span><br><span class="line">    epoch_itr = load_data_iterator(task, <span class="string">&quot;train&quot;</span>, epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)</span><br></pre></td></tr></table></figure>
<h2 id="Submission">Submission</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># averaging a few checkpoints can have a similar effect to ensemble</span></span><br><span class="line">checkdir=config.savedir</span><br><span class="line">!python ./fairseq/scripts/average_checkpoints.py \</span><br><span class="line">--inputs &#123;checkdir&#125; \</span><br><span class="line">--num-epoch-checkpoints <span class="number">5</span> \</span><br><span class="line">--output &#123;checkdir&#125;/avg_last_5_checkpoint.pt</span><br></pre></td></tr></table></figure>
<h3 id="Confirm-model-weights-used-to-generate-submission">Confirm model weights used to generate submission</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># checkpoint_last.pt : latest epoch</span></span><br><span class="line"><span class="comment"># checkpoint_best.pt : highest validation bleu</span></span><br><span class="line"><span class="comment"># avg_last_5_checkpoint.pt:　the average of last 5 epochs</span></span><br><span class="line">try_load_checkpoint(model, name=<span class="string">&quot;avg_last_5_checkpoint.pt&quot;</span>)</span><br><span class="line">validate(model, task, criterion, log_to_wandb=<span class="literal">False</span>)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h3 id="Generate-Prediction">Generate Prediction</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_prediction</span>(<span class="params">model, task, split=<span class="string">&quot;test&quot;</span>, outfile=<span class="string">&quot;./prediction.txt&quot;</span></span>):</span><br><span class="line">    task.load_dataset(split=split, epoch=<span class="number">1</span>)</span><br><span class="line">    itr = load_data_iterator(task, split, <span class="number">1</span>, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    idxs = []</span><br><span class="line">    hyps = []</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    progress = tqdm.tqdm(itr, desc=<span class="string">f&quot;prediction&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(progress):</span><br><span class="line">            <span class="comment"># validation loss</span></span><br><span class="line">            sample = utils.move_to_cuda(sample, device=device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># do inference</span></span><br><span class="line">            s, h, r = inference_step(sample, model)</span><br><span class="line"></span><br><span class="line">            hyps.extend(h)</span><br><span class="line">            idxs.extend(<span class="built_in">list</span>(sample[<span class="string">&#x27;id&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort based on the order before preprocess</span></span><br><span class="line">    hyps = [x <span class="keyword">for</span> _,x <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">zip</span>(idxs,hyps))]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(outfile, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> hyps:</span><br><span class="line">            f.write(h+<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generate_prediction(model, task)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">raise</span></span><br></pre></td></tr></table></figure>
<h2 id="Back-translation">Back-translation</h2>
<h3 id="Train-a-backward-translation-model">Train a backward translation model</h3>
<ol>
<li>Switch the source_lang and target_lang in <strong>config</strong></li>
<li>Change the savedir in <strong>config</strong> (eg. “./checkpoints/transformer-back”)</li>
<li>Train model</li>
</ol>
<h3 id="Generate-synthetic-data-with-backward-model">Generate synthetic data with backward model</h3>
<h4 id="Download-monolingual-data">Download monolingual data</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mono_dataset_name = <span class="string">&#x27;mono&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">mono_prefix = Path(data_dir).absolute() / mono_dataset_name</span><br><span class="line">mono_prefix.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">urls = (</span><br><span class="line">    <span class="string">&#x27;&quot;https://onedrive.live.com/download?cid=3E549F3B24B238B4&amp;resid=3E549F3B24B238B4%214986&amp;authkey=AANUKbGfZx0kM80&quot;&#x27;</span>,</span><br><span class="line"><span class="comment"># # If the above links die, use the following instead.</span></span><br><span class="line"><span class="comment">#     &quot;https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted_zh_corpus.deduped.gz&quot;,</span></span><br><span class="line"><span class="comment"># # If the above links die, use the following instead.</span></span><br><span class="line"><span class="comment">#     &quot;https://mega.nz/#!vMNnDShR!4eHDxzlpzIpdpeQTD-htatU_C7QwcBTwGDaSeBqH534&quot;,</span></span><br><span class="line">)</span><br><span class="line">file_names = (</span><br><span class="line">    <span class="string">&#x27;ted_zh_corpus.deduped.gz&#x27;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> u, f <span class="keyword">in</span> <span class="built_in">zip</span>(urls, file_names):</span><br><span class="line">    path = mono_prefix/f</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> path.exists():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;mega&#x27;</span> <span class="keyword">in</span> u:</span><br><span class="line">            !megadl &#123;u&#125; --path &#123;path&#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            !wget &#123;u&#125; -O &#123;path&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;f&#125;</span> is exist, skip downloading&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> path.suffix == <span class="string">&quot;.tgz&quot;</span>:</span><br><span class="line">        !tar -xvf &#123;path&#125; -C &#123;prefix&#125;</span><br><span class="line">    <span class="keyword">elif</span> path.suffix == <span class="string">&quot;.zip&quot;</span>:</span><br><span class="line">        !unzip -o &#123;path&#125; -d &#123;prefix&#125;</span><br><span class="line">    <span class="keyword">elif</span> path.suffix == <span class="string">&quot;.gz&quot;</span>:</span><br><span class="line">        !gzip -fkd &#123;path&#125;</span><br></pre></td></tr></table></figure>
<h4 id="TODO-clean-corpus">TODO: clean corpus</h4>
<ol>
<li>remove sentences that are too long or too short</li>
<li>unify punctuation</li>
</ol>
<p>hint: you can use clean_s() defined above to do this</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="TODO-Subword-Units">TODO: Subword Units</h4>
<p>Use the spm model of the backward model to tokenize the data into subword units</p>
<p>hint: spm model is located at DATA/raw-data/[dataset]/spm[vocab_num].model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="Binarize">Binarize</h4>
<p>use fairseq to binarize data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">binpath = Path(<span class="string">&#x27;./DATA/data-bin&#x27;</span>, mono_dataset_name)</span><br><span class="line">src_dict_file = <span class="string">&#x27;./DATA/data-bin/ted2020/dict.en.txt&#x27;</span></span><br><span class="line">tgt_dict_file = src_dict_file</span><br><span class="line">monopref = <span class="built_in">str</span>(mono_prefix/<span class="string">&quot;mono.tok&quot;</span>) <span class="comment"># whatever filepath you get after applying subword tokenization</span></span><br><span class="line"><span class="keyword">if</span> binpath.exists():</span><br><span class="line">    <span class="built_in">print</span>(binpath, <span class="string">&quot;exists, will not overwrite!&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    !python -m fairseq_cli.preprocess\</span><br><span class="line">        --source-lang <span class="string">&#x27;zh&#x27;</span>\</span><br><span class="line">        --target-lang <span class="string">&#x27;en&#x27;</span>\</span><br><span class="line">        --trainpref &#123;monopref&#125;\</span><br><span class="line">        --destdir &#123;binpath&#125;\</span><br><span class="line">        --srcdict &#123;src_dict_file&#125;\</span><br><span class="line">        --tgtdict &#123;tgt_dict_file&#125;\</span><br><span class="line">        --workers <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h4 id="TODO-Generate-synthetic-data-with-backward-model">TODO: Generate synthetic data with backward model</h4>
<p>Add binarized monolingual data to the original data directory, and name it with “split_name”</p>
<p>ex. ./DATA/data-bin/ted2020/[split_name].zh-en.[“en”, “zh”].[“bin”, “idx”]</p>
<p>then you can use ‘generate_prediction(model, task, split=“split_name”)’ to generate translation prediction</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add binarized monolingual data to the original data directory, and name it with &quot;split_name&quot;</span></span><br><span class="line"><span class="comment"># ex. ./DATA/data-bin/ted2020/\[split_name\].zh-en.\[&quot;en&quot;, &quot;zh&quot;\].\[&quot;bin&quot;, &quot;idx&quot;\]</span></span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/mono/train.zh-en.zh.<span class="built_in">bin</span> ./DATA/data-<span class="built_in">bin</span>/ted2020/mono.zh-en.zh.<span class="built_in">bin</span></span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/mono/train.zh-en.zh.idx ./DATA/data-<span class="built_in">bin</span>/ted2020/mono.zh-en.zh.idx</span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/mono/train.zh-en.en.<span class="built_in">bin</span> ./DATA/data-<span class="built_in">bin</span>/ted2020/mono.zh-en.en.<span class="built_in">bin</span></span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/mono/train.zh-en.en.idx ./DATA/data-<span class="built_in">bin</span>/ted2020/mono.zh-en.en.idx</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hint: do prediction on split=&#x27;mono&#x27; to create prediction_file</span></span><br><span class="line"><span class="comment"># generate_prediction( ... ,split=... ,outfile=... )</span></span><br></pre></td></tr></table></figure>
<h4 id="TODO-Create-new-dataset">TODO: Create new dataset</h4>
<ol>
<li>Combine the prediction data with monolingual data</li>
<li>Use the original spm model to tokenize data into Subword Units</li>
<li>Binarize data with fairseq</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># hint: tokenize prediction_file with the spm model</span></span><br><span class="line"><span class="comment"># spm_model.encode(line, out_type=str)</span></span><br><span class="line"><span class="comment"># output: ./DATA/rawdata/mono/mono.tok.en &amp; mono.tok.zh</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># hint: use fairseq to binarize these two files again</span></span><br><span class="line"><span class="comment"># binpath = Path(&#x27;./DATA/data-bin/synthetic&#x27;)</span></span><br><span class="line"><span class="comment"># src_dict_file = &#x27;./DATA/data-bin/ted2020/dict.en.txt&#x27;</span></span><br><span class="line"><span class="comment"># tgt_dict_file = src_dict_file</span></span><br><span class="line"><span class="comment"># monopref = ./DATA/rawdata/mono/mono.tok # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)</span></span><br><span class="line"><span class="comment"># if binpath.exists():</span></span><br><span class="line"><span class="comment">#     print(binpath, &quot;exists, will not overwrite!&quot;)</span></span><br><span class="line"><span class="comment"># else:</span></span><br><span class="line"><span class="comment">#     !python -m fairseq_cli.preprocess\</span></span><br><span class="line"><span class="comment">#         --source-lang &#x27;zh&#x27;\</span></span><br><span class="line"><span class="comment">#         --target-lang &#x27;en&#x27;\</span></span><br><span class="line"><span class="comment">#         --trainpref &#123;monopref&#125;\</span></span><br><span class="line"><span class="comment">#         --destdir &#123;binpath&#125;\</span></span><br><span class="line"><span class="comment">#         --srcdict &#123;src_dict_file&#125;\</span></span><br><span class="line"><span class="comment">#         --tgtdict &#123;tgt_dict_file&#125;\</span></span><br><span class="line"><span class="comment">#         --workers 2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a new dataset from all the files prepared above</span></span><br><span class="line">!cp -r ./DATA/data-<span class="built_in">bin</span>/ted2020/ ./DATA/data-<span class="built_in">bin</span>/ted2020_with_mono/</span><br><span class="line"></span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/synthetic/train.zh-en.zh.<span class="built_in">bin</span> ./DATA/data-<span class="built_in">bin</span>/ted2020_with_mono/train1.en-zh.zh.<span class="built_in">bin</span></span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/synthetic/train.zh-en.zh.idx ./DATA/data-<span class="built_in">bin</span>/ted2020_with_mono/train1.en-zh.zh.idx</span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/synthetic/train.zh-en.en.<span class="built_in">bin</span> ./DATA/data-<span class="built_in">bin</span>/ted2020_with_mono/train1.en-zh.en.<span class="built_in">bin</span></span><br><span class="line">!cp ./DATA/data-<span class="built_in">bin</span>/synthetic/train.zh-en.en.idx ./DATA/data-<span class="built_in">bin</span>/ted2020_with_mono/train1.en-zh.en.idx</span><br></pre></td></tr></table></figure>
<p>Created new dataset “ted2020_with_mono”</p>
<ol>
<li>Change the datadir in <strong>config</strong> (“./DATA/data-bin/ted2020_with_mono”)</li>
<li>Switch back the source_lang and target_lang in <strong>config</strong> (“en”, “zh”)</li>
<li>Change the savedir in <strong>config</strong> (eg. “./checkpoints/transformer-bt”)</li>
<li>Train model</li>
</ol>
<h2 id="References">References</h2>
<ol>
<li><a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., … &amp; Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).</li>
<li><a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).</li>
<li><a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., &amp; Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).</li>
<li><a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).</li>
<li><a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., &amp; Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).</li>
<li><a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., &amp; Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).</li>
<li><a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., &amp; Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus">https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus</a></li>
<li><a target="_blank" rel="noopener" href="https://ithelp.ithome.com.tw/articles/10233122">https://ithelp.ithome.com.tw/articles/10233122</a></li>
<li><a target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://isSeymour.github.io/butterflyblog">isSeymour</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://isseymour.github.io/butterflyblog/2024/09/23/ML2021-HW5/">https://isseymour.github.io/butterflyblog/2024/09/23/ML2021-HW5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://isSeymour.github.io/butterflyblog" target="_blank">isSeymour</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/butterflyblog/tags/ML/">ML</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/ML2021/ML2021.png" data-sites="wechat,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY1.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY1.jpg" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY2.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY2.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/butterflyblog/2024/09/21/ML2021-HW4/" title="ML2021 - HW 4"><img class="cover" src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/ML2021/HW3/header.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">ML2021 - HW 4</div></div></a></div><div class="next-post pull-right"><a href="/butterflyblog/2024/10/06/Graph-DeepWalk/" title="DeepWalk 图嵌入：维基百科词条"><img class="cover" src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/CS224W/DeepWalk/DW.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">DeepWalk 图嵌入：维基百科词条</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/butterflyblog/2024/09/01/Data-Visualization/" title="Data Visualization - Kaggle 官方课程"><img class="cover" src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/Kaggle/Data-Visualization/data-visualization.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-01</div><div class="title">Data Visualization - Kaggle 官方课程</div></div></a></div><div><a href="/butterflyblog/2024/08/31/Intermediate-ML/" title="Intermediate ML - Kaggle 官方课程"><img class="cover" src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/Kaggle/intermediate-machine-learning.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-31</div><div class="title">Intermediate ML - Kaggle 官方课程</div></div></a></div><div><a href="/butterflyblog/2024/08/29/INTRO-to-ML/" title="INTRO to ML - Kaggle 官方课程"><img class="cover" src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/Kaggle/intro-to-machine-learning.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-29</div><div class="title">INTRO to ML - Kaggle 官方课程</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/T6.jpg" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">isSeymour</div><div class="author-info__description">志之所趋，无远弗届，穷山距海，不能限也。</div></div><div class="card-info-data site-data is-center"><a href="/butterflyblog/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/butterflyblog/tags/"><div class="headline">标签</div><div class="length-num">34</div></a><a href="/butterflyblog/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isSeymour/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://isSeymour.github.io/profile/" target="_blank" title="学术主页"><i class="fa-regular fa-address-card" style="color: #000000;"></i></a><a class="social-icon" href="https://github.com/isSeymour/" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="https://space.bilibili.com/79699613/" target="_blank" title="B站"><i class="fa-brands fa-bilibili" style="color: #000000;"></i></a><a class="social-icon" href="https://blog.csdn.net/m0_63205991/" target="_blank" title="CSDN"><i class="fa-solid fa-code" style="color: #000000;"></i></a><a class="social-icon" href="mailto:isSeymour@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Homework 5 - Sequence-to-sequence</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-21-Updates"><span class="toc-text">(4&#x2F;21 Updates)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-14-Updates"><span class="toc-text">(4&#x2F;14 Updates)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sequence-to-Sequence-Introduction"><span class="toc-text">Sequence-to-Sequence Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Homework-Description"><span class="toc-text">Homework Description</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Download-and-import-required-packages"><span class="toc-text">Download and import required packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fix-random-seed"><span class="toc-text">Fix random seed</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset-Information"><span class="toc-text">Dataset Information</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#En-Zh-Bilingual-Parallel-Corpus"><span class="toc-text">En-Zh Bilingual Parallel Corpus</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Testdata"><span class="toc-text">Testdata</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset-Download"><span class="toc-text">Dataset Download</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Install-megatools-optional"><span class="toc-text">Install megatools (optional)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Download-and-extract"><span class="toc-text">Download and extract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Language"><span class="toc-text">Language</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Preprocess-files"><span class="toc-text">Preprocess files</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Split-into-train-valid"><span class="toc-text">Split into train&#x2F;valid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Subword-Units"><span class="toc-text">Subword Units</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Binarize-the-data-with-fairseq"><span class="toc-text">Binarize the data with fairseq</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Configuration-for-Experiments"><span class="toc-text">Configuration for Experiments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logging"><span class="toc-text">Logging</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-Environment"><span class="toc-text">CUDA Environment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataloading"><span class="toc-text">Dataloading</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#We-borrow-the-TranslationTask-from-fairseq"><span class="toc-text">We borrow the TranslationTask from fairseq</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset-Iterator"><span class="toc-text">Dataset Iterator</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Architecture"><span class="toc-text">Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder"><span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention"><span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Seq2Seq"><span class="toc-text">Seq2Seq</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Initialization"><span class="toc-text">Model Initialization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Architecture-Related-Configuration"><span class="toc-text">Architecture Related Configuration</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimization"><span class="toc-text">Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-Label-Smoothing-Regularization"><span class="toc-text">Loss: Label Smoothing Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimizer-Adam-lr-scheduling"><span class="toc-text">Optimizer: Adam + lr scheduling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scheduling-Visualized"><span class="toc-text">Scheduling Visualized</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-Procedure"><span class="toc-text">Training Procedure</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Training"><span class="toc-text">Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Validation-Inference"><span class="toc-text">Validation &amp; Inference</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Save-and-Load-Model-Weights"><span class="toc-text">Save and Load Model Weights</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Main"><span class="toc-text">Main</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-loop"><span class="toc-text">Training loop</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Submission"><span class="toc-text">Submission</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Confirm-model-weights-used-to-generate-submission"><span class="toc-text">Confirm model weights used to generate submission</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generate-Prediction"><span class="toc-text">Generate Prediction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Back-translation"><span class="toc-text">Back-translation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-a-backward-translation-model"><span class="toc-text">Train a backward translation model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generate-synthetic-data-with-backward-model"><span class="toc-text">Generate synthetic data with backward model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Download-monolingual-data"><span class="toc-text">Download monolingual data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TODO-clean-corpus"><span class="toc-text">TODO: clean corpus</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TODO-Subword-Units"><span class="toc-text">TODO: Subword Units</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Binarize"><span class="toc-text">Binarize</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TODO-Generate-synthetic-data-with-backward-model"><span class="toc-text">TODO: Generate synthetic data with backward model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TODO-Create-new-dataset"><span class="toc-text">TODO: Create new dataset</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/butterflyblog/2024/10/14/CS224W_Colab_2/" title="CS224W - Colab 2"><img src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/CS224W/GCN/shared-parameters.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="CS224W - Colab 2"/></a><div class="content"><a class="title" href="/butterflyblog/2024/10/14/CS224W_Colab_2/" title="CS224W - Colab 2">CS224W - Colab 2</a><time datetime="2024-10-14T14:00:00.000Z" title="发表于 2024-10-14 22:00:00">2024-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/butterflyblog/2024/10/14/GCN/" title="GCN 图卷积神经网络"><img src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/CS224W/GCN/DeepGraphEncoders.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="GCN 图卷积神经网络"/></a><div class="content"><a class="title" href="/butterflyblog/2024/10/14/GCN/" title="GCN 图卷积神经网络">GCN 图卷积神经网络</a><time datetime="2024-10-14T09:00:00.000Z" title="发表于 2024-10-14 17:00:00">2024-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/butterflyblog/2024/10/12/Graph-PageRank/" title="PageRank:《哈利·波特》人物节点重要度"><img src="https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/CS224W/PageRank/Algorithm.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="PageRank:《哈利·波特》人物节点重要度"/></a><div class="content"><a class="title" href="/butterflyblog/2024/10/12/Graph-PageRank/" title="PageRank:《哈利·波特》人物节点重要度">PageRank:《哈利·波特》人物节点重要度</a><time datetime="2024-10-12T14:00:00.000Z" title="发表于 2024-10-12 22:00:00">2024-10-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/isSeymour/PicGo/posts/ML2021/ML2021.png')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By isSeymour</div><div class="footer_custom_text">欢迎乘坐我的生活地铁！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/butterflyblog/js/utils.js"></script><script src="/butterflyblog/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23liAZxomGv7Hapw8g',
      clientSecret: 'f7cafde192c4ada8bef4b76952c422d90575cf8b',
      repo: 'gitalk',
      owner: 'isSeymour',
      admin: ['isSeymour'],
      id: 'd5ffa3fe0c662356610d0ee5202a2630',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/butterflyblog/js/search/local-search.js"></script></div></div></body></html>