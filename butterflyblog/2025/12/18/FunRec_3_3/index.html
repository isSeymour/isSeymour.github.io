<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>精排（三）序列建模 | isSeymour</title><meta name="author" content="isSeymour"><meta name="copyright" content="isSeymour"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参考  精排（三）序列建模 在上一节中，我们探讨了如何通过各类特征交叉模型，让机器自动学习特征之间复杂的组合关系。无论是二阶交叉的FM、AFM，还是高阶交叉的DCN、xDeepFM，它们的核心目标都是从一个静态的特征集合中挖掘出有价值的信息。然而，这些模型普遍存在一个共同的局限：它们大多将用户的历史行为看作一个无序的&quot;物品袋&quot;（a bag of items），如同用户的兴趣是">
<meta property="og:type" content="article">
<meta property="og:title" content="精排（三）序列建模">
<meta property="og:url" content="https://isseymour.github.io/butterflyblog/2025/12/18/FunRec_3_3/index.html">
<meta property="og:site_name" content="isSeymour">
<meta property="og:description" content="参考  精排（三）序列建模 在上一节中，我们探讨了如何通过各类特征交叉模型，让机器自动学习特征之间复杂的组合关系。无论是二阶交叉的FM、AFM，还是高阶交叉的DCN、xDeepFM，它们的核心目标都是从一个静态的特征集合中挖掘出有价值的信息。然而，这些模型普遍存在一个共同的局限：它们大多将用户的历史行为看作一个无序的&quot;物品袋&quot;（a bag of items），如同用户的兴趣是">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://datawhalechina.github.io/fun-rec/_images/din_architecture.png">
<meta property="article:published_time" content="2025-12-18T07:00:00.000Z">
<meta property="article:modified_time" content="2025-12-18T07:00:00.000Z">
<meta property="article:author" content="isSeymour">
<meta property="article:tag" content="精排">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://datawhalechina.github.io/fun-rec/_images/din_architecture.png"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/IC1011.ico"><link rel="canonical" href="https://isseymour.github.io/butterflyblog/2025/12/18/FunRec_3_3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/butterflyblog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/butterflyblog/',
  algolia: undefined,
  localSearch: {"path":"/butterflyblog/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '精排（三）序列建模',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-12-18 15:00:00'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyblog/code/style.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/T6.jpg" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/butterflyblog/archives/"><div class="headline">文章</div><div class="length-num">89</div></a><a href="/butterflyblog/tags/"><div class="headline">标签</div><div class="length-num">43</div></a><a href="/butterflyblog/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-bookmark"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/archives/"><i class="fa-fw fa-solid fa-timeline"></i><span> 总览</span></a></li><li><a class="site-page child" href="/butterflyblog/tags/"><i class="fa-fw fa-sharp fa-solid fa-hashtag"></i><span> 标签</span></a></li><li><a class="site-page child" href="/butterflyblog/categories/"><i class="fa-fw fa-sharp fa-solid fa-folder"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-sharp fa-solid fa-list"></i><span> 功能</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/ctf/"><i class="fa-fw fa-solid fa-shield-halved"></i><span> CTF</span></a></li><li><a class="site-page child" href="/butterflyblog/music/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/butterflyblog/tools/"><i class="fa-fw fa-solid fa-screwdriver-wrench"></i><span> 下载</span></a></li><li><a class="site-page child" href="/butterflyblog/link/"><i class="fa-fw fa-solid fa-paper-plane"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-university"></i><span> 算法</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/algorithm/hot100/"><i class="fa-fw fa-solid fa-fire"></i><span> HOT100</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/labuladong/"><i class="fa-fw fa-solid fa-coffee"></i><span> labuladong</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/luogu/"><i class="fa-fw fa-solid fa-magnet"></i><span> 洛谷</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/template/"><i class="fa-fw fa-solid fa-code"></i><span> Template</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-plane"></i><span> 旅途</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/journey/Diary/"><i class="fa-fw fa-solid fa-book"></i><span> 日记本</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/BAOYAN/"><i class="fa-fw fa-solid fa-hashtag"></i><span> BAOYAN</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/english/"><i class="fa-fw fa-solid fa-globe"></i><span> 英语</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/mathAI/"><i class="fa-fw fa-solid fa-bar-chart"></i><span> 数学&amp;AI</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/RecSys/"><i class="fa-fw fa-solid fa-thumbs-up"></i><span> RecSys</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-user"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/about/"><i class="fa-fw fa-regular fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/butterflyblog/message/"><i class="fa-fw fa-solid fa-message"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/butterflyblog/develop/"><i class="fa-fw fa-brands fa-windows"></i><span> 开发日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://datawhalechina.github.io/fun-rec/_images/din_architecture.png')"><nav id="nav"><span id="blog-info"><a href="/butterflyblog/" title="isSeymour"><span class="site-name">isSeymour</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-bookmark"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/archives/"><i class="fa-fw fa-solid fa-timeline"></i><span> 总览</span></a></li><li><a class="site-page child" href="/butterflyblog/tags/"><i class="fa-fw fa-sharp fa-solid fa-hashtag"></i><span> 标签</span></a></li><li><a class="site-page child" href="/butterflyblog/categories/"><i class="fa-fw fa-sharp fa-solid fa-folder"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-sharp fa-solid fa-list"></i><span> 功能</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/ctf/"><i class="fa-fw fa-solid fa-shield-halved"></i><span> CTF</span></a></li><li><a class="site-page child" href="/butterflyblog/music/"><i class="fa-fw fa-solid fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/butterflyblog/tools/"><i class="fa-fw fa-solid fa-screwdriver-wrench"></i><span> 下载</span></a></li><li><a class="site-page child" href="/butterflyblog/link/"><i class="fa-fw fa-solid fa-paper-plane"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-university"></i><span> 算法</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/algorithm/hot100/"><i class="fa-fw fa-solid fa-fire"></i><span> HOT100</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/labuladong/"><i class="fa-fw fa-solid fa-coffee"></i><span> labuladong</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/luogu/"><i class="fa-fw fa-solid fa-magnet"></i><span> 洛谷</span></a></li><li><a class="site-page child" href="/butterflyblog/algorithm/template/"><i class="fa-fw fa-solid fa-code"></i><span> Template</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-plane"></i><span> 旅途</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/journey/Diary/"><i class="fa-fw fa-solid fa-book"></i><span> 日记本</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/BAOYAN/"><i class="fa-fw fa-solid fa-hashtag"></i><span> BAOYAN</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/english/"><i class="fa-fw fa-solid fa-globe"></i><span> 英语</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/mathAI/"><i class="fa-fw fa-solid fa-bar-chart"></i><span> 数学&amp;AI</span></a></li><li><a class="site-page child" href="/butterflyblog/journey/RecSys/"><i class="fa-fw fa-solid fa-thumbs-up"></i><span> RecSys</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-user"></i><span> 关于</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/butterflyblog/about/"><i class="fa-fw fa-regular fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/butterflyblog/message/"><i class="fa-fw fa-solid fa-message"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/butterflyblog/develop/"><i class="fa-fw fa-brands fa-windows"></i><span> 开发日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">精排（三）序列建模</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-18T07:00:00.000Z" title="发表于 2025-12-18 15:00:00">2025-12-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-18T07:00:00.000Z" title="更新于 2025-12-18 15:00:00">2025-12-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/butterflyblog/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>44分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="精排（三）序列建模"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/fun-rec/chapter_2_ranking/3.sequence.html">参考</a></p>
</blockquote>
<h1>精排（三）序列建模</h1>
<p>在上一节中，我们探讨了如何通过各类特征交叉模型，让机器自动学习特征之间复杂的组合关系。无论是二阶交叉的FM、AFM，还是高阶交叉的DCN、xDeepFM，它们的核心目标都是从一个静态的特征集合中挖掘出有价值的信息。然而，这些模型普遍存在一个共同的局限：它们大多将用户的历史行为看作一个无序的&quot;物品袋&quot;（a bag of items），如同用户的兴趣是一个静态的表示。</p>
<p>但用户的兴趣不是静止的，而是具有明显的<strong>时序性</strong>和<strong>动态演化</strong>特点。一个用户先浏览&quot;鼠标&quot;再浏览&quot;显示器&quot;，与先浏览&quot;小说&quot;再浏览&quot;显示器&quot;，这两个行为序列背后指向的购买意图截然不同。前者可能是一位正在组装电脑的数码爱好者，而后者可能只是在工作之余的随性浏览。传统的特征交叉模型难以捕捉这种蕴含在行为顺序中的、随时间变化的意图。</p>
<p>因此，本节我们将转换视角，不再将用户历史看作一堆静态特征的集合，而是将其视为一个动态的序列。我们将聚焦于如何对用户的行为序列进行建模，从这个序列中挖掘出用户动态、演化的兴趣。接下来，我们将介绍工业界在序列建模方向上的三个代表性模型：DIN、DIEN和DSIN，看看它们是如何解决这个核心挑战的。</p>
<h2 id="1、DIN：局部激活的注意力机制">1、DIN：局部激活的注意力机制</h2>
<p>在大型电商平台中，用户的兴趣是<strong>多样</strong>的。一个用户可能在一段时间内，既关注数码产品，又浏览运动装备，还会购买生活用品。在传统的深度学习模型（即Embedding&amp;MLP范式）中，通常的做法是将用户所有的历史行为（如点击过的商品ID）对应的Embedding向量通过池化（Pooling）操作，压缩成一个<strong>固定长度的向量</strong>来代表该用户。</p>
<p>这个固定长度的用户向量，很快就成为了表达用户多样兴趣的瓶颈。想象一下，无论系统准备向这个用户推荐&quot;跑鞋&quot;还是&quot;手机&quot;，代表他的都是同一个向量。这个向量试图&quot;一视同仁&quot;地蕴含该用户所有的兴趣点，这不仅非常困难，而且在面对具体推荐任务时显得不够聚焦。为了增强表达能力而粗暴地增加向量维度，又会带来参数量爆炸和过拟合的风险。</p>
<h4 id="DIN的核心思想：局部激活-Local-Activation">DIN的核心思想：局部激活 (Local Activation)</h4>
<p>深度兴趣网络（Deep Interest Network, DIN）的提出者们发现，用户的某一次具体点击行为，通常只由其历史兴趣中的<strong>一部分</strong>所&quot;激活&quot;。当向一位数码爱好者推荐&quot;机械键盘&quot;时，真正起决定性作用的，很可能是他最近浏览&quot;游戏鼠标&quot;和&quot;显卡&quot;的行为，而不是他上个月购买的&quot;跑鞋&quot;。</p>
<p>基于此，DIN提出了一个观点：<strong>用户的兴趣表示不应该是固定的，而应是根据当前的候选广告（Target Ad）不同而动态变化的。</strong></p>
<p><img src="https://datawhalechina.github.io/fun-rec/_images/din_architecture.png" alt="DIN模型架构图（右）及其与基准模型（左）的对比"></p>
<h4 id="技术实现：注意力机制">技术实现：注意力机制</h4>
<p>为了实现&quot;局部激活&quot;这一思想，DIN在模型中引入了一个关键模块——<strong>局部激活单元（Local Activation Unit）</strong>，其本质就是<strong>注意力机制</strong>。如上图右侧所示，DIN不再像基准模型(:numref:<code>din_architecture</code> 左)那样对所有历史行为的Embedding进行简单的池化，而是进行了一次&quot;加权求和&quot;。</p>
<p>这个权重（即注意力分数）的计算，体现了DIN的核心思想。具体来说，对于一个给定的用户U和候选广告A，用户的兴趣表示向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">v</mi><mi>U</mi></msub><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{U}(A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span>是这样计算的：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold-italic">v</mi><mi>U</mi></msub><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">v</mi><mi>A</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">e</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">e</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">e</mi><mi>H</mi></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></munderover><mi>a</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">e</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">v</mi><mi>A</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold-italic">e</mi><mi>j</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></munderover><msub><mi>w</mi><mi>j</mi></msub><msub><mi mathvariant="bold-italic">e</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{U}(A)=f(\boldsymbol{v}_{A},\boldsymbol{e}_{1},\boldsymbol{e}_{2},\ldots,\boldsymbol{e}_{H})=\sum_{j=1}^{H}a(\boldsymbol{e}_{j},\boldsymbol{v}_{A})\boldsymbol{e}_{j}=\sum_{j=1}^{H}w_{j}\boldsymbol{e}_{j}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中：</p>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">e</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">e</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">e</mi><mi>H</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{e}_{1}, \boldsymbol{e}_{2}, \ldots, \boldsymbol{e}_{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是用户U的历史行为Embedding向量列表。</p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">v</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5944em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是候选广告A的Embedding向量。</p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">e</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">v</mi><mi>A</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a(\boldsymbol{e}_{j}, \boldsymbol{v}_{A})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 是一个激活单元（通常是一个小型前馈神经网络），它接收历史行为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">e</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{e}_{j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>和候选广告<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">v</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5944em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>作为输入，输出一个权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{w}_{j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>。这个权重就代表了历史行为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">e</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{e}_{j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>在面对广告<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">v</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5944em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>时的&quot;相关性&quot;或&quot;注意力得分&quot;。</p>
</li>
</ul>
<p>通过这个公式，用户的最终兴趣表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">v</mi><mi>U</mi></msub><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{U}(A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span>不再是一个固定的向量，而是与候选广告A紧密相关。与广告A越相关的历史行为，会获得越高的权重，从而在最终的兴趣向量中占据主导地位。</p>
<p>一个值得注意的细节是，DIN计算出的注意力权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{w}_{j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>没有经过Softmax归一化。这意味着<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∑</mo><msub><mi mathvariant="bold-italic">w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\sum \boldsymbol{w}_{j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>不一定等于1。这样设计的目的是为了保留用户兴趣的绝对强度。例如，如果一个用户的历史行为大部分都与某个广告高度相关，那么加权和之后的向量模长就会比较大，反之则较小。这种设计使得模型不仅能捕捉兴趣的&quot;方向&quot;，还能感知兴趣的&quot;强度&quot;。</p>
<h4 id="代码">代码</h4>
<ul>
<li>DIN的注意力机制通过将候选广告与历史行为进行多角度交互来计算权重。</li>
<li>这种设计的关键在于：通过[query, keys, query-keys, query*keys]四种交互方式，模型能够从多个角度衡量历史行为与候选广告的相关性，同时不使用softmax归一化以保留兴趣强度信息。<br>
<code>din.py</code> 文件：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .utils <span class="keyword">import</span> (</span><br><span class="line">    build_input_layer,</span><br><span class="line">    build_group_feature_embedding_table_dict,</span><br><span class="line">    concat_group_embedding,</span><br><span class="line">    get_linear_logits,</span><br><span class="line">    add_tensor_func,</span><br><span class="line">    parse_din_feature_columns,</span><br><span class="line">    concat_func,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> .layers <span class="keyword">import</span> DNNs, DinAttentionLayer, PredictLayer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_din_model</span>(<span class="params">feature_columns, model_config</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建 DIN (深度兴趣网络) 排序模型。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        feature_columns: FeatureColumn 列表</span></span><br><span class="line"><span class="string">        model_config: 包含以下参数的字典:</span></span><br><span class="line"><span class="string">            - dnn_units: list, 隐藏层单元数包括输出大小 (默认 [128, 64, 1])</span></span><br><span class="line"><span class="string">            - linear_logits: bool, 是否添加线性项 (默认 True)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        (model, None, None): 排序模型元组</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dnn_units = model_config.get(<span class="string">&quot;dnn_units&quot;</span>, [<span class="number">128</span>, <span class="number">64</span>, <span class="number">1</span>])</span><br><span class="line">    use_linear_logits = model_config.get(<span class="string">&quot;linear_logits&quot;</span>, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入和嵌入</span></span><br><span class="line">    input_layer_dict = build_input_layer(feature_columns)</span><br><span class="line">    group_embedding_feature_dict = build_group_feature_embedding_table_dict(</span><br><span class="line">        feature_columns, input_layer_dict, prefix=<span class="string">&quot;embedding/&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 来自 &#x27;dnn&#x27; 组的基础 DNN 输入</span></span><br><span class="line">    dnn_inputs = concat_group_embedding(group_embedding_feature_dict, <span class="string">&quot;dnn&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对序列特征进行 DIN 注意力机制</span></span><br><span class="line">    din_output_list = []</span><br><span class="line">    din_feature_list = parse_din_feature_columns(feature_columns)</span><br><span class="line">    <span class="keyword">for</span> k_name, v_name <span class="keyword">in</span> din_feature_list:</span><br><span class="line">        query_feature = group_embedding_feature_dict[<span class="string">&quot;din_sequence&quot;</span>][k_name]</span><br><span class="line">        key_feature = group_embedding_feature_dict[<span class="string">&quot;din_sequence&quot;</span>][v_name]</span><br><span class="line">        din_output = DinAttentionLayer(name=v_name + <span class="string">&quot;_din_layer&quot;</span>)(</span><br><span class="line">            [</span><br><span class="line">                query_feature,</span><br><span class="line">                key_feature,</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        din_output_list.append(din_output)</span><br><span class="line">    din_output = concat_func(din_output_list, axis=<span class="number">1</span>, flatten=<span class="literal">True</span>)</span><br><span class="line">    dnn_inputs = concat_func([dnn_inputs, din_output], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DNN 塔</span></span><br><span class="line">    dnn_logits = DNNs(dnn_units, use_bn=<span class="literal">True</span>)(dnn_inputs)</span><br><span class="line">    <span class="keyword">if</span> use_linear_logits:</span><br><span class="line">        linear_logit = get_linear_logits(input_layer_dict, feature_columns)</span><br><span class="line">        dnn_logits = add_tensor_func(</span><br><span class="line">            [dnn_logits, linear_logit], name=<span class="string">&quot;din_linear_logits&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出: 遵循排序约定以确保 (batch,) 标签兼容性</span></span><br><span class="line">    final_logits = tf.keras.layers.Flatten()(dnn_logits)</span><br><span class="line">    output = tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>, name=<span class="string">&quot;din_output&quot;</span>)(</span><br><span class="line">        final_logits</span><br><span class="line">    )</span><br><span class="line">    output = tf.keras.layers.Flatten()(output)</span><br><span class="line">    model = tf.keras.models.Model(</span><br><span class="line">        inputs=<span class="built_in">list</span>(input_layer_dict.values()), outputs=output</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2、DIEN：兴趣的演化建模">2、DIEN：兴趣的演化建模</h2>
<p>DIN成功地捕捉了用户兴趣的&quot;多样性&quot;和&quot;局部激活&quot;特性，但它仍然存在一个局限：它将用户的历史行为看作是一个无序的集合，忽略了行为之间的<strong>时序依赖关系</strong>。用户的兴趣不仅是多样的，更是在持续<strong>演化</strong>的。</p>
<p>为了解决这个问题，深度兴趣演化网络（Deep Interest Evolution Network, DIEN） 被提出。DIEN认为，我们不仅要关注哪些历史兴趣是相关的，更要理解这些兴趣是如何一步步演化至今的。</p>
<p><img src="https://datawhalechina.github.io/fun-rec/_images/dien.png" alt="DIEN模型架构图"></p>
<p>DIEN的核心思想是，直接对原始、显性的行为序列建模是不够的。行为只是表象，我们更应该关注行为背后那个潜在的、抽象的 <strong>&quot;兴趣&quot;状态</strong>，并对这个兴趣状态的演化过程进行建模。为此，DIEN设计了一个两阶段结构，如上图所示。</p>
<h4 id="第一阶段：兴趣提取层-Interest-Extractor-Layer">第一阶段：兴趣提取层 (Interest Extractor Layer)</h4>
<p>这一层的目标是从原始的行为序列中，抽取出更能代表&quot;潜在兴趣&quot;的<strong>兴趣状态序列</strong>。</p>
<p>DIEN使用门控循环单元（GRU）来按时间顺序处理用户的行为Embedding序列<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">e</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">e</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">e</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">{\boldsymbol{e}_1, \boldsymbol{e}_2, \dots, \boldsymbol{e}_T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>。理论上，GRU在t时刻的隐状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{h}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>就捕捉了到该时刻为止的序列信息。但DIEN的作者认为，这样的隐状态还不足以精准地代表&quot;兴趣&quot;。</p>
<p>因此，他们引入了一项关键创新：<strong>辅助损失 (Auxiliary Loss)</strong>。其核心思想是：<strong>用户在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>时刻的兴趣，直接导致了他在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>时刻的行为</strong>。基于此，DIEN增加了一个辅助的监督任务：用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>时刻的兴趣状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{h}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>去预测用户在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>时刻的真实行为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{e}_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<p>具体地，辅助损失<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>a</mi><mi>u</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{aux}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ux</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>定义如下：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mrow><mi>a</mi><mi>u</mi><mi>x</mi></mrow></msub><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><mo fence="true">(</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy="false">(</mo><msubsup><mi mathvariant="bold-italic">h</mi><mi>t</mi><mi>i</mi></msubsup><mo separator="true">,</mo><msubsup><mi mathvariant="bold-italic">e</mi><mrow><mi>b</mi><mo stretchy="false">[</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mi>i</mi></msubsup><mo stretchy="false">)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>σ</mi><mo stretchy="false">(</mo><msubsup><mi mathvariant="bold-italic">h</mi><mi>t</mi><mi>i</mi></msubsup><mo separator="true">,</mo><msubsup><mi><mover accent="true"><mi mathvariant="bold-italic">e</mi><mo>^</mo></mover></mi><mrow><mi>b</mi><mo stretchy="false">[</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mi>i</mi></msubsup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">L_{aux}=-\frac{1}{N}\left(\sum_{i=1}^{N}\sum_{t=1}^{T}\log\sigma(\boldsymbol{h}^i_t,\boldsymbol{e}^i_{b[t+1]})+\log(1-\sigma(\boldsymbol{h}^i_t,\boldsymbol{\hat{e}}^i_{b[t+1]}))\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ux</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.909em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8747em;"><span style="top:-2.428em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mopen mtight">[</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.447em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.909em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord boldsymbol">e</span></span><span style="top:-3.0134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9224em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mopen mtight">[</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span><span style="top:-3.1608em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.422em;"><span></span></span></span></span></span></span><span class="mclose">))</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></p>
<p>其中：</p>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold-italic">h</mi><mi>t</mi><mi>i</mi></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{h}^i_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.156em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.909em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> 是用户i在t时刻的兴趣状态（即GRU的隐状态）。</p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold-italic">e</mi><mrow><mi>b</mi><mo stretchy="false">[</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mi>i</mi></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{e}^i_{b[t+1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3217em;vertical-align:-0.497em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-2.378em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mopen mtight">[</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.497em;"><span></span></span></span></span></span></span></span></span></span> 是用户i在t+1时刻真实点击的物品Embedding（正样本）。</p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi><mover accent="true"><mi mathvariant="bold-italic">e</mi><mo>^</mo></mover></mi><mrow><mi>b</mi><mo stretchy="false">[</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mi>i</mi></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{\hat{e}}^i_{b[t+1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3444em;vertical-align:-0.422em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord boldsymbol">e</span></span><span style="top:-3.0134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9224em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mopen mtight">[</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span><span style="top:-3.1608em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.422em;"><span></span></span></span></span></span></span></span></span></span> 是从物品池中负采样得到的物品Embedding（负样本）。</p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sigma(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span> 是Sigmoid函数，这里用于计算两个向量的点积并映射到(0,1)区间。</p>
</li>
</ul>
<p>这个辅助损失会与模型最终的CTR预测损失<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{target}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>加在一起共同优化：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub><mo>+</mo><mi>α</mi><msub><mi>L</mi><mrow><mi>a</mi><mi>u</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L = L_{target} + \alpha L_{aux}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ux</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。这个额外的监督信号，在每个时间步都对GRU的学习进行指导，使其产出的隐状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{h}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>能够更精准地表达用户的潜在兴趣。</p>
<h4 id="第二阶段：兴趣演化层-Interest-Evolving-Layer">第二阶段：兴趣演化层 (Interest Evolving Layer)</h4>
<p>经过第一阶段，我们得到了一个更能代表用户内在兴趣的<strong>兴趣状态序列</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">h</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">h</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">h</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{h}_1, \boldsymbol{h}_2, \dots, \boldsymbol{h}_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。第二阶段的目标，就是对这个兴趣序列的演化过程进行建模。</p>
<p>然而，兴趣的演化并不总是平滑的，常常会伴随着<strong>兴趣漂移</strong>（Interest Drifting）现象，即用户可能在不同的兴趣点之间快速切换。如果用一个标准的GRU来建模这个兴趣序列，不相关的历史兴趣（漂移）可能会干扰对当前主要兴趣演化的判断。</p>
<p>为了解决这个问题，DIEN再次借鉴了DIN的思想，并将其与序列模型融合，设计了带注意力更新门的GRU（AUGRU）。AUGRU的核心是在GRU的更新门（Update Gate）上融入了注意力机制。注意力得分<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>时刻的兴趣状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{h}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<strong>候选广告</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">e</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{e}_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5944em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>共同决定：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">h</mi><mi>t</mi></msub><mi>W</mi><msub><mi mathvariant="bold-italic">e</mi><mi>a</mi></msub><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">h</mi><mi>j</mi></msub><mi>W</mi><msub><mi mathvariant="bold-italic">e</mi><mi>a</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">a_t = \frac{\exp(\boldsymbol{h}_t W \boldsymbol{e}_a)}{\sum_{j=1}^T\exp(\boldsymbol{h}_j W \boldsymbol{e}_a)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.734em;vertical-align:-1.307em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.1288em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">e</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>然后，这个注意力得分<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>会去调整（scale）GRU的原始更新门<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold-italic">u</mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{u}&#x27;_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9989em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">u</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi><mover accent="true"><mi mathvariant="bold-italic">u</mi><mo>~</mo></mover></mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>=</mo><msub><mi>a</mi><mi>t</mi></msub><mo>⋅</mo><msubsup><mi mathvariant="bold-italic">u</mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{\tilde{u}}&#x27;_t = a_t \cdot \boldsymbol{u}&#x27;_t
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0967em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord boldsymbol">u</span></span><span style="top:-3.3634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">~</span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8497em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1608em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.5945em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0489em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">u</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8019em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>最后，使用这个被注意力调整过的更新门<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi><mover accent="true"><mi mathvariant="bold-italic">u</mi><mo>~</mo></mover></mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{\tilde{u}}&#x27;_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0967em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord boldsymbol">u</span></span><span style="top:-3.3634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">~</span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8497em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1608em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>来更新隐状态：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi mathvariant="bold-italic">h</mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msubsup><mi><mover accent="true"><mi mathvariant="bold-italic">u</mi><mo>~</mo></mover></mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo stretchy="false">)</mo><mo>∘</mo><msubsup><mi mathvariant="bold-italic">h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>+</mo><msubsup><mi><mover accent="true"><mi mathvariant="bold-italic">u</mi><mo>~</mo></mover></mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>∘</mo><msubsup><mi><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>~</mo></mover></mi><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{h}_{t}&#x27; = (1 - \boldsymbol{\tilde{u}}_t&#x27;) \circ \boldsymbol{h}_{t-1}&#x27; + \boldsymbol{\tilde{u}}_t&#x27; \circ \boldsymbol{\tilde{h}}_{t}&#x27;
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0832em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8362em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0997em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord boldsymbol">u</span></span><span style="top:-3.3634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">~</span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8497em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1608em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∘</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1416em;vertical-align:-0.3053em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8362em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3053em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0967em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord boldsymbol">u</span></span><span style="top:-3.3634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">~</span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8497em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.1608em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∘</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.3467em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord boldsymbol">h</span></span><span style="top:-3.6134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">~</span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0997em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.4108em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∘</mo></mrow><annotation encoding="application/x-tex">\circ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord">∘</span></span></span></span>表示元素级乘积（element-wise product）。</p>
<p>通过这种方式，AUGRU在兴趣演化的每一步，都会参考当前的候选广告，来判断历史兴趣的相关性。与候选广告越相关的兴趣，其对应的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>越大，其信息在更新门中的权重也越大，从而能更顺畅地在序列中传递；反之，不相关的兴趣（漂移）其影响力就会被削弱。这使得模型能够聚焦于与当前推荐任务最相关的兴趣演化路径。</p>
<h4 id="代码-2">代码</h4>
<ul>
<li>辅助损失的计算通过预测下一个行为来监督兴趣状态的学习。</li>
<li>这种设计确保GRU的隐状态不仅能记录历史信息，还能有效预测未来行为，从而学到更有意义的兴趣表示。</li>
<li>AUGRU的核心在于用注意力分数调整GRU的更新门。</li>
<li>AUGRU通过注意力分数动态调整更新门，使得与目标广告相关的兴趣能够顺利传递，而不相关的兴趣（漂移）被抑制，从而更精准地捕捉兴趣演化路径。<br>
<code>dien.py</code> 文件：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">DIEN (深度兴趣演化网络) 排序模型实现，用于 FunRec。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">在 funrec 中自包含。构建与统一训练/评估管道兼容的单一排序模型（返回 (model, None, None)）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">两个关键层：</span></span><br><span class="line"><span class="string">1) InterestExtractorLayer: 对行为序列使用 GRU，可选辅助损失</span></span><br><span class="line"><span class="string">2) InterestEvolutionLayer: 双线性注意力 + AIGRU/AGRU/AUGRU</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .utils <span class="keyword">import</span> (</span><br><span class="line">    build_input_layer,</span><br><span class="line">    build_group_feature_embedding_table_dict,</span><br><span class="line">    concat_group_embedding,</span><br><span class="line">    get_linear_logits,</span><br><span class="line">    add_tensor_func,</span><br><span class="line">    concat_func,</span><br><span class="line">    parse_dien_feature_columns <span class="keyword">as</span> _parse_dien_feature_columns,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> .layers <span class="keyword">import</span> DNNs, PredictLayer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InterestExtractorLayer</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    DIEN 模型的兴趣提取层。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    该层使用带辅助损失的 GRU 从行为序列中提取用户兴趣。</span></span><br><span class="line"><span class="string">    辅助损失通过使用下一个行为来监督当前兴趣状态，帮助 GRU 隐藏状态更好地表示用户兴趣。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        hidden_units (int): GRU 中的隐藏单元数 (默认: 128)</span></span><br><span class="line"><span class="string">        use_auxiliary_loss (bool): 是否使用辅助损失 (默认: True)</span></span><br><span class="line"><span class="string">        auxiliary_loss_weight (float): 辅助损失的权重 (默认: 0.1)</span></span><br><span class="line"><span class="string">        dropout_rate (float): 正则化的 dropout 率 (默认: 0.0)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_units=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">        use_auxiliary_loss=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        auxiliary_loss_weight=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        dropout_rate=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        **kwargs,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(InterestExtractorLayer, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line">        self.use_auxiliary_loss = use_auxiliary_loss</span><br><span class="line">        self.auxiliary_loss_weight = auxiliary_loss_weight</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 兴趣提取 GRU</span></span><br><span class="line">        self.interest_gru = tf.keras.layers.GRU(</span><br><span class="line">            units=hidden_units,</span><br><span class="line">            return_sequences=<span class="literal">True</span>,</span><br><span class="line">            return_state=<span class="literal">False</span>,</span><br><span class="line">            name=<span class="string">&quot;interest_extractor_gru&quot;</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 辅助损失组件</span></span><br><span class="line">        <span class="keyword">if</span> self.use_auxiliary_loss:</span><br><span class="line">            <span class="comment"># 辅助损失预测的 MLP</span></span><br><span class="line">            self.auxiliary_mlp = tf.keras.Sequential(</span><br><span class="line">                [</span><br><span class="line">                    tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;aux_dense_1&quot;</span>),</span><br><span class="line">                    tf.keras.layers.Dropout(dropout_rate),</span><br><span class="line">                    tf.keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&quot;aux_dense_2&quot;</span>),</span><br><span class="line">                    tf.keras.layers.Dropout(dropout_rate),</span><br><span class="line">                    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>, name=<span class="string">&quot;aux_output&quot;</span>),</span><br><span class="line">                ],</span><br><span class="line">                name=<span class="string">&quot;auxiliary_mlp&quot;</span>,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.auxiliary_mlp = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout 层</span></span><br><span class="line">        <span class="keyword">if</span> dropout_rate &gt; <span class="number">0</span>:</span><br><span class="line">            self.dropout = tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;构建方法，如果需要则初始化辅助 MLP。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(InterestExtractorLayer, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果存在辅助 MLP 则构建它</span></span><br><span class="line">        <span class="keyword">if</span> self.auxiliary_mlp <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 辅助 MLP 输入形状: [batch_size, seq_len-1, hidden_units + embedding_dim]</span></span><br><span class="line">            <span class="comment"># embedding_dim 从 input_shape 确定</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(input_shape, <span class="built_in">list</span>) <span class="keyword">and</span> <span class="built_in">len</span>(input_shape) &gt; <span class="number">0</span>:</span><br><span class="line">                embedding_dim = input_shape[<span class="number">0</span>][-<span class="number">1</span>]  <span class="comment"># 从行为嵌入获取嵌入维度</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                embedding_dim = <span class="number">8</span>  <span class="comment"># 默认嵌入维度</span></span><br><span class="line"></span><br><span class="line">            aux_input_shape = (<span class="literal">None</span>, self.hidden_units + embedding_dim)</span><br><span class="line">            self.auxiliary_mlp.build(aux_input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        兴趣提取层的前向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            inputs: 包含 [behavior_embeddings, neg_behavior_embeddings (可选)] 的列表</span></span><br><span class="line"><span class="string">                - behavior_embeddings: 形状为 [batch_size, seq_len, embedding_dim] 的张量</span></span><br><span class="line"><span class="string">                - neg_behavior_embeddings: 形状为 [batch_size, seq_len, embedding_dim] 的张量 (用于辅助损失)</span></span><br><span class="line"><span class="string">            training: 训练模式标志</span></span><br><span class="line"><span class="string">            mask: 序列的填充掩码</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            interest_states: 形状为 [batch_size, seq_len, hidden_units] 的张量</span></span><br><span class="line"><span class="string">                表示每个时间步用户兴趣的隐藏状态</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        behavior_embeddings = inputs[<span class="number">0</span>]  <span class="comment"># 形状: [batch_size, seq_len, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用 GRU 提取兴趣</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len, hidden_units]</span></span><br><span class="line">        interest_states = self.interest_gru(</span><br><span class="line">            behavior_embeddings, mask=mask, training=training</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果启用则应用 dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            interest_states = self.dropout(interest_states, training=training)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果启用且在训练模式下计算辅助损失</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            self.use_auxiliary_loss</span><br><span class="line">            <span class="keyword">and</span> training</span><br><span class="line">            <span class="keyword">and</span> <span class="built_in">len</span>(inputs) &gt; <span class="number">1</span></span><br><span class="line">            <span class="keyword">and</span> self.auxiliary_mlp <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        ):</span><br><span class="line">            neg_behavior_embeddings = inputs[</span><br><span class="line">                <span class="number">1</span></span><br><span class="line">            ]  <span class="comment"># 形状: [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">            aux_loss = self._compute_auxiliary_loss(</span><br><span class="line">                interest_states, behavior_embeddings, neg_behavior_embeddings, mask</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 将辅助损失添加到层损失中</span></span><br><span class="line">            self.add_loss(self.auxiliary_loss_weight * aux_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> interest_states</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_compute_auxiliary_loss</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, interest_states, pos_behaviors, neg_behaviors, mask</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算兴趣提取的辅助损失。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        辅助损失使用下一个行为来监督当前兴趣状态：</span></span><br><span class="line"><span class="string">        - 正样本: 当前兴趣应该预测下一个正行为</span></span><br><span class="line"><span class="string">        - 负样本: 当前兴趣不应该预测下一个负行为</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            interest_states: 形状为 [batch_size, seq_len, hidden_units] 的张量</span></span><br><span class="line"><span class="string">            pos_behaviors: 形状为 [batch_size, seq_len, embedding_dim] 的张量</span></span><br><span class="line"><span class="string">            neg_behaviors: 形状为 [batch_size, seq_len, embedding_dim] 的张量</span></span><br><span class="line"><span class="string">            mask: 填充掩码</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            aux_loss: 标量辅助损失</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 获取兴趣和下一个行为（兴趣排除最后一个时间步）</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len-1, hidden_units]</span></span><br><span class="line">        current_interests = interest_states[:, :-<span class="number">1</span>, :]</span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len-1, embedding_dim]</span></span><br><span class="line">        next_pos_behaviors = pos_behaviors[:, <span class="number">1</span>:, :]</span><br><span class="line">        next_neg_behaviors = neg_behaviors[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 连接兴趣和行为嵌入作为 MLP 输入</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len-1, hidden_units + embedding_dim]</span></span><br><span class="line">        pos_input = tf.concat([current_interests, next_pos_behaviors], axis=-<span class="number">1</span>)</span><br><span class="line">        neg_input = tf.concat([current_interests, next_neg_behaviors], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预测概率</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len-1, 1]</span></span><br><span class="line">        pos_probs = self.auxiliary_mlp(pos_input)</span><br><span class="line">        neg_probs = self.auxiliary_mlp(neg_input)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算二元交叉熵损失</span></span><br><span class="line">        pos_loss = -tf.math.log(pos_probs + <span class="number">1e-8</span>)  <span class="comment"># 形状: [batch_size, seq_len-1, 1]</span></span><br><span class="line">        neg_loss = -tf.math.log(</span><br><span class="line">            <span class="number">1</span> - neg_probs + <span class="number">1e-8</span></span><br><span class="line">        )  <span class="comment"># 形状: [batch_size, seq_len-1, 1]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用掩码（排除填充位置）</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 为 seq_len-1 调整掩码</span></span><br><span class="line">            loss_mask = mask[:, <span class="number">1</span>:]  <span class="comment"># 形状: [batch_size, seq_len-1]</span></span><br><span class="line">            loss_mask = tf.expand_dims(</span><br><span class="line">                loss_mask, axis=-<span class="number">1</span></span><br><span class="line">            )  <span class="comment"># 形状: [batch_size, seq_len-1, 1]</span></span><br><span class="line">            loss_mask = tf.cast(loss_mask, tf.float32)</span><br><span class="line"></span><br><span class="line">            pos_loss = pos_loss * loss_mask</span><br><span class="line">            neg_loss = neg_loss * loss_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 平均损失</span></span><br><span class="line">        aux_loss = tf.reduce_mean(pos_loss + neg_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> aux_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InterestEvolutionLayer</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    DIEN 模型的兴趣演化层。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    该层使用双线性注意力结合 GRU 变体来建模用户兴趣随时间的演化。支持三种演化类型：</span></span><br><span class="line"><span class="string">    1. AIGRU: 基于注意力的输入 GRU</span></span><br><span class="line"><span class="string">    2. AGRU: 基于注意力的 GRU</span></span><br><span class="line"><span class="string">    3. AUGRU: 基于注意力的更新 GRU（推荐）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    注意力机制遵循原始 DIEN 论文使用双线性形式：</span></span><br><span class="line"><span class="string">    a_t = exp(h_t * W * e_a) / sum(exp(h_j * W * e_a))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        hidden_units (int): 演化 GRU 中的隐藏单元数 (默认: 128)</span></span><br><span class="line"><span class="string">        evolution_type (str): 演化机制类型 (&#x27;AIGRU&#x27;, &#x27;AGRU&#x27;, &#x27;AUGRU&#x27;) (默认: &#x27;AUGRU&#x27;)</span></span><br><span class="line"><span class="string">        dropout_rate (float): Dropout 率 (默认: 0.0)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, hidden_units=<span class="number">128</span>, evolution_type=<span class="string">&quot;AUGRU&quot;</span>, dropout_rate=<span class="number">0.0</span>, **kwargs</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(InterestEvolutionLayer, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line">        self.evolution_type = evolution_type</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 验证演化类型</span></span><br><span class="line">        <span class="keyword">if</span> evolution_type <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;AIGRU&quot;</span>, <span class="string">&quot;AGRU&quot;</span>, <span class="string">&quot;AUGRU&quot;</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;evolution_type 必须是 [&#x27;AIGRU&#x27;, &#x27;AGRU&#x27;, &#x27;AUGRU&#x27;] 之一，得到 <span class="subst">&#123;evolution_type&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如原始 DIEN 论文中的双线性注意力机制</span></span><br><span class="line">        <span class="comment"># 注意力分数: h_t * W * e_a，其中 W 是双线性权重矩阵</span></span><br><span class="line">        self.bilinear_weight = <span class="literal">None</span>  <span class="comment"># 将在 build 方法中初始化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 演化 GRU（基于 evolution_type 的不同实现）</span></span><br><span class="line">        <span class="keyword">if</span> evolution_type == <span class="string">&quot;AIGRU&quot;</span>:</span><br><span class="line">            self.evolution_gru = tf.keras.layers.GRU(</span><br><span class="line">                units=hidden_units,</span><br><span class="line">                return_sequences=<span class="literal">False</span>,</span><br><span class="line">                return_state=<span class="literal">False</span>,</span><br><span class="line">                name=<span class="string">&quot;evolution_gru&quot;</span>,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> evolution_type == <span class="string">&quot;AGRU&quot;</span>:</span><br><span class="line">            <span class="comment"># 自定义 AGRU 实现</span></span><br><span class="line">            self.evolution_gru = self._build_agru()</span><br><span class="line">        <span class="keyword">elif</span> evolution_type == <span class="string">&quot;AUGRU&quot;</span>:</span><br><span class="line">            <span class="comment"># 自定义 AUGRU 实现</span></span><br><span class="line">            self.evolution_gru = self._build_augru()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout 层</span></span><br><span class="line">        <span class="keyword">if</span> dropout_rate &gt; <span class="number">0</span>:</span><br><span class="line">            self.dropout = tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;构建方法，初始化双线性权重矩阵。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(InterestEvolutionLayer, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化注意力计算的双线性权重矩阵</span></span><br><span class="line">        <span class="comment"># 形状: [hidden_units, embedding_dim]</span></span><br><span class="line">        <span class="comment"># 这将用于: h_t * W * e_a</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(input_shape, <span class="built_in">list</span>) <span class="keyword">and</span> <span class="built_in">len</span>(input_shape) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># input_shape[0] 是 interest_states 形状，input_shape[1] 是 target_item_embedding 形状</span></span><br><span class="line">            embedding_dim = (</span><br><span class="line">                input_shape[<span class="number">1</span>][-<span class="number">1</span>] <span class="keyword">if</span> <span class="built_in">len</span>(input_shape[<span class="number">1</span>]) &gt; <span class="number">1</span> <span class="keyword">else</span> input_shape[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embedding_dim = <span class="number">8</span>  <span class="comment"># 默认嵌入维度</span></span><br><span class="line"></span><br><span class="line">        self.bilinear_weight = self.add_weight(</span><br><span class="line">            name=<span class="string">&quot;bilinear_attention_weight&quot;</span>,</span><br><span class="line">            shape=(self.hidden_units, embedding_dim),</span><br><span class="line">            initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_agru</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;构建自定义 AGRU 单元。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> AGRULayer(self.hidden_units, name=<span class="string">&quot;agru_evolution&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_augru</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;构建自定义 AUGRU 单元。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> AUGRULayer(self.hidden_units, name=<span class="string">&quot;augru_evolution&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        兴趣演化层的前向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            inputs: 包含 [interest_states, target_item_embedding] 的列表</span></span><br><span class="line"><span class="string">                - interest_states: 形状为 [batch_size, seq_len, hidden_units] 的张量</span></span><br><span class="line"><span class="string">                - target_item_embedding: 形状为 [batch_size, embedding_dim] 的张量</span></span><br><span class="line"><span class="string">            training: 训练模式标志</span></span><br><span class="line"><span class="string">            mask: 序列的填充掩码</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            final_interest: 形状为 [batch_size, hidden_units] 的张量</span></span><br><span class="line"><span class="string">                最终演化的兴趣表示</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        interest_states = inputs[<span class="number">0</span>]  <span class="comment"># 形状: [batch_size, seq_len, hidden_units]</span></span><br><span class="line">        target_item_embedding = inputs[<span class="number">1</span>]  <span class="comment"># 形状: [batch_size, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个兴趣状态与目标物品之间的注意力分数</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len, 1]</span></span><br><span class="line">        attention_scores = self._compute_attention_scores(</span><br><span class="line">            interest_states, target_item_embedding</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 基于演化类型应用注意力机制</span></span><br><span class="line">        <span class="keyword">if</span> self.evolution_type == <span class="string">&quot;AIGRU&quot;</span>:</span><br><span class="line">            <span class="comment"># 基于注意力的输入 GRU: 将注意力分数与输入相乘</span></span><br><span class="line">            <span class="comment"># 形状: [batch_size, seq_len, hidden_units]</span></span><br><span class="line">            attended_interests = interest_states * attention_scores</span><br><span class="line">            <span class="comment"># 形状: [batch_size, hidden_units]</span></span><br><span class="line">            final_interest = self.evolution_gru(</span><br><span class="line">                attended_interests, mask=mask, training=training</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.evolution_type <span class="keyword">in</span> [<span class="string">&quot;AGRU&quot;</span>, <span class="string">&quot;AUGRU&quot;</span>]:</span><br><span class="line">            <span class="comment"># 使用内部集成注意力的自定义 GRU 实现</span></span><br><span class="line">            <span class="comment"># 形状: [batch_size, hidden_units]</span></span><br><span class="line">            final_interest = self.evolution_gru(</span><br><span class="line">                [interest_states, attention_scores], mask=mask, training=training</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果启用则应用 dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            final_interest = self.dropout(final_interest, training=training)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> final_interest</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_compute_attention_scores</span>(<span class="params">self, interest_states, target_item_embedding</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用双线性形式计算兴趣状态与目标物品之间的注意力分数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        遵循原始 DIEN 论文: a_t = exp(h_t * W * e_a) / sum(exp(h_j * W * e_a))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            interest_states: 形状为 [batch_size, seq_len, hidden_units] 的张量</span></span><br><span class="line"><span class="string">            target_item_embedding: 形状为 [batch_size, 1, embedding_dim] 或 [batch_size, embedding_dim] 的张量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            attention_scores: 形状为 [batch_size, seq_len, 1] 的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 确保 target_item_embedding 具有正确的形状 [batch_size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(target_item_embedding.shape) == <span class="number">3</span>:</span><br><span class="line">            <span class="comment"># 形状: [batch_size, 1, embedding_dim] -&gt; [batch_size, embedding_dim]</span></span><br><span class="line">            target_item_embedding = tf.squeeze(target_item_embedding, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 双线性注意力计算: h_t * W * e_a</span></span><br><span class="line">        <span class="comment"># 步骤 1: h_t * W -&gt; [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        h_W = tf.tensordot(interest_states, self.bilinear_weight, axes=[[<span class="number">2</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 步骤 2: (h_t * W) * e_a -&gt; [batch_size, seq_len]</span></span><br><span class="line">        <span class="comment"># 将目标嵌入扩展为 [batch_size, 1, embedding_dim] 以进行广播</span></span><br><span class="line">        target_expanded = tf.expand_dims(target_item_embedding, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐元素乘法并在嵌入维度上求和</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len, embedding_dim] * [batch_size, 1, embedding_dim] -&gt; [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        attention_scores = tf.reduce_sum(</span><br><span class="line">            h_W * target_expanded, axis=<span class="number">2</span></span><br><span class="line">        )  <span class="comment"># [batch_size, seq_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用 softmax 来归一化注意力分数</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len]</span></span><br><span class="line">        attention_scores = tf.nn.softmax(attention_scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 扩展为 [batch_size, seq_len, 1] 以与模型其余部分保持一致</span></span><br><span class="line">        attention_scores = tf.expand_dims(attention_scores, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> attention_scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AGRULayer</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于注意力的 GRU 层。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    在 AGRU 中，注意力分数直接替换更新门值。</span></span><br><span class="line"><span class="string">    这是一种简化的方法，但可能会失去一些表示能力。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        units (int): 隐藏单元数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AGRULayer, self).__init__(**kwargs)</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">        <span class="comment"># GRU 单元组件（在 init 中构建它们）</span></span><br><span class="line">        self.dense_input_reset = tf.keras.layers.Dense(units, name=<span class="string">&quot;input_reset&quot;</span>)</span><br><span class="line">        self.dense_hidden_reset = tf.keras.layers.Dense(units, name=<span class="string">&quot;hidden_reset&quot;</span>)</span><br><span class="line">        self.dense_input_candidate = tf.keras.layers.Dense(</span><br><span class="line">            units, name=<span class="string">&quot;input_candidate&quot;</span></span><br><span class="line">        )</span><br><span class="line">        self.dense_hidden_candidate = tf.keras.layers.Dense(</span><br><span class="line">            units, name=<span class="string">&quot;hidden_candidate&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, mask=<span class="literal">None</span>, training=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        AGRU 的前向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            inputs: [interest_states, attention_scores] 的列表</span></span><br><span class="line"><span class="string">                - interest_states: 形状为 [batch_size, seq_len, units] 的张量</span></span><br><span class="line"><span class="string">                - attention_scores: 形状为 [batch_size, seq_len, 1] 的张量</span></span><br><span class="line"><span class="string">            mask: 填充掩码</span></span><br><span class="line"><span class="string">            training: 训练模式标志</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            final_state: 形状为 [batch_size, units] 的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        interest_states, attention_scores = inputs</span><br><span class="line">        batch_size = tf.shape(interest_states)[<span class="number">0</span>]</span><br><span class="line">        seq_len = tf.shape(interest_states)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化隐藏状态</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, units]</span></span><br><span class="line">        hidden_state = tf.zeros([batch_size, self.units])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐步处理序列</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">            current_input = interest_states[:, t, :]  <span class="comment"># 形状: [batch_size, units]</span></span><br><span class="line">            current_attention = attention_scores[:, t, <span class="number">0</span>]  <span class="comment"># 形状: [batch_size]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用注意力作为更新门的自定义 GRU 步骤</span></span><br><span class="line">            <span class="comment"># 使用标准 GRU 计算重置门和候选状态</span></span><br><span class="line">            reset_gate = tf.nn.sigmoid(</span><br><span class="line">                self.dense_input_reset(current_input)</span><br><span class="line">                + self.dense_hidden_reset(hidden_state)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            candidate_state = tf.nn.tanh(</span><br><span class="line">                self.dense_input_candidate(current_input)</span><br><span class="line">                + self.dense_hidden_candidate(reset_gate * hidden_state)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用注意力分数作为更新门</span></span><br><span class="line">            <span class="comment"># 形状: [batch_size, units]</span></span><br><span class="line">            update_gate = tf.expand_dims(current_attention, axis=<span class="number">1</span>)</span><br><span class="line">            update_gate = tf.tile(update_gate, [<span class="number">1</span>, self.units])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新隐藏状态</span></span><br><span class="line">            hidden_state = (</span><br><span class="line">                <span class="number">1</span> - update_gate</span><br><span class="line">            ) * hidden_state + update_gate * candidate_state</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hidden_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AUGRULayer</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于注意力的更新 GRU 层。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    AUGRU 保持更新门的多维性质，同时通过注意力分数对其进行缩放。</span></span><br><span class="line"><span class="string">    这是 DIEN 中推荐的方法。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        units (int): 隐藏单元数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AUGRULayer, self).__init__(**kwargs)</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">        <span class="comment"># GRU 单元权重</span></span><br><span class="line">        self.dense_input_update = tf.keras.layers.Dense(units, name=<span class="string">&quot;input_update&quot;</span>)</span><br><span class="line">        self.dense_hidden_update = tf.keras.layers.Dense(units, name=<span class="string">&quot;hidden_update&quot;</span>)</span><br><span class="line">        self.dense_input_reset = tf.keras.layers.Dense(units, name=<span class="string">&quot;input_reset&quot;</span>)</span><br><span class="line">        self.dense_hidden_reset = tf.keras.layers.Dense(units, name=<span class="string">&quot;hidden_reset&quot;</span>)</span><br><span class="line">        self.dense_input_candidate = tf.keras.layers.Dense(</span><br><span class="line">            units, name=<span class="string">&quot;input_candidate&quot;</span></span><br><span class="line">        )</span><br><span class="line">        self.dense_hidden_candidate = tf.keras.layers.Dense(</span><br><span class="line">            units, name=<span class="string">&quot;hidden_candidate&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, mask=<span class="literal">None</span>, training=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        AUGRU 的前向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            inputs: [interest_states, attention_scores] 的列表</span></span><br><span class="line"><span class="string">                - interest_states: 形状为 [batch_size, seq_len, units] 的张量</span></span><br><span class="line"><span class="string">                - attention_scores: 形状为 [batch_size, seq_len, 1] 的张量</span></span><br><span class="line"><span class="string">            mask: 填充掩码</span></span><br><span class="line"><span class="string">            training: 训练模式标志</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            final_state: 形状为 [batch_size, units] 的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        interest_states, attention_scores = inputs</span><br><span class="line">        batch_size = tf.shape(interest_states)[<span class="number">0</span>]</span><br><span class="line">        seq_len = tf.shape(interest_states)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化隐藏状态</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, units]</span></span><br><span class="line">        hidden_state = tf.zeros([batch_size, self.units])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐步处理序列</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">            current_input = interest_states[:, t, :]  <span class="comment"># 形状: [batch_size, units]</span></span><br><span class="line">            current_attention = attention_scores[:, t, <span class="number">0</span>]  <span class="comment"># 形状: [batch_size]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 标准 GRU 计算</span></span><br><span class="line">            <span class="comment"># 更新门: 形状 [batch_size, units]</span></span><br><span class="line">            update_gate = tf.nn.sigmoid(</span><br><span class="line">                self.dense_input_update(current_input)</span><br><span class="line">                + self.dense_hidden_update(hidden_state)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 重置门: 形状 [batch_size, units]</span></span><br><span class="line">            reset_gate = tf.nn.sigmoid(</span><br><span class="line">                self.dense_input_reset(current_input)</span><br><span class="line">                + self.dense_hidden_reset(hidden_state)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 候选状态: 形状 [batch_size, units]</span></span><br><span class="line">            candidate_state = tf.nn.tanh(</span><br><span class="line">                self.dense_input_candidate(current_input)</span><br><span class="line">                + self.dense_hidden_candidate(reset_gate * hidden_state)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 基于注意力的更新门缩放</span></span><br><span class="line">            <span class="comment"># 形状: [batch_size, units]</span></span><br><span class="line">            attention_expanded = tf.expand_dims(current_attention, axis=<span class="number">1</span>)</span><br><span class="line">            attention_expanded = tf.tile(attention_expanded, [<span class="number">1</span>, self.units])</span><br><span class="line">            attention_update_gate = attention_expanded * update_gate</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用注意力缩放的更新门更新隐藏状态</span></span><br><span class="line">            hidden_state = (</span><br><span class="line">                <span class="number">1</span> - attention_update_gate</span><br><span class="line">            ) * hidden_state + attention_update_gate * candidate_state</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hidden_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_dien_model</span>(<span class="params">feature_columns, model_config</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建 DIEN (深度兴趣演化网络) 模型。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    DIEN 通过两个关键组件捕获用户兴趣演化：</span></span><br><span class="line"><span class="string">    1. 兴趣提取层: 从行为序列中提取兴趣</span></span><br><span class="line"><span class="string">    2. 兴趣演化层: 使用双线性注意力建模兴趣随时间的演化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        feature_columns: 特征列规范列表</span></span><br><span class="line"><span class="string">        dnn_units: 最终 DNN 层的隐藏单元列表 (默认: [256, 128, 64, 1])</span></span><br><span class="line"><span class="string">        interest_hidden_units: 兴趣提取的隐藏单元 (默认: 128)</span></span><br><span class="line"><span class="string">        evolution_type: 演化机制类型 (&#x27;AIGRU&#x27;, &#x27;AGRU&#x27;, &#x27;AUGRU&#x27;) (默认: &#x27;AUGRU&#x27;)</span></span><br><span class="line"><span class="string">        use_auxiliary_loss: 是否使用辅助损失 (默认: True)</span></span><br><span class="line"><span class="string">        auxiliary_loss_weight: 辅助损失的权重 (默认: 0.1)</span></span><br><span class="line"><span class="string">        dropout_rate: 正则化的 dropout 率 (默认: 0.0)</span></span><br><span class="line"><span class="string">        linear_logits: 是否包含线性 logits (默认: True)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        (model, None, None): 统一接口的排序模型元组</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 解析配置</span></span><br><span class="line">    dnn_units = model_config.get(<span class="string">&quot;dnn_units&quot;</span>, [<span class="number">200</span>, <span class="number">80</span>, <span class="number">1</span>])</span><br><span class="line">    interest_hidden_units = model_config.get(<span class="string">&quot;interest_hidden_units&quot;</span>, <span class="number">64</span>)</span><br><span class="line">    evolution_type = model_config.get(<span class="string">&quot;evolution_type&quot;</span>, <span class="string">&quot;AUGRU&quot;</span>)</span><br><span class="line">    use_auxiliary_loss = model_config.get(<span class="string">&quot;use_auxiliary_loss&quot;</span>, <span class="literal">True</span>)</span><br><span class="line">    auxiliary_loss_weight = model_config.get(<span class="string">&quot;auxiliary_loss_weight&quot;</span>, <span class="number">0.1</span>)</span><br><span class="line">    dropout_rate = model_config.get(<span class="string">&quot;dropout_rate&quot;</span>, <span class="number">0.1</span>)</span><br><span class="line">    use_linear_logits = model_config.get(<span class="string">&quot;linear_logits&quot;</span>, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 构建输入层</span></span><br><span class="line">    input_layer_dict = build_input_layer(feature_columns)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建嵌入层</span></span><br><span class="line">    group_embedding_feature_dict = build_group_feature_embedding_table_dict(</span><br><span class="line">        feature_columns, input_layer_dict, prefix=<span class="string">&quot;embedding/&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取 DNN 输入（非序列特征）</span></span><br><span class="line">    dnn_inputs = concat_group_embedding(group_embedding_feature_dict, <span class="string">&quot;dnn&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理 DIEN 序列特征</span></span><br><span class="line">    dien_feature_list = _parse_dien_feature_columns(feature_columns)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dien_feature_list) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;未找到 DIEN 序列特征。请在 combiner 中添加包含 &#x27;dien&#x27; 的序列特征。&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理每个 DIEN 特征对（query, key）</span></span><br><span class="line">    dien_outputs = []</span><br><span class="line">    <span class="keyword">for</span> target_feature, sequence_feature <span class="keyword">in</span> dien_feature_list:</span><br><span class="line">        <span class="comment"># 获取目标物品嵌入（query）</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, embedding_dim]</span></span><br><span class="line">        target_embedding = group_embedding_feature_dict[<span class="string">&quot;dien_sequence&quot;</span>][target_feature]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取行为序列嵌入（keys）</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        behavior_embeddings = group_embedding_feature_dict[<span class="string">&quot;dien_sequence&quot;</span>][</span><br><span class="line">            sequence_feature</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为辅助损失生成负样本（简化方法）</span></span><br><span class="line">        <span class="keyword">if</span> use_auxiliary_loss:</span><br><span class="line">            <span class="comment"># 通过打乱行为嵌入创建负样本</span></span><br><span class="line">            <span class="comment"># 形状: [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">            neg_behavior_embeddings = tf.random.shuffle(behavior_embeddings)</span><br><span class="line">            interest_extractor_inputs = [behavior_embeddings, neg_behavior_embeddings]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            interest_extractor_inputs = [behavior_embeddings]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 兴趣提取层</span></span><br><span class="line">        <span class="comment"># 从行为序列中提取用户兴趣</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, seq_len, interest_hidden_units]</span></span><br><span class="line">        interest_extractor = InterestExtractorLayer(</span><br><span class="line">            hidden_units=interest_hidden_units,</span><br><span class="line">            use_auxiliary_loss=use_auxiliary_loss,</span><br><span class="line">            auxiliary_loss_weight=auxiliary_loss_weight,</span><br><span class="line">            dropout_rate=dropout_rate,</span><br><span class="line">            name=<span class="string">f&quot;<span class="subst">&#123;sequence_feature&#125;</span>_interest_extractor&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        interest_states = interest_extractor(interest_extractor_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 兴趣演化层</span></span><br><span class="line">        <span class="comment"># 基于目标物品使用双线性注意力建模兴趣演化</span></span><br><span class="line">        <span class="comment"># 形状: [batch_size, interest_hidden_units]</span></span><br><span class="line">        interest_evolution = InterestEvolutionLayer(</span><br><span class="line">            hidden_units=interest_hidden_units,</span><br><span class="line">            evolution_type=evolution_type,</span><br><span class="line">            dropout_rate=dropout_rate,</span><br><span class="line">            name=<span class="string">f&quot;<span class="subst">&#123;sequence_feature&#125;</span>_interest_evolution&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        evolved_interest = interest_evolution([interest_states, target_embedding])</span><br><span class="line"></span><br><span class="line">        dien_outputs.append(evolved_interest)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 连接所有 DIEN 输出</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dien_outputs) &gt; <span class="number">1</span>:</span><br><span class="line">        dien_output = concat_func(dien_outputs, axis=<span class="number">1</span>, flatten=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dien_output = dien_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 与其他 DNN 输入结合</span></span><br><span class="line">    <span class="comment"># 形状: [batch_size, dnn_dim + interest_hidden_units]</span></span><br><span class="line">    final_dnn_inputs = concat_func([dnn_inputs, dien_output], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终 DNN 层</span></span><br><span class="line">    dnn_logits = DNNs(dnn_units, use_bn=<span class="literal">True</span>, dropout_rate=dropout_rate)(</span><br><span class="line">        final_dnn_inputs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可选线性项</span></span><br><span class="line">    <span class="keyword">if</span> use_linear_logits:</span><br><span class="line">        linear_logit = get_linear_logits(input_layer_dict, feature_columns)</span><br><span class="line">        dnn_logits = add_tensor_func(</span><br><span class="line">            [dnn_logits, linear_logit], name=<span class="string">&quot;dien_linear_logits&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出: 确保与排序评估器兼容的 rank-1 预测</span></span><br><span class="line">    final_logits = tf.keras.layers.Flatten()(dnn_logits)</span><br><span class="line">    output = tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>, name=<span class="string">&quot;dien_output&quot;</span>)(</span><br><span class="line">        final_logits</span><br><span class="line">    )</span><br><span class="line">    output = tf.keras.layers.Flatten()(output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    model = tf.keras.models.Model(</span><br><span class="line">        inputs=<span class="built_in">list</span>(input_layer_dict.values()), outputs=output</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="3、DSIN：从行为序列到会话序列">3、DSIN：从行为序列到会话序列</h2>
<p>从DIN到DIEN，我们看到了模型对用户兴趣的理解从&quot;静态相关&quot;走向了&quot;动态演化&quot;。然而，它们都将用户的行为看作一条连续的序列。但现实中，用户的行为模式更多是间断性的。用户通常在<strong>一个会话（Session）</strong> 内拥有一个明确且集中的意图，而在<strong>不同会话</strong>之间，兴趣点可能发生巨大转变。</p>
<p><img src="https://datawhalechina.github.io/fun-rec/_images/dsin_session.png" alt="用户行为的会话结构示例"></p>
<p>如上图所示，一个用户可能在一个会话里集中浏览各种裤子，而在下一个会话则专注于戒指。这种<strong>会话内同质、会话间异质</strong>的现象非常普遍。如果直接用一个RNN模型处理这种&quot;断层&quot;明显的长序列，模型需要花费很大力气去学习这种兴趣的突变，效果并不理想。</p>
<p>深度会话兴趣网络（Deep Session Interest Network, DSIN） 基于这一观察，提出我们应该将&quot;会话&quot;作为分析用户行为的基本单元，并采用一种<strong>分层</strong>的思想来建模。</p>
<p><img src="https://datawhalechina.github.io/fun-rec/_images/dsin_architecture.png" alt="DSIN模型架构图"></p>
<h4 id="DSIN的技术实现：分层建模">DSIN的技术实现：分层建模</h4>
<p>DSIN的架构如上图所示，其建模过程可以清晰地分为几个层次：</p>
<ol>
<li>
<p><strong>会话划分层 (Session Division Layer)</strong>：这是模型的第一步，也是DSIN的基础。它根据行为发生的时间间隔（例如，如果两个行为间隔超过30分钟），将原始的、连续的用户行为长序列<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">S</mi></mrow><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">S</span></span></span></span>，切分成多个独立的<strong>会话短序列</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi mathvariant="bold">Q</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">Q</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi mathvariant="bold">Q</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{Q} = [\mathbf{Q}_1, \mathbf{Q}_2, ..., \mathbf{Q}_K]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord mathbf">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathbf">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>。</p>
</li>
<li>
<p><strong>会话兴趣提取层 (Session Interest Extractor Layer)</strong>：这一层的目标是为每一个会话<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">Q</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{Q}_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>提取出一个核心的兴趣向量。DSIN认为，一个会话内的行为虽然意图集中，但彼此之间的重要性也不同。因此，它没有使用简单的池化，而是采用了<strong>自注意力机制（Self−Attention）</strong>（与Transformer的核心思想一致）。自注意力网络能够捕捉该会话内部所有行为之间的内在关联，并聚合最重要的信息，最终为每个会话<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">Q</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{Q}_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>生成一个浓缩的兴趣向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">I</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{I}_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8361em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathbf">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p>
</li>
<li>
<p><strong>会话兴趣交互层 (Session Interest Interacting Layer)</strong>：经过上一步，我们得到了一个更高层次的序列——<strong>会话兴趣向量的序列</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">I</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">I</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi mathvariant="bold">I</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{I}_1, \mathbf{I}_2, ..., \mathbf{I}_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。这个序列反映了用户兴趣在更长的时间尺度上的演变。DSIN使用一个 <strong>双向长短期记忆网络（Bi-LSTM）</strong> 来对这个会话序列进行建模，从而捕捉不同会话之间的演进和依赖关系。Bi-LSTM的输出是一个包含了上下文信息的会话兴趣序列<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi mathvariant="bold">H</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">H</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi mathvariant="bold">H</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\mathbf{H}_1, \mathbf{H}_2, ..., \mathbf{H}_K]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathbf">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>。</p>
</li>
<li>
<p><strong>会话兴趣激活层 (Session Interest Activating Layer)</strong>：最后一步与DIN的思想一脉相承。模型会根据当前的<strong>候选广告</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">X</mi><mi>I</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{X}_I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8361em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathbf">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，使用注意力机制来计算每个会话兴趣的重要性，并进行加权求和，得到最终的用户兴趣表示。DSIN分别对会话兴趣提取层和交互层的输出都进行了激活：</p>
</li>
</ol>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi mathvariant="bold">U</mi><mi>I</mi></msup><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>a</mi><mi>k</mi><mi>I</mi></msubsup><msub><mi mathvariant="bold">I</mi><mi>k</mi></msub><mspace width="1em"/><mtext>和</mtext><mspace width="1em"/><msup><mi mathvariant="bold">U</mi><mi>H</mi></msup><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>a</mi><mi>k</mi><mi>H</mi></msubsup><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{U}^{I} = \sum_{k=1}^{K} a_{k}^{I} \mathbf{I}_{k} \quad \text{和} \quad \mathbf{U}^{H} = \sum_{k=1}^{K} a_{k}^{H} \mathbf{H}_{k}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathbf">U</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1304em;vertical-align:-1.3021em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord cjk_fallback">和</span></span><span class="mspace" style="margin-right:1em;"></span><span class="mord"><span class="mord mathbf">U</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1304em;vertical-align:-1.3021em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>a</mi><mi>k</mi><mi>I</mi></msubsup></mrow><annotation encoding="application/x-tex">a_{k}^{I}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1244em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>a</mi><mi>k</mi><mi>H</mi></msubsup></mrow><annotation encoding="application/x-tex">a_{k}^{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1244em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span>是根据候选广告计算出的注意力权重。最终，将这两个激活后的向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">U</mi><mi>I</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{U}^{I}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbf">U</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">U</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{U}^{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbf">U</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span></span></span></span></span></span></span></span>拼接，得到用户的最终兴趣表示。</p>
<p>DSIN通过引入&quot;会话&quot;这一更符合用户实际行为模式的中间单元，将复杂的长序列建模问题分解为&quot;<strong>会话内信息聚合</strong>&quot;（通过自注意力）和&quot;<strong>会话间信息传递</strong>&quot;（通过Bi-LSTM）两个更清晰的子问题。这种分层建模思想，使得模型能够对用户兴趣进行更精细的刻画。</p>
<p>本节介绍了序列建模的三个关键模型：DIN通过注意力机制解决用户兴趣多样性问题，DIEN进一步建模兴趣的时序演化过程，DSIN则引入会话概念进行分层建模。这些模型体现了序列建模的核心思想：动态性（根据任务调整兴趣表示）、序列性（利用时间顺序信息）和聚焦性（针对任务筛选相关信息）。随着技术发展，未来的序列建模方法将结合更多先进技术来更好地理解用户动态需求。</p>
<h4 id="代码-3">代码</h4>
<ul>
<li>DSIN的三个核心组件实现了分层的会话建模。</li>
<li>这种设计体现了“分层建模”的思想：先用自注意力在会话内聚合，再用BiLSTM在会话间传递，最后用注意力激活相关会话，从而实现对用户行为的细粒度刻画。</li>
<li>本节介绍了序列建模的三个关键模型：DIN通过注意力机制解决用户兴趣多样性问题，DIEN进一步建模兴趣的时序演化过程，DSIN则引入会话概念进行分层建模。这些模型体现了序列建模的核心思想：动态性（根据任务调整兴趣表示）、序列性（利用时间顺序信息）和聚焦性（针对任务筛选相关信息）。随着技术发展，未来的序列建模方法将结合更多先进技术来更好地理解用户动态需求。<br>
<code>dsin.py</code> 文件：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> .utils <span class="keyword">import</span> (</span><br><span class="line">    build_input_layer,</span><br><span class="line">    build_group_feature_embedding_table_dict,</span><br><span class="line">    concat_group_embedding,</span><br><span class="line">    concat_func,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> .layers <span class="keyword">import</span> DNNs, PredictLayer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 build_dsin_model 函数之前添加 BiasEncoding 类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiasEncoding</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    DSIN 模型的偏置编码层。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    该层为会话嵌入添加三种类型的偏置：</span></span><br><span class="line"><span class="string">    1. 会话偏置：每个会话不同（捕获会话级特征）</span></span><br><span class="line"><span class="string">    2. 位置偏置：会话内每个位置不同（捕获时间模式）</span></span><br><span class="line"><span class="string">    3. Item偏置：每个嵌入维度不同（捕获特征级偏置）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    公式：BE(k,t,c) = w_k^K + w_t^T + w_c^C</span></span><br><span class="line"><span class="string">    其中：</span></span><br><span class="line"><span class="string">    - k：会话索引</span></span><br><span class="line"><span class="string">    - t：会话内位置索引</span></span><br><span class="line"><span class="string">    - c：嵌入维度索引</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sess_max_count, sess_max_len, seed=<span class="number">1024</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiasEncoding, self).__init__(**kwargs)</span><br><span class="line">        self.sess_max_count = sess_max_count</span><br><span class="line">        self.sess_max_len = sess_max_len</span><br><span class="line">        self.seed = seed</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        <span class="comment"># 从输入形状获取嵌入大小</span></span><br><span class="line">        <span class="comment"># input_shape: [batch_size, sess_max_count, sess_max_len, embedding_dim]</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(input_shape) == <span class="number">4</span>:</span><br><span class="line">            embedding_dim = input_shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;期望4维输入形状，得到 <span class="subst">&#123;input_shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 会话偏置：每个会话不同</span></span><br><span class="line">        <span class="comment"># 形状：[sess_max_count, 1, 1]</span></span><br><span class="line">        self.sess_bias_embedding = self.add_weight(</span><br><span class="line">            <span class="string">&quot;sess_bias_embedding&quot;</span>,</span><br><span class="line">            shape=(self.sess_max_count, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            initializer=tf.keras.initializers.TruncatedNormal(</span><br><span class="line">                mean=<span class="number">0.0</span>, stddev=<span class="number">0.0001</span>, seed=self.seed</span><br><span class="line">            ),</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 位置偏置：会话内每个位置不同</span></span><br><span class="line">        <span class="comment"># 形状：[1, sess_max_len, 1]</span></span><br><span class="line">        self.seq_bias_embedding = self.add_weight(</span><br><span class="line">            <span class="string">&quot;seq_bias_embedding&quot;</span>,</span><br><span class="line">            shape=(<span class="number">1</span>, self.sess_max_len, <span class="number">1</span>),</span><br><span class="line">            initializer=tf.keras.initializers.TruncatedNormal(</span><br><span class="line">                mean=<span class="number">0.0</span>, stddev=<span class="number">0.0001</span>, seed=self.seed</span><br><span class="line">            ),</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item偏置：每个嵌入维度不同</span></span><br><span class="line">        <span class="comment"># 形状：[1, 1, embedding_dim]</span></span><br><span class="line">        self.item_bias_embedding = self.add_weight(</span><br><span class="line">            <span class="string">&quot;item_bias_embedding&quot;</span>,</span><br><span class="line">            shape=(<span class="number">1</span>, <span class="number">1</span>, embedding_dim),</span><br><span class="line">            initializer=tf.keras.initializers.TruncatedNormal(</span><br><span class="line">                mean=<span class="number">0.0</span>, stddev=<span class="number">0.0001</span>, seed=self.seed</span><br><span class="line">            ),</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(BiasEncoding, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        对会话嵌入应用偏置编码。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            inputs：会话嵌入张量</span></span><br><span class="line"><span class="string">                    形状：[batch_size, sess_max_count, sess_max_len, embedding_dim]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回：</span></span><br><span class="line"><span class="string">            偏置编码后的会话嵌入，形状相同</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 将所有三种偏置添加到输入中</span></span><br><span class="line">        <span class="comment"># 广播将处理维度对齐</span></span><br><span class="line">        <span class="comment"># 形状：[batch_size, sess_max_count, sess_max_len, embedding_dim]</span></span><br><span class="line">        encoded_inputs = (</span><br><span class="line">            inputs</span><br><span class="line">            + self.sess_bias_embedding  <span class="comment"># 会话偏置</span></span><br><span class="line">            + self.seq_bias_embedding  <span class="comment"># 位置偏置</span></span><br><span class="line">            + self.item_bias_embedding</span><br><span class="line">        )  <span class="comment"># Item偏置</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> encoded_inputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_output_shape</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        <span class="keyword">return</span> input_shape</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">        config = &#123;</span><br><span class="line">            <span class="string">&quot;sess_max_count&quot;</span>: self.sess_max_count,</span><br><span class="line">            <span class="string">&quot;sess_max_len&quot;</span>: self.sess_max_len,</span><br><span class="line">            <span class="string">&quot;seed&quot;</span>: self.seed,</span><br><span class="line">        &#125;</span><br><span class="line">        base_config = <span class="built_in">super</span>(BiasEncoding, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(<span class="built_in">list</span>(base_config.items()) + <span class="built_in">list</span>(config.items()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_dsin_model</span>(<span class="params">feature_columns, model_config</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建深度会话兴趣网络 (DSIN) 模型用于 CTR 预测。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    DSIN 引入会话概念来更好地建模用户行为序列。</span></span><br><span class="line"><span class="string">    关键组件：</span></span><br><span class="line"><span class="string">    1. 会话划分：基于时间间隔将用户行为序列划分为会话</span></span><br><span class="line"><span class="string">    2. 偏置编码：应用会话、位置和Item偏置（如果启用）</span></span><br><span class="line"><span class="string">    3. 会话兴趣提取：使用多头注意力提取会话级兴趣</span></span><br><span class="line"><span class="string">    4. 会话兴趣交互：使用双向 LSTM 建模会话间交互</span></span><br><span class="line"><span class="string">    5. 会话兴趣激活：应用注意力激活相关的会话兴趣</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        feature_columns：特征列定义列表</span></span><br><span class="line"><span class="string">        model_config：包含以下参数的字典：</span></span><br><span class="line"><span class="string">            - session_feature_list：list，例如 [&#x27;video_id&#x27;]</span></span><br><span class="line"><span class="string">            - sess_max_count：int</span></span><br><span class="line"><span class="string">            - sess_max_len：int</span></span><br><span class="line"><span class="string">            - bias_encoding：bool</span></span><br><span class="line"><span class="string">            - att_embedding_size：int</span></span><br><span class="line"><span class="string">            - att_head_num：int</span></span><br><span class="line"><span class="string">            - dnn_units：list</span></span><br><span class="line"><span class="string">            - dropout_rate：float</span></span><br><span class="line"><span class="string">            - l2_reg：float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        (model, None, None)：排序模型元组</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    session_feature_list = model_config.get(<span class="string">&quot;session_feature_list&quot;</span>, [<span class="string">&quot;video_id&quot;</span>])</span><br><span class="line">    sess_max_count = model_config.get(<span class="string">&quot;sess_max_count&quot;</span>, <span class="number">5</span>)</span><br><span class="line">    sess_max_len = model_config.get(<span class="string">&quot;sess_max_len&quot;</span>, <span class="number">10</span>)</span><br><span class="line">    bias_encoding = model_config.get(<span class="string">&quot;bias_encoding&quot;</span>, <span class="literal">True</span>)</span><br><span class="line">    att_embedding_size = model_config.get(<span class="string">&quot;att_embedding_size&quot;</span>, <span class="number">8</span>)</span><br><span class="line">    att_head_num = model_config.get(<span class="string">&quot;att_head_num&quot;</span>, <span class="number">2</span>)</span><br><span class="line">    dnn_units = model_config.get(<span class="string">&quot;dnn_units&quot;</span>, [<span class="number">128</span>, <span class="number">64</span>, <span class="number">1</span>])</span><br><span class="line">    dropout_rate = model_config.get(<span class="string">&quot;dropout_rate&quot;</span>, <span class="number">0.2</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        l2_reg = <span class="built_in">float</span>(model_config.get(<span class="string">&quot;l2_reg&quot;</span>, <span class="number">1e-6</span>))</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        l2_reg = <span class="number">0.000001</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为所有特征构建输入层</span></span><br><span class="line">    <span class="comment"># 形状：input_layer_dict 包含所有输入张量</span></span><br><span class="line">    input_layer_dict = build_input_layer(feature_columns)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为所有特征构建嵌入表</span></span><br><span class="line">    <span class="comment"># 形状：group_embedding_feature_dict 包含按用途分组的嵌入</span></span><br><span class="line">    group_embedding_feature_dict = build_group_feature_embedding_table_dict(</span><br><span class="line">        feature_columns, input_layer_dict, prefix=<span class="string">&quot;embedding/&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取常规 DNN 输入（非会话特征）</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, total_embedding_dim]</span></span><br><span class="line">    dnn_inputs = concat_group_embedding(group_embedding_feature_dict, <span class="string">&quot;dnn&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 DSIN 组件处理会话特征</span></span><br><span class="line">    session_embeddings = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查找用于会话处理的 video_id 嵌入</span></span><br><span class="line">    video_id_embedding_layer = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> feature_columns:</span><br><span class="line">        <span class="keyword">if</span> fc.name == <span class="string">&quot;video_id&quot;</span>:</span><br><span class="line">            video_id_embedding_layer = tf.keras.layers.Embedding(</span><br><span class="line">                fc.vocab_size,</span><br><span class="line">                fc.emb_dim,</span><br><span class="line">                name=<span class="string">&quot;video_id_session_embedding&quot;</span>,</span><br><span class="line">                mask_zero=<span class="literal">True</span>,</span><br><span class="line">                embeddings_regularizer=tf.keras.regularizers.l2(l2_reg),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取会话嵌入</span></span><br><span class="line">    session_inputs_found = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> sess_idx <span class="keyword">in</span> <span class="built_in">range</span>(sess_max_count):</span><br><span class="line">        <span class="keyword">for</span> feature_name <span class="keyword">in</span> session_feature_list:</span><br><span class="line">            session_key = <span class="string">f&quot;sess_<span class="subst">&#123;sess_idx&#125;</span>_<span class="subst">&#123;feature_name&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> session_key <span class="keyword">in</span> input_layer_dict:</span><br><span class="line">                session_inputs_found = <span class="literal">True</span></span><br><span class="line">                <span class="comment"># 获取会话输入张量</span></span><br><span class="line">                <span class="comment"># 形状：[batch_size, sess_max_len]</span></span><br><span class="line">                session_input = input_layer_dict[session_key]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 获取此会话的嵌入</span></span><br><span class="line">                <span class="comment"># 形状：[batch_size, sess_max_len, embedding_dim]</span></span><br><span class="line">                session_emb = video_id_embedding_layer(session_input)</span><br><span class="line">                session_embeddings.append(session_emb)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 堆叠会话嵌入</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, sess_max_count, sess_max_len, embedding_dim]</span></span><br><span class="line">    session_embeddings_stack = tf.stack(session_embeddings, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果启用，应用偏置编码</span></span><br><span class="line">    <span class="keyword">if</span> bias_encoding:</span><br><span class="line">        bias_encoder = BiasEncoding(sess_max_count, sess_max_len, seed=<span class="number">1024</span>)</span><br><span class="line">        session_embeddings_stack = bias_encoder(session_embeddings_stack)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSIN 组件 1：会话兴趣提取</span></span><br><span class="line">    session_interests = apply_session_interest_extractor(</span><br><span class="line">        session_embeddings_stack,</span><br><span class="line">        sess_max_count,</span><br><span class="line">        sess_max_len,</span><br><span class="line">        att_embedding_size,</span><br><span class="line">        att_head_num,</span><br><span class="line">        dropout_rate,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSIN 组件 2：会话兴趣交互</span></span><br><span class="line">    session_interactions = apply_session_interest_interaction(</span><br><span class="line">        session_interests,</span><br><span class="line">        sess_max_count,</span><br><span class="line">        att_embedding_size * att_head_num,</span><br><span class="line">        dropout_rate,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSIN 组件 3：会话兴趣激活</span></span><br><span class="line">    <span class="comment"># 获取用于激活的目标Item嵌入</span></span><br><span class="line">    target_item_embedding = get_target_item_embedding_by_prefix(</span><br><span class="line">        group_embedding_feature_dict, <span class="string">&quot;embedding/video_id&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于目标Item激活会话兴趣和交互</span></span><br><span class="line">    activated_session_interests = apply_attention_activation(</span><br><span class="line">        session_interests, target_item_embedding, name_suffix=<span class="string">&quot;interests&quot;</span></span><br><span class="line">    )</span><br><span class="line">    activated_session_interactions = apply_attention_activation(</span><br><span class="line">        session_interactions, target_item_embedding, name_suffix=<span class="string">&quot;interactions&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 组合所有特征进行最终预测</span></span><br><span class="line">    all_features = [dnn_inputs]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果可用，添加会话特征</span></span><br><span class="line">    <span class="keyword">if</span> session_inputs_found:</span><br><span class="line">        all_features.extend(</span><br><span class="line">            [activated_session_interests, activated_session_interactions]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 连接所有特征</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, total_feature_dim]</span></span><br><span class="line">    final_inputs = concat_func(all_features, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用 DNN 层</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, 1]（最终预测）</span></span><br><span class="line">    dnn_logits = DNNs(dnn_units, use_bn=<span class="literal">True</span>, dropout_rate=dropout_rate)(final_inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终预测层</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, 1] 带 sigmoid 激活</span></span><br><span class="line">    output = PredictLayer(name=<span class="string">&quot;dsin_output&quot;</span>)(dnn_logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    model = tf.keras.models.Model(</span><br><span class="line">        inputs=<span class="built_in">list</span>(input_layer_dict.values()), outputs=output</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_session_interest_extractor</span>(<span class="params"></span></span><br><span class="line"><span class="params">    session_embeddings,</span></span><br><span class="line"><span class="params">    sess_max_count,</span></span><br><span class="line"><span class="params">    sess_max_len,</span></span><br><span class="line"><span class="params">    att_embedding_size,</span></span><br><span class="line"><span class="params">    att_head_num,</span></span><br><span class="line"><span class="params">    dropout_rate,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用多头自注意力应用会话兴趣提取器。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    这是 DSIN 的核心组件，用于提取会话级兴趣。</span></span><br><span class="line"><span class="string">    对于每个会话，我们应用多头自注意力来捕获</span></span><br><span class="line"><span class="string">    该会话内Item之间的关系。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    教育说明：</span></span><br><span class="line"><span class="string">    - 多头注意力允许模型同时关注会话的不同方面</span></span><br><span class="line"><span class="string">    - 自注意力意味着会话中的每个Item都关注同一会话中的所有其他Item</span></span><br><span class="line"><span class="string">    - 这捕获了会话内的依赖关系和兴趣</span></span><br><span class="line"><span class="string">    - 偏置编码可能已应用于输入，以提供位置和会话级信息</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        session_embeddings：堆叠的会话嵌入（可能已偏置编码）</span></span><br><span class="line"><span class="string">                           形状：[batch_size, sess_max_count, sess_max_len, embedding_dim]</span></span><br><span class="line"><span class="string">        sess_max_count：每个用户的最大会话数</span></span><br><span class="line"><span class="string">        sess_max_len：最大会话长度</span></span><br><span class="line"><span class="string">        att_embedding_size：每个注意力头的嵌入大小</span></span><br><span class="line"><span class="string">        att_head_num：注意力头数</span></span><br><span class="line"><span class="string">        dropout_rate：正则化的 dropout 率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        tf.Tensor：会话兴趣张量</span></span><br><span class="line"><span class="string">                   形状：[batch_size, sess_max_count, att_embedding_size * att_head_num]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    d_model = att_embedding_size * att_head_num</span><br><span class="line">    session_interests = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sess_idx <span class="keyword">in</span> <span class="built_in">range</span>(sess_max_count):</span><br><span class="line">        <span class="comment"># 获取此会话的会话嵌入</span></span><br><span class="line">        <span class="comment"># 形状：[batch_size, sess_max_len, embedding_dim]</span></span><br><span class="line">        session_emb = session_embeddings[:, sess_idx, :, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在此会话内应用多头自注意力</span></span><br><span class="line">        <span class="comment"># 这捕获同一会话中Item之间的关系</span></span><br><span class="line">        <span class="comment"># 形状：[batch_size, sess_max_len, d_model]</span></span><br><span class="line">        attention_output = tf.keras.layers.MultiHeadAttention(</span><br><span class="line">            num_heads=att_head_num,</span><br><span class="line">            key_dim=att_embedding_size,</span><br><span class="line">            dropout=dropout_rate,</span><br><span class="line">            name=<span class="string">f&quot;session_attention_<span class="subst">&#123;sess_idx&#125;</span>&quot;</span>,</span><br><span class="line">        )(session_emb, session_emb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用层归一化以提高训练稳定性</span></span><br><span class="line">        attention_output = tf.keras.layers.LayerNormalization(</span><br><span class="line">            name=<span class="string">f&quot;session_attention_norm_<span class="subst">&#123;sess_idx&#125;</span>&quot;</span></span><br><span class="line">        )(attention_output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用平均池化获取会话级表示</span></span><br><span class="line">        <span class="comment"># 这将会话中的所有Item聚合为单一表示</span></span><br><span class="line">        <span class="comment"># 形状：[batch_size, d_model]</span></span><br><span class="line">        session_interest = tf.reduce_mean(</span><br><span class="line">            attention_output, axis=<span class="number">1</span>, name=<span class="string">f&quot;session_pooling_<span class="subst">&#123;sess_idx&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line">        session_interests.append(session_interest)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 堆叠所有会话兴趣</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, sess_max_count, d_model]</span></span><br><span class="line">    session_interests = tf.stack(</span><br><span class="line">        session_interests, axis=<span class="number">1</span>, name=<span class="string">&quot;session_interests_stack&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> session_interests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_session_interest_interaction</span>(<span class="params"></span></span><br><span class="line"><span class="params">    session_interests, sess_max_count, d_model, dropout_rate</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用双向 LSTM 应用会话兴趣交互。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    该组件建模会话之间的时间关系。</span></span><br><span class="line"><span class="string">    直觉是会话按时间排序，后续会话</span></span><br><span class="line"><span class="string">    可能受到早期会话的影响。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    教育说明：</span></span><br><span class="line"><span class="string">    - 双向 LSTM 在前向和后向方向处理会话</span></span><br><span class="line"><span class="string">    - 这捕获了过去会话如何影响当前会话以及</span></span><br><span class="line"><span class="string">      未来会话如何为当前会话提供上下文</span></span><br><span class="line"><span class="string">    - 输出捕获会话间依赖关系</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        session_interests：会话兴趣张量</span></span><br><span class="line"><span class="string">                          形状：[batch_size, sess_max_count, d_model]</span></span><br><span class="line"><span class="string">        sess_max_count：最大会话数</span></span><br><span class="line"><span class="string">        d_model：模型维度</span></span><br><span class="line"><span class="string">        dropout_rate：正则化的 dropout 率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        tf.Tensor：会话交互输出</span></span><br><span class="line"><span class="string">                   形状：[batch_size, sess_max_count, d_model * 2]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用双向 LSTM 建模会话间的时间依赖关系</span></span><br><span class="line">    <span class="comment"># 前向方向：过去会话如何影响当前会话</span></span><br><span class="line">    <span class="comment"># 后向方向：未来会话如何为当前会话提供上下文</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, sess_max_count, d_model * 2]</span></span><br><span class="line">    session_interactions = tf.keras.layers.Bidirectional(</span><br><span class="line">        tf.keras.layers.LSTM(</span><br><span class="line">            d_model // <span class="number">2</span>,  <span class="comment"># 每个方向的单元数为一半，以保持输出大小一致</span></span><br><span class="line">            return_sequences=<span class="literal">True</span>,</span><br><span class="line">            dropout=dropout_rate,</span><br><span class="line">            recurrent_dropout=dropout_rate,</span><br><span class="line">            name=<span class="string">&quot;session_interaction_lstm_forward&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        name=<span class="string">&quot;session_interaction_lstm&quot;</span>,</span><br><span class="line">        backward_layer=tf.keras.layers.LSTM(</span><br><span class="line">            d_model // <span class="number">2</span>,</span><br><span class="line">            return_sequences=<span class="literal">True</span>,</span><br><span class="line">            dropout=dropout_rate,</span><br><span class="line">            recurrent_dropout=dropout_rate,</span><br><span class="line">            go_backwards=<span class="literal">True</span>,</span><br><span class="line">            name=<span class="string">&quot;session_interaction_lstm_backward&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">    )(session_interests)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用层归一化以提高训练稳定性</span></span><br><span class="line">    session_interactions = tf.keras.layers.LayerNormalization(</span><br><span class="line">        name=<span class="string">&quot;session_interaction_norm&quot;</span></span><br><span class="line">    )(session_interactions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> session_interactions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_attention_activation</span>(<span class="params">session_features, target_item_embedding, name_suffix=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于目标Item对会话特征应用注意力激活。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    该组件基于会话特征与目标Item的相关性来激活会话特征。</span></span><br><span class="line"><span class="string">    直觉是并非所有会话对预测用户对特定Item的兴趣都同等相关。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    教育说明：</span></span><br><span class="line"><span class="string">    - 注意力机制允许模型关注最相关的会话</span></span><br><span class="line"><span class="string">    - 目标Item嵌入为我们试图预测的内容提供上下文</span></span><br><span class="line"><span class="string">    - 这创建了会话特征的加权组合</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        session_features：会话特征张量</span></span><br><span class="line"><span class="string">                         形状：[batch_size, sess_max_count, feature_dim]</span></span><br><span class="line"><span class="string">        target_item_embedding：目标Item嵌入</span></span><br><span class="line"><span class="string">                              形状：[batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">        name_suffix：层名称后缀以确保唯一性</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        tf.Tensor：激活的会话特征</span></span><br><span class="line"><span class="string">                   形状：[batch_size, feature_dim]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取维度</span></span><br><span class="line">    batch_size = tf.shape(session_features)[<span class="number">0</span>]</span><br><span class="line">    sess_max_count = tf.shape(session_features)[<span class="number">1</span>]</span><br><span class="line">    feature_dim = tf.shape(session_features)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 扩展目标Item嵌入以匹配会话维度</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, 1, embedding_dim]</span></span><br><span class="line">    target_expanded = tf.expand_dims(target_item_embedding, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为每个会话重复目标嵌入</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, sess_max_count, embedding_dim]</span></span><br><span class="line">    target_repeated = tf.tile(target_expanded, [<span class="number">1</span>, sess_max_count, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将会话特征与目标Item嵌入连接</span></span><br><span class="line">    <span class="comment"># 这为计算注意力分数提供上下文</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, sess_max_count, feature_dim + embedding_dim]</span></span><br><span class="line">    combined_features = tf.concat([session_features, target_repeated], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用密集层计算注意力分数</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, sess_max_count, 1]</span></span><br><span class="line">    attention_scores = tf.keras.layers.Dense(</span><br><span class="line">        <span class="number">1</span>, activation=<span class="string">&quot;tanh&quot;</span>, name=<span class="string">f&quot;attention_score_<span class="subst">&#123;name_suffix&#125;</span>&quot;</span></span><br><span class="line">    )(combined_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用 softmax 获取注意力权重</span></span><br><span class="line">    <span class="comment"># 这确保权重在会话间的和为 1</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, sess_max_count, 1]</span></span><br><span class="line">    attention_weights = tf.nn.softmax(</span><br><span class="line">        attention_scores, axis=<span class="number">1</span>, name=<span class="string">f&quot;attention_weights_<span class="subst">&#123;name_suffix&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将注意力权重应用于会话特征</span></span><br><span class="line">    <span class="comment"># 这创建了会话特征的加权组合</span></span><br><span class="line">    <span class="comment"># 形状：[batch_size, feature_dim]</span></span><br><span class="line">    activated_features = tf.reduce_sum(</span><br><span class="line">        session_features * attention_weights,</span><br><span class="line">        axis=<span class="number">1</span>,</span><br><span class="line">        name=<span class="string">f&quot;activated_features_<span class="subst">&#123;name_suffix&#125;</span>&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> activated_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_target_item_embedding_by_prefix</span>(<span class="params">group_embedding_feature_dict, prefix</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取用于注意力激活的目标Item嵌入。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    该函数从特征嵌入中提取目标Item嵌入。</span></span><br><span class="line"><span class="string">    目标Item是我们试图预测用户兴趣的Item。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        group_embedding_feature_dict：分组嵌入特征字典</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        tf.Tensor：目标Item嵌入</span></span><br><span class="line"><span class="string">                   形状：[batch_size, embedding_dim]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dnn_embeddings = group_embedding_feature_dict.get(<span class="string">&quot;dnn&quot;</span>, [])</span><br><span class="line">    <span class="comment"># 查找以 &#x27;embedding/video_id&#x27; 开头的嵌入</span></span><br><span class="line">    target_embedding = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> embedding <span class="keyword">in</span> dnn_embeddings:</span><br><span class="line">        <span class="keyword">if</span> embedding.name.startswith(prefix):</span><br><span class="line">            target_embedding = embedding</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(target_embedding.shape) &gt; <span class="number">2</span>:</span><br><span class="line">        <span class="comment"># 如果嵌入有额外维度，取平均值</span></span><br><span class="line">        target_embedding = tf.reduce_mean(target_embedding, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> target_embedding</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="性能对比">性能对比</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+--------+--------+--------+------------+</span><br><span class="line">| 模型   |    auc |   gauc |   val_user |</span><br><span class="line">+========+========+========+============+</span><br><span class="line">| din    | 0.5687 | 0.5507 |        928 |</span><br><span class="line">+--------+--------+--------+------------+</span><br><span class="line">| dien   | 0.5796 | 0.5568 |        928 |</span><br><span class="line">+--------+--------+--------+------------+</span><br><span class="line">| dsin   | 0.5616 | 0.5577 |         99 |</span><br><span class="line">+--------+--------+--------+------------+</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://isSeymour.github.io/butterflyblog">isSeymour</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://isseymour.github.io/butterflyblog/2025/12/18/FunRec_3_3/">https://isseymour.github.io/butterflyblog/2025/12/18/FunRec_3_3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://isSeymour.github.io/butterflyblog" target="_blank">isSeymour</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/butterflyblog/tags/%E7%B2%BE%E6%8E%92/">精排</a></div><div class="post_share"><div class="social-share" data-image="https://datawhalechina.github.io/fun-rec/_images/din_architecture.png" data-sites="wechat,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY1.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY1.jpg" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY2.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/pay/PAY2.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/butterflyblog/2025/12/18/FunRec_3_2/" title="精排（二）特征交叉"><img class="cover" src="https://datawhalechina.github.io/fun-rec/_images/fibinet_architecture.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">精排（二）特征交叉</div></div></a></div><div class="next-post pull-right"><a href="/butterflyblog/2025/12/18/FunRec_3_4/" title="精排（四）多目标建模"><img class="cover" src="https://datawhalechina.github.io/fun-rec/_images/esm2.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">精排（四）多目标建模</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/butterflyblog/2025/12/17/FunRec_3_1/" title="精排（一）记忆与泛化"><img class="cover" src="https://datawhalechina.github.io/fun-rec/_images/wide_and_deep.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-17</div><div class="title">精排（一）记忆与泛化</div></div></a></div><div><a href="/butterflyblog/2025/12/18/FunRec_3_2/" title="精排（二）特征交叉"><img class="cover" src="https://datawhalechina.github.io/fun-rec/_images/fibinet_architecture.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-18</div><div class="title">精排（二）特征交叉</div></div></a></div><div><a href="/butterflyblog/2025/12/18/FunRec_3_4/" title="精排（四）多目标建模"><img class="cover" src="https://datawhalechina.github.io/fun-rec/_images/esm2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-18</div><div class="title">精排（四）多目标建模</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/T6.jpg" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Seymour0314/PicGo/blog/page_img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">isSeymour</div><div class="author-info__description">志之所趋，无远弗届，穷山距海，不能限也。</div></div><div class="card-info-data site-data is-center"><a href="/butterflyblog/archives/"><div class="headline">文章</div><div class="length-num">89</div></a><a href="/butterflyblog/tags/"><div class="headline">标签</div><div class="length-num">43</div></a><a href="/butterflyblog/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isSeymour/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://isSeymour.github.io/" target="_blank" title="学术主页"><i class="fa-regular fa-address-card" style="color: #000000;"></i></a><a class="social-icon" href="https://github.com/isSeymour/" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="https://space.bilibili.com/79699613/" target="_blank" title="B站"><i class="fa-brands fa-bilibili" style="color: #000000;"></i></a><a class="social-icon" href="https://blog.csdn.net/m0_63205991/" target="_blank" title="CSDN"><i class="fa-solid fa-code" style="color: #000000;"></i></a><a class="social-icon" href="mailto:seymour0314@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">精排（三）序列建模</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81DIN%EF%BC%9A%E5%B1%80%E9%83%A8%E6%BF%80%E6%B4%BB%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">1、DIN：局部激活的注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DIN%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%9A%E5%B1%80%E9%83%A8%E6%BF%80%E6%B4%BB-Local-Activation"><span class="toc-text">DIN的核心思想：局部激活 (Local Activation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">技术实现：注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-text">代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81DIEN%EF%BC%9A%E5%85%B4%E8%B6%A3%E7%9A%84%E6%BC%94%E5%8C%96%E5%BB%BA%E6%A8%A1"><span class="toc-text">2、DIEN：兴趣的演化建模</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%85%B4%E8%B6%A3%E6%8F%90%E5%8F%96%E5%B1%82-Interest-Extractor-Layer"><span class="toc-text">第一阶段：兴趣提取层 (Interest Extractor Layer)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%85%B4%E8%B6%A3%E6%BC%94%E5%8C%96%E5%B1%82-Interest-Evolving-Layer"><span class="toc-text">第二阶段：兴趣演化层 (Interest Evolving Layer)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="toc-text">代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81DSIN%EF%BC%9A%E4%BB%8E%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%88%B0%E4%BC%9A%E8%AF%9D%E5%BA%8F%E5%88%97"><span class="toc-text">3、DSIN：从行为序列到会话序列</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DSIN%E7%9A%84%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%EF%BC%9A%E5%88%86%E5%B1%82%E5%BB%BA%E6%A8%A1"><span class="toc-text">DSIN的技术实现：分层建模</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-3"><span class="toc-text">代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="toc-text">性能对比</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://datawhalechina.github.io/fun-rec/_images/din_architecture.png')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By isSeymour</div><div class="footer_custom_text">欢迎乘坐我的生活地铁！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/butterflyblog/js/utils.js"></script><script src="/butterflyblog/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23liAZxomGv7Hapw8g',
      clientSecret: 'f7cafde192c4ada8bef4b76952c422d90575cf8b',
      repo: 'gitalk',
      owner: 'isSeymour',
      admin: ['isSeymour'],
      id: '2403a9ecec9e090bf880087347154a99',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/butterflyblog/js/search/local-search.js"></script></div></div></body></html>